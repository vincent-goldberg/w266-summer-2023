{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BzBd-N9mS1"
      },
      "source": [
        "# Assignment 2: Text Classification with Various Neural Networks\n",
        "\n",
        "**Description:** This assignment covers various neural network architectures and components, largely used in the context of classification. You will compare Deep Averaging Networks, Deep Weighted Averaging Networks using Attention, and BERT-based models. You should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "*   The effects of fine-tuning word vectors or starting with random word vectors\n",
        "*   How various networks behave when the training set size changes\n",
        "*   The effect of shuffling your training data\n",
        "*   The benefits of Attention calculations\n",
        "*   Working with BERT\n",
        "\n",
        "\n",
        "The assignment notebook closely follows the lesson notebooks. We will use the IMDB dataset and will leverage some of the models, or part of the code, for our current investigation.\n",
        "\n",
        "The initial part of the notebook is purely setup. We will then evaluate how Attention can make Deep Averaging networks better. \n",
        "\n",
        "Do not try to run this entire notebook on your GCP instance as the training of models requires a GPU to work in a timely fashion. This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1h.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2023-summer-main/blob/master/assignment/a2/Text_classification.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "\n",
        "1. Setup\n",
        "  \n",
        "  1.1 Libraries, Embeddings,  & Helper Functions\n",
        "\n",
        "  1.2 Data Acquisition\n",
        "\n",
        "  1.3. Data Preparation\n",
        "\n",
        "      1.3.1 Training/Test Sets using Word2Vec \n",
        "\n",
        "      1.3.2 Training/Test Sets for BERT-based models\n",
        "\n",
        "\n",
        "2. Classification with various Word2Vec-based Models\n",
        "\n",
        "  2.1 The Role of Shuffling of the Training Set\n",
        "\n",
        "  2.2 DAN vs Weighted Averaging Models using Attention\n",
        "\n",
        "    2.2.1 Warm-Up\n",
        "    \n",
        "    2.2.2 The WAN Model\n",
        "    \n",
        "  2.3 Approaches for Training of Embeddings \n",
        "\n",
        "\n",
        "3. Classification with BERT\n",
        "\n",
        "  3.1. BERT Basics\n",
        "\n",
        "  3.2 CLS-Token-based Classification \n",
        "\n",
        "  3.3 Averaging of BERT Outputs\n",
        "\n",
        "  3.4. Adding a CNN on top of BERT\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**: \n",
        "\n",
        "* Questions are always indicated as **QUESTION**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1.  Please do **not** remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* If you want to, you can run all of the cells in section 1 in bulk. This is setup work and no questions are in there. At the end of section 1 we will state all of the relevant variables that were defined and created in section 1.\n",
        "\n",
        "* Finally, unless otherwise indicated your validation accuracy will be 0.65 or higher if you have correctly implemented the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so-yur1S9mS4"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "### 1.1. Libraries and Helper Functions\n",
        "\n",
        "This notebook requires the TensorFlow dataset and other prerequisites that you must download. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "8uQnMctL9mS5"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "\n",
        "!pip install pydot --quiet\n",
        "!pip install gensim --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install -U tensorflow-text --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFFBvPMR9mS8"
      },
      "source": [
        "Now we are ready to do the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "Q8b9aykE9mS8"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import sklearn as sk\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESElm33U9mS9"
      },
      "source": [
        "Below is a helper function to plot histories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "YKWj6pPM9mS-"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Function\n",
        "\n",
        "# 4-window plot. Small modification from matplotlib examples.\n",
        "\n",
        "def make_plot(axs,\n",
        "              model_history1, \n",
        "              model_history2, \n",
        "              model_1_name='model 1',\n",
        "              model_2_name='model 2',\n",
        "              ):\n",
        "    box = dict(facecolor='yellow', pad=5, alpha=0.2)\n",
        "\n",
        "    for i, metric in enumerate(['loss', 'accuracy']):\n",
        "        # small adjustment to account for the 2 accuracy measures in the Weighted Averging Model with Attention\n",
        "        if 'classification_%s' % metric in model_history2.history:\n",
        "            metric2 = 'classification_%s' % metric\n",
        "        else:\n",
        "            metric2 = metric\n",
        "        \n",
        "        y_lim_lower1 = np.min(model_history1.history[metric])\n",
        "        y_lim_lower2 = np.min(model_history2.history[metric2])\n",
        "        y_lim_lower = min(y_lim_lower1, y_lim_lower2) * 0.9\n",
        "\n",
        "        y_lim_upper1 = np.max(model_history1.history[metric])\n",
        "        y_lim_upper2 = np.max(model_history2.history[metric2])\n",
        "        y_lim_upper = max(y_lim_upper1, y_lim_upper2) * 1.1\n",
        "\n",
        "        for j, model_history in enumerate([model_history1, model_history2]):\n",
        "            model_name = [model_1_name, model_2_name][j]\n",
        "            model_metric = [metric, metric2][j]\n",
        "            ax1 = axs[i, j]\n",
        "            ax1.plot(model_history.history[model_metric])\n",
        "            ax1.plot(model_history.history['val_%s' % model_metric])\n",
        "            ax1.set_title('%s - %s' % (metric, model_name))\n",
        "            ax1.set_ylabel(metric, bbox=box)\n",
        "            ax1.set_ylim(y_lim_lower, y_lim_upper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QDi-Kg49mS-"
      },
      "source": [
        "Next, we get the word2vec model from nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49X1T6an9mS_",
        "outputId": "8d10fca5-8556-41f8-8e68-8e5877e29c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title NLTK & Word2Vec\n",
        "\n",
        "nltk.download('word2vec_sample')\n",
        "\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(datapath(word2vec_sample), binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_rdVE3z9mTA"
      },
      "source": [
        "Now here we have the embedding **model** defined, let's see how many words are in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoL6l_q89mTA",
        "outputId": "cf6b5a43-62cf-402d-c8b4-527e99bb2b14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43981"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "len(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Q0zOkJ9mTB"
      },
      "source": [
        "What do the word vectors look like? As expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyAGMYGK9mTB",
        "outputId": "61b7e772-6156-48ae-e1a3-a05c236c152b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0306035 ,  0.0886877 , -0.0121269 ,  0.0761965 ,  0.0566269 ,\n",
              "       -0.0424702 ,  0.0410129 , -0.0497567 , -0.00364328,  0.0632889 ,\n",
              "       -0.0142608 , -0.0791111 ,  0.0174877 , -0.0383064 ,  0.00926433,\n",
              "        0.0295626 ,  0.0770293 ,  0.0949334 , -0.0428866 , -0.0295626 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "model['great'][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMraFZS9mTB"
      },
      "source": [
        "We can now build the embedding matrix and a vocabulary dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "lOTIN3G39mTB"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = len(model['university'])      # we know... it's 300\n",
        "\n",
        "# initialize embedding matrix and word-to-id map:  \n",
        "embedding_matrix = np.zeros((len(model) + 1, EMBEDDING_DIM))  \n",
        "vocab_dict = {}\n",
        "\n",
        "# build the embedding matrix and the word-to-id map:\n",
        "for i, word in enumerate(model.index_to_key):\n",
        "    embedding_vector = model[word]\n",
        "\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        vocab_dict[word] = i\n",
        "\n",
        "# we can use the last index at the end of the vocab for unknown tokens\n",
        "vocab_dict['[UNK]'] = len(vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KlSpLnP6VqA",
        "outputId": "2158bd97-7ed7-4457-94e7-cde9a9b2f595"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43982, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGYcZu0N9mTC",
        "outputId": "6d3f45e6-84ee-445d-8912-cfcaae20b83b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0891758 ,  0.121832  , -0.0671959 ,  0.0477279 , -0.013659  ],\n",
              "       [ 0.0526281 ,  0.013157  , -0.010104  ,  0.0540819 ,  0.0386715 ],\n",
              "       [ 0.0786419 ,  0.0373911 , -0.0131472 ,  0.0347375 ,  0.0288273 ],\n",
              "       [-0.00157585, -0.0564239 ,  0.00320281,  0.0422498 ,  0.15264399],\n",
              "       [ 0.0356899 , -0.00367283, -0.065534  ,  0.0213832 ,  0.00788408]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "embedding_matrix[:5, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIL1eUtV9mTC"
      },
      "source": [
        "The last row consists of all zeros. We will use that for the UNK token, the placeholder token for unknown words.\n",
        "\n",
        "### 1.2 Data Acquisition\n",
        "\n",
        "\n",
        "We will use the IMDB dataset delivered as part of the TensorFlow-datasets library, and split into training and test sets. For expedience, we will limit ourselves in terms of train and test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "uwOF0qYb9mTC"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(5000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPHFtgGkHNOQ"
      },
      "source": [
        "It is always highly recommended to look at the data. What do the records look like? Are they clean or do they contain a lot of cruft (potential noise)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvmWKdVQ9mTC",
        "outputId": "b5a1e423-d4a7-4524-f092-64298939a18f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
              "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.',\n",
              "       b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.',\n",
              "       b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "train_examples[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzEnCspD9mTD",
        "outputId": "c807c4a3-1a27-470f-c83e-14a0a4401a70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "train_labels[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplHsqSDMKCa"
      },
      "source": [
        "For convenience, in this assignment we will define a sequence length and truncate all records at that length. For records that are shorter than our defined sequence length we will add padding characters to insure that our input shapes are consistent across all records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Zxu9U3qXMKTW"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHwj4vu9mTD"
      },
      "source": [
        "## 1.3. Data Preparation\n",
        "\n",
        "### 1.3.1. Training/Test Sets for Word2Vec-based Models\n",
        "\n",
        "First, we tokenize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ToVTmC8V9mTD"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "train_tokens = tokenizer.tokenize(train_examples)\n",
        "test_tokens = tokenizer.tokenize(test_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXauPwil9mTD"
      },
      "source": [
        "Let's look at some tokens.  Does they look acceptable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ22GGb-9mTD",
        "outputId": "e8259757-0087-487d-9b0b-635afb168e35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(116,), dtype=string, numpy=\n",
              "array([b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.',\n",
              "       b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher', b'Walken',\n",
              "       b'or', b'Michael', b'Ironside.', b'Both', b'are', b'great',\n",
              "       b'actors,', b'but', b'this', b'must', b'simply', b'be', b'their',\n",
              "       b'worst', b'role', b'in', b'history.', b'Even', b'their', b'great',\n",
              "       b'acting', b'could', b'not', b'redeem', b'this', b\"movie's\",\n",
              "       b'ridiculous', b'storyline.', b'This', b'movie', b'is', b'an',\n",
              "       b'early', b'nineties', b'US', b'propaganda', b'piece.', b'The',\n",
              "       b'most', b'pathetic', b'scenes', b'were', b'those', b'when',\n",
              "       b'the', b'Columbian', b'rebels', b'were', b'making', b'their',\n",
              "       b'cases', b'for', b'revolutions.', b'Maria', b'Conchita',\n",
              "       b'Alonso', b'appeared', b'phony,', b'and', b'her', b'pseudo-love',\n",
              "       b'affair', b'with', b'Walken', b'was', b'nothing', b'but', b'a',\n",
              "       b'pathetic', b'emotional', b'plug', b'in', b'a', b'movie', b'that',\n",
              "       b'was', b'devoid', b'of', b'any', b'real', b'meaning.', b'I',\n",
              "       b'am', b'disappointed', b'that', b'there', b'are', b'movies',\n",
              "       b'like', b'this,', b'ruining', b\"actor's\", b'like', b'Christopher',\n",
              "       b\"Walken's\", b'good', b'name.', b'I', b'could', b'barely', b'sit',\n",
              "       b'through', b'it.'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "train_tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9D9nqdg9mTE"
      },
      "source": [
        "Yup... looks right. Of course we will need to take care of the encoding later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiqFULXx9mTE"
      },
      "source": [
        "Next, we define a simple function that converts the tokens above into the appropriate word2vec index values.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ytUsu3kmuM3n"
      },
      "outputs": [],
      "source": [
        "def docs_to_vocab_ids(tokenized_texts_list):\n",
        "    \"\"\"\n",
        "    converting a list of strings to a list of lists of word ids\n",
        "    \"\"\"\n",
        "    texts_vocab_ids = []\n",
        "    text_labels = []\n",
        "    valid_example_list = []\n",
        "    for i, token_list in enumerate(tokenized_texts_list):\n",
        "\n",
        "        # Get the vocab id for each token in this doc ([UNK] if not in vocab)\n",
        "        vocab_ids = []\n",
        "        for token in list(token_list.numpy()):\n",
        "            decoded = token.decode('utf-8', errors='ignore')\n",
        "            if decoded in vocab_dict:\n",
        "                vocab_ids.append(vocab_dict[decoded])\n",
        "            else:\n",
        "                vocab_ids.append(vocab_dict['[UNK]'])\n",
        "            \n",
        "        # Truncate text to max length, add padding up to max length\n",
        "        vocab_ids = vocab_ids[:MAX_SEQUENCE_LENGTH]\n",
        "        n_padding = (MAX_SEQUENCE_LENGTH - len(vocab_ids))\n",
        "        # For simplicity in this model, we'll just pad with unknown tokens\n",
        "        vocab_ids += [vocab_dict['[UNK]']] * n_padding\n",
        "        # Add this example to the list of converted docs\n",
        "        texts_vocab_ids.append(vocab_ids)\n",
        "            \n",
        "        if i % 5000 == 0:\n",
        "            print('Examples processed: ', i)\n",
        "\n",
        "    print('Total examples: ', i)\n",
        "    return np.array(texts_vocab_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv_elC2m9mTE"
      },
      "source": [
        "Now we can create training and test data that can be fed into the models of interest.  We need to convert all of the tokens in to their respective input ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpUJLBRkCbtE",
        "outputId": "cb38e5b3-7cd9-44ec-85ab-74911f39233c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples processed:  0\n",
            "Examples processed:  5000\n",
            "Examples processed:  10000\n",
            "Examples processed:  15000\n",
            "Total examples:  19999\n",
            "Examples processed:  0\n",
            "Total examples:  4999\n"
          ]
        }
      ],
      "source": [
        "train_input_ids = docs_to_vocab_ids(train_tokens)\n",
        "test_input_ids = docs_to_vocab_ids(test_tokens)\n",
        "\n",
        "train_input_labels = np.array(train_labels)\n",
        "test_input_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dP2KY7U9mTF"
      },
      "source": [
        "Let's convince ourselves that the data looks correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtU56wVR9mTF",
        "outputId": "02826dce-e529-40b2-9910-49df51001d33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21531, 25272, 12291,  7427, 37254, 43981,  6891, 12917, 38232,\n",
              "        16915, 12929, 16182, 43981, 20526, 23487, 43981, 23807, 42958,\n",
              "        35058, 43981, 19123, 35029, 41270, 29275, 12917, 32597, 20659,\n",
              "          638, 16915, 43981,   174, 32597, 35058, 39971,  2326,  3636,\n",
              "        22434, 35029, 43981, 33922, 43981, 21531, 34710, 16908, 12291,\n",
              "        36880, 28137,  5376, 28038, 43981, 15402, 29155, 18063, 24951,\n",
              "        17433, 17595,  8856, 14193, 43981, 43248, 17433,  6290, 32597,\n",
              "         9001, 11511, 43981, 21807, 39168, 43981, 16856, 43981, 43981,\n",
              "        23245, 43981,  8889,  1331, 43981, 25272, 31976, 19123, 43981,\n",
              "        18063, 36309, 24099, 16915, 43981, 34710, 36633, 25272, 20413,\n",
              "        43981, 33458, 14926, 43981, 12139, 12289, 39617, 36633,  9483,\n",
              "        42958],\n",
              "       [12139,  7841, 19666, 31757, 43981, 17853, 25745, 15445, 43981,\n",
              "        19123, 35029, 16908, 21113, 21068, 43981, 43981,  5668, 43981,\n",
              "        33456, 43981, 34554, 43981,  1200, 27498, 43981, 18802, 20514,\n",
              "        14193, 43981, 43981, 23955, 14042, 15400, 43981, 43981, 32334,\n",
              "        20514, 35029,  7870, 12139, 17108, 25745, 43830, 14193, 28743,\n",
              "        25272, 43981, 15402, 17006,   222, 25272, 43981, 43981, 33994,\n",
              "        43981, 43981, 42659, 12375, 43981, 43981, 19123,  1331, 19870,\n",
              "         7816, 43981, 31696, 25272, 23801, 13877, 20526, 43981, 12139,\n",
              "        43981, 12139, 37939,  7841, 24998, 29109, 43981, 14193, 43981,\n",
              "        19123, 16266,  8017, 14193, 43099, 43981, 16909, 43981, 30811,\n",
              "        14042, 12375, 43981, 25574, 43981, 34511, 21851, 14087,  9370,\n",
              "        33458]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "train_input_ids[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5svfigoOgaE"
      },
      "source": [
        "### 1.3.2. Training/Test Sets for BERT-based models\n",
        "\n",
        "We already imported the BERT model and the Tokenizer libraries. Now, let's load the pretrained BERT model and tokenizer. Always make sure to load the tokenizer that goes with the model you're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "LEyBUFlT53zk"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuBp01dtAZ4e"
      },
      "source": [
        "Next, we will preprocess our train and test data for use in the BERT model. We need to convert our documents into vocab IDs, like we did above with the Word2Vec vocabulary. But this time we'll use the BERT tokenizer, which has a different vocabulary specific to the BERT model we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "cpSk9zvw532w"
      },
      "outputs": [],
      "source": [
        "#@title BERT Tokenization of training and test data\n",
        "\n",
        "train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "bert_train_tokenized = bert_tokenizer(train_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_train_inputs = [bert_train_tokenized.input_ids,\n",
        "                     bert_train_tokenized.token_type_ids,\n",
        "                     bert_train_tokenized.attention_mask]\n",
        "bert_train_labels = np.array(train_labels)\n",
        "\n",
        "bert_test_tokenized = bert_tokenizer(test_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "bert_test_inputs = [bert_test_tokenized.input_ids,\n",
        "                     bert_test_tokenized.token_type_ids,\n",
        "                     bert_test_tokenized.attention_mask]\n",
        "bert_test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myNK4ZhQDQBL"
      },
      "source": [
        "Overall, here are the key variables and sets that we encoded for word2vec and BERT and that may be used moving forward. If the variable naming does not make it obvious, we also state the purpose:\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "* MAX_SEQUENCE_LENGTH (100)\n",
        "\n",
        "\n",
        "#### Word2vec-based models:\n",
        "\n",
        "* train(/test)_input_ids: input ids for the training(/test) sets for word2vec models\n",
        "* train(/test)_input_labels: the corresponding labels\n",
        "\n",
        "#### BERT:\n",
        "\n",
        "\n",
        "* bert_train(/test)_inputs: list of input_ids, token_type_ids and attention_mask for the training(/test) sets for BERT models\n",
        "* bert_train(/test)_labels: the corresponding labels for BERT\n",
        "\n",
        "**NOTE:** We recommend you inspect these variables if you have not gone through the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzbPHBf3GP2O"
      },
      "source": [
        "## 2. Classification with various Word2Vec-based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7yp2gI-AtCl"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.a. Revisit the dataset. Is it balanced? Find the percentage of positive examples in the training set. (Copy and paste the decimal value for your calculation, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6EAE6cjA9jM",
        "outputId": "5e2aa564-ef0d-4a47-e98c-8e8c1ef27506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 20000 examples in the training data set.\n",
            "Of those, 9969 are positive examples.\n",
            "\n",
            "The percent of positive examples in the training data is:\n",
            "0.49845\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# viewing positive vs negative examples in training data\n",
        "\n",
        "train_total_traning_examples = len(train_labels.numpy())\n",
        "train_total_positive_examples = sum(train_labels.numpy())\n",
        "positive_percentage = train_total_positive_examples/train_total_traning_examples\n",
        "\n",
        "print(f\"There are {train_total_traning_examples} examples in the training data set.\\n\" \n",
        "      + f\"Of those, {train_total_positive_examples} are positive examples.\\n\")\n",
        "print(f\"The percent of positive examples in the training data is:\\n{positive_percentage}\")\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGkEVpmu6Bs2"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.b. Now find the percentage of positive examples in the test set.  (Copy and paste the decimal value for your calculation, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K_8eBTHArme",
        "outputId": "a2177b73-fa81-4d2e-e658-6bbf6ef3346c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 5000 examples in the test data set.\n",
            "Of those, 2513 are positive examples.\n",
            "\n",
            "The percent of positive examples in the test data is:\n",
            "0.5026\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# viewing positive vs negative examples in testing data\n",
        "\n",
        "test_total_traning_examples = len(test_labels.numpy())\n",
        "test_total_positive_examples = sum(test_labels.numpy())\n",
        "positive_percentage = test_total_positive_examples/test_total_traning_examples\n",
        "\n",
        "print(f\"There are {test_total_traning_examples} examples in the test data set.\\n\" \n",
        "      + f\"Of those, {test_total_positive_examples} are positive examples.\\n\")\n",
        "print(f\"The percent of positive examples in the test data is:\\n{positive_percentage}\")\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY6X0wL3BQKD"
      },
      "source": [
        "### 2.1 The Role of Shuffling of the Training Set\n",
        "\n",
        "\n",
        "We will first revisit the DAN model. \n",
        "\n",
        "2. Reuse the code from the class notebook to build a DAN network with one hidden layer of dimension 100. The optimizer should be Adam. Wrap the model creation in a function according to this API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "vk-4mCgyBO9S"
      },
      "outputs": [],
      "source": [
        "def create_dan_model(retrain_embeddings=False, \n",
        "                     max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                     hidden_dim=100,\n",
        "                     dropout=0.3,\n",
        "                     embedding_initializer='word2vec', \n",
        "                     learning_rate=0.001):\n",
        "  \"\"\"\n",
        "  Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
        "  :param retrain_embeddings: boolean, indicating whether  the word embeddings are trainable\n",
        "  :param hidden_dim: dimension of the hidden layer\n",
        "  :param dropout: dropout applied to the hidden layer\n",
        "\n",
        "  :returns: the compiled model\n",
        "  \"\"\"\n",
        "\n",
        "  if embedding_initializer == 'word2vec':\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)\n",
        "  else:\n",
        "    embeddings_initializer='uniform'\n",
        "\n",
        "  \n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "  # start by creating the dan_embedding_layer. Use the embeddings_initializer. variable defined above.\n",
        "  \n",
        "  # inputs and embeddings\n",
        "  dan_input_layer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
        "  dan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                                    embedding_matrix.shape[1],\n",
        "                                    embeddings_initializer=embeddings_initializer,\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=retrain_embeddings)\n",
        "  dan_embeddings = dan_embedding_layer(dan_input_layer)\n",
        "    ## averaging embeddings\n",
        "  dan_avg_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1),\n",
        "                                              name='averaging')(dan_embeddings)\n",
        "  \n",
        "  # hidden layers\n",
        "  last_hidden_output = dan_avg_embeddings\n",
        "  last_hidden_output = tf.keras.layers.Dense(hidden_dim, activation='relu',\n",
        "                                             name='dan_hidden_1')(last_hidden_output)\n",
        "  last_hidden_output = tf.keras.layers.Dropout(dropout)(last_hidden_output)\n",
        "\n",
        "  dan_classification = tf.keras.layers.Dense(1, activation='sigmoid',\n",
        "                                             name='dan_classification')(last_hidden_output)\n",
        "\n",
        "  ## creating model/compiling\n",
        "  dan_model = tf.keras.models.Model(inputs=dan_input_layer, outputs=[dan_classification])\n",
        "  dan_model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "                                                         beta_1=0.9,\n",
        "                                                         beta_2=0.999,\n",
        "                                                         epsilon=1e-07,\n",
        "                                                         amsgrad=False,\n",
        "                                                         name='Adam'),\n",
        "                    metrics='accuracy')\n",
        "\n",
        "  ### END YOUR CODE\n",
        "  return dan_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb4LOJkFlYwF"
      },
      "source": [
        "Let us create a sorted version of the training dataset to run our simulations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "ZX2hWslCflw1"
      },
      "outputs": [],
      "source": [
        "sorted_train_input_data = [(x, y) for (x, y) in zip(list(train_input_ids), list(train_input_labels))]\n",
        "sorted_train_input_data.sort(key = lambda x: x[1])\n",
        "sorted_training_input_ids = np.array([x[0] for x in sorted_train_input_data])\n",
        "sorted_training_labels = np.array([x[1] for x in sorted_train_input_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riQ59wcQmtzs"
      },
      "source": [
        "Next, create your DAN model using the default parameters and train it by:\n",
        "\n",
        "1.  Using the sorted dataset\n",
        "2.  Using 'shuffle=False' as one of the model.fit parameters.\n",
        "3.  Train for 10 epochs with a batch size of 32\n",
        "\n",
        "Make sure you store the history (name it 'dan_sorted_history') as we did in the lesson notebooks. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIgwDUfpi7nu",
        "outputId": "5e590fa8-62e5-49e6-ccc3-ffe2710d6d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3220 - accuracy: 0.9052 - val_loss: 2.4866 - val_accuracy: 0.5026\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.7562 - accuracy: 0.6796 - val_loss: 1.6350 - val_accuracy: 0.5026\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.6390 - accuracy: 0.7111 - val_loss: 1.2937 - val_accuracy: 0.5026\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5994 - accuracy: 0.7215 - val_loss: 1.1815 - val_accuracy: 0.5026\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.5908 - accuracy: 0.7234 - val_loss: 1.1796 - val_accuracy: 0.5026\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.5861 - accuracy: 0.7239 - val_loss: 1.1767 - val_accuracy: 0.5026\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.5772 - accuracy: 0.7294 - val_loss: 1.1848 - val_accuracy: 0.5026\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.5712 - accuracy: 0.7376 - val_loss: 1.2012 - val_accuracy: 0.5026\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5624 - accuracy: 0.7396 - val_loss: 1.3078 - val_accuracy: 0.5026\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.5572 - accuracy: 0.7490 - val_loss: 1.2195 - val_accuracy: 0.5026\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# DAN model with sorted data \n",
        "dan_model_sorted = create_dan_model()\n",
        "\n",
        "dan_sorted_history = dan_model_sorted.fit(sorted_training_input_ids,\n",
        "                            sorted_training_labels,\n",
        "                            validation_data=(test_input_ids, test_input_labels),\n",
        "                            batch_size=32,\n",
        "                            epochs=10,\n",
        "                            shuffle=False\n",
        "                            )\n",
        "\n",
        "### END YOUR CODE                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4zFifGHMS1S"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.1.a What is the highest validation accuracy that you observed after you completed the 10 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "Hint: You should have an accuracy number above 0.30.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNYdZ8rnaNX"
      },
      "source": [
        "Next, recreate the same model and train it with **'shuffle=True'**. (Note that this is also the default.). Use 'dan_shuffled_history' for the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEsrjV2QkCo_",
        "outputId": "3aed37d7-0afe-450f-dfcc-e8938b8478fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 5ms/step - loss: 0.6374 - accuracy: 0.6571 - val_loss: 0.5925 - val_accuracy: 0.6934\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.5471 - accuracy: 0.7325 - val_loss: 0.5362 - val_accuracy: 0.7338\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5183 - accuracy: 0.7474 - val_loss: 0.5225 - val_accuracy: 0.7418\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5057 - accuracy: 0.7570 - val_loss: 0.5185 - val_accuracy: 0.7420\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4996 - accuracy: 0.7576 - val_loss: 0.5073 - val_accuracy: 0.7542\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4932 - accuracy: 0.7623 - val_loss: 0.5040 - val_accuracy: 0.7548\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.4870 - accuracy: 0.7638 - val_loss: 0.5012 - val_accuracy: 0.7554\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.4832 - accuracy: 0.7692 - val_loss: 0.4996 - val_accuracy: 0.7546\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4808 - accuracy: 0.7692 - val_loss: 0.4954 - val_accuracy: 0.7564\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4774 - accuracy: 0.7703 - val_loss: 0.4937 - val_accuracy: 0.7582\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# DAN model with shuffled data\n",
        "dan_model_shuffled = create_dan_model()                                                      \n",
        "\n",
        "dan_shuffled_history = dan_model_shuffled.fit(sorted_training_input_ids,\n",
        "                            sorted_training_labels,\n",
        "                            validation_data=(test_input_ids, test_input_labels),\n",
        "                            batch_size=32,\n",
        "                            epochs=10,\n",
        "                            shuffle=True\n",
        "                            )\n",
        "\n",
        "### END YOUR CODE                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXs6UX44ko7P"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.1.b What is the highest validation accuracy that you observed for the shuffled run after completing 10 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYCwHBzyoY0_"
      },
      "source": [
        "Compare the 2 histories in a plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "ZAlGkoidkun-",
        "outputId": "8dd6f204-f99b-411f-c60b-2e5a31164c4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1850x1050 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABV8AAANqCAYAAACaRDs0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADj1ElEQVR4nOzdd3hUZfrG8fvMZEp6SKeE3ouggAhYcEVZxYIVK4h17Yjr/kBXEQvsqqusgmJ3dXVF17aWxYLYcemKCgm9mgakJzPJzPn9McmQgQAJZDKT5Pu5rnPN5J1TnkmU5L3z5jmGaZqmAAAAAAAAAACNyhLqAgAAAAAAAACgJSJ8BQAAAAAAAIAgIHwFAAAAAAAAgCAgfAUAAAAAAACAICB8BQAAAAAAAIAgIHwFAAAAAAAAgCAgfAUAAAAAAACAICB8BQAAAAAAAIAgIHwFAAAAAAAAgCAgfAXQIC+//LIMw9DmzZtDXUqrxdcAAAAAzV1r+Jl28+bNMgxDjz76aKOet6SkRNdcc43S09NlGIYmT54sScrJydEFF1ygpKQkGYah2bNn68svv5RhGPryyy8b7fr33XefDMNotPMBLV1EqAsAgJaorKxMDz/8sEaNGqVRo0aFuhwAAAAALcTMmTP18ssv65577lG3bt3Up08fSdLtt9+uTz75RNOnT1d6erqGDBmi7OzsEFcLgPAVAIKgrKxMM2bMkCTCVwAAAACN5osvvtBxxx2n6dOn7zd+zjnn6I9//KN/jPAVCD3aDgBAI/J6vaqoqAh1GQAAAABaqNzcXCUkJNR7HEBoEb4CaBRPPfWU+vXrJ4fDoXbt2ummm25SQUFBwD7r1q3T+eefr/T0dDmdTnXo0EEXX3yxCgsL/ft89tlnOv7445WQkKCYmBj16tVLd911V6PWumzZMo0ZM0bJycmKjIxUly5ddNVVVwXsU1paqjvuuEMZGRlyOBzq1auXHn30UZmmGbCfYRi6+eab9dprr/nf/7x585SSkiJJmjFjhgzDkGEYuu+++/zHrV27VhdccIESExPldDo1ZMgQ/ec//9mv1l9++UW/+93vFBkZqQ4dOujBBx+U1+tt1M8HAAAAEC5a2ryixrPPPqtu3brJ4XBo6NChWrp0acDrB2pXduWVV6pz586S5O/fumnTJn300Uf+eUZN/1zTNDV37lz/+MH873//0+9//3vFx8crKipKJ510kr777rv99vv22281dOhQOZ1OdevWTc8880z9PjkA/Gg7AOCI3XfffZoxY4ZGjx6tG264QZmZmXr66ae1dOlSfffdd7LZbHK73RozZoxcLpduueUWpaena8eOHfrwww9VUFCg+Ph4/fLLLzrzzDN11FFH6f7775fD4dD69evr/CHgcOXm5uq0005TSkqKpk6dqoSEBG3evFnvvPOOfx/TNHX22Wdr0aJFuvrqqzVo0CB98sknuvPOO7Vjxw49/vjjAef84osv9Oabb+rmm29WcnKyBg4cqKefflo33HCDzj33XJ133nmSpKOOOkqSL1AdOXKk2rdvr6lTpyo6Olpvvvmmxo0bp7ffflvnnnuuJN+fCJ188smqqqry7/fss88qMjKy0T4fAAAAQLhoafOKGq+//rqKi4t1/fXXyzAMPfzwwzrvvPO0ceNG2Wy2el+zT58+evXVV3X77berQ4cOuuOOOyRJRx99tF599VVdccUVOvXUUzVhwoSDnueLL77Q6aefrsGDB2v69OmyWCx66aWX9Lvf/U7ffPONjj32WEnS6tWr/e/xvvvuU1VVlaZPn660tLQGfKYAyASABnjppZdMSeamTZtM0zTN3Nxc0263m6eddprp8Xj8+82ZM8eUZL744oumaZrmypUrTUnmW2+9dcBzP/7446YkMy8vL2j1v/vuu6Ykc+nSpQfc57333jMlmQ8++GDA+AUXXGAahmGuX7/ePybJtFgs5i+//BKwb15eninJnD59+n7nP+WUU8wBAwaYFRUV/jGv12uOGDHC7NGjh39s8uTJpiTzf//7n38sNzfXjI+PD/gaAAAAAM1Na5hXbNq0yZRkJiUlmbt37/aPv//++6Yk84MPPvCPnXTSSeZJJ5203zkmTpxodurUKWCsU6dO5tixY/fbV5J50003BYwtWrTIlGQuWrTINE3fvKNHjx7mmDFjTK/X69+vrKzM7NKli3nqqaf6x8aNG2c6nU5zy5Yt/rFff/3VtFqtJnESUH+0HQBwRD7//HO53W5NnjxZFsvef1KuvfZaxcXF6aOPPpIkxcfHS5I++eQTlZWV1Xmumv5E77//ftD+tL7mGh9++KEqKyvr3Ofjjz+W1WrVrbfeGjB+xx13yDRN/fe//w0YP+mkk9S3b996XX/37t364osvdNFFF6m4uFj5+fnKz8/Xrl27NGbMGK1bt047duzw13Hcccf5f/MsSSkpKbrsssvq+3YBAACAZqElzitqjB8/Xm3atPF/fMIJJ0iSNm7cGJTaDmbVqlVat26dLr30Uu3atcs/HyktLdUpp5yir7/+Wl6vVx6PR5988onGjRunjh07+o/v06ePxowZ0+R1A80Z4SuAI7JlyxZJUq9evQLG7Xa7unbt6n+9S5cumjJlip5//nklJydrzJgxmjt3bkBfpvHjx2vkyJG65pprlJaWposvvlhvvvnmIX9g2r17t7Kzs/1b7XPu66STTtL555+vGTNmKDk5Weecc45eeukluVyugPfUrl07xcbGBhzbp0+fgPdco0uXLgetr7b169fLNE3dc889SklJCdhq7laam5vrv06PHj32O8e+n2sAAACguWuJ84oatcNLSf4gds+ePQetJxjWrVsnSZo4ceJ+85Hnn39eLpdLhYWFysvLU3l5OfMRoBEQvgJoMn/729/0008/6a677lJ5ebluvfVW9evXT9u3b5ckRUZG6uuvv9bnn3+uK664Qj/99JPGjx+vU089VR6P54DnPe+889S2bVv/dttttx1wX8Mw9O9//1uLFy/WzTffrB07duiqq67S4MGDVVJScljvqyE9WGt+4PvjH/+ozz77rM6te/fuh1UHAAAA0Bo0t3mF1Wqt8xxmrZv5HugGWQer93DUzEceeeSRA85HYmJiGvWaQGvHDbcAHJFOnTpJkjIzM9W1a1f/uNvt1qZNmzR69OiA/QcMGKABAwboz3/+s77//nuNHDlS8+bN04MPPihJslgsOuWUU3TKKafoscce08yZM3X33Xdr0aJF+52rxt/+9reA3xq3a9fukHUfd9xxOu644/TQQw/p9ddf12WXXaY33nhD11xzjTp16qTPP/9cxcXFAatf165dG/CeD+ZAPzzVfI5sNtsB30+NTp06+X8zXVtmZuYhrw8AAAA0Jy1xXtEQbdq0qbMNwb5/dXekunXrJkmKi4s76HwkJSVFkZGRzEeARsDKVwBHZPTo0bLb7XriiScCfnP7wgsvqLCwUGPHjpUkFRUVqaqqKuDYAQMGyGKx+P80Z/fu3fudf9CgQZJU55/v1Bg8eLBGjx7t3w7Wf3XPnj0BddZ1jTPOOEMej0dz5swJ2O/xxx+XYRg6/fTTD3j+GlFRUZKkgoKCgPHU1FSNGjVKzzzzjH777bf9jsvLy/M/P+OMM/TDDz9oyZIlAa+/9tprh7w+AAAA0Jy0xHlFQ3Tr1k1r164NmA/8+OOP+u677xp8roMZPHiwunXrpkcffbTOv/yrub7VatWYMWP03nvvaevWrf7X16xZo08++aRRawJaOla+AjgiKSkpmjZtmmbMmKHf//73Ovvss5WZmamnnnpKQ4cO1eWXXy5J+uKLL3TzzTfrwgsvVM+ePVVVVaVXX31VVqtV559/viTp/vvv19dff62xY8eqU6dOys3N1VNPPaUOHTro+OOPb5R6//GPf+ipp57Sueeeq27duqm4uFjPPfec4uLidMYZZ0iSzjrrLJ188sm6++67tXnzZg0cOFCffvqp3n//fU2ePNn/2+KDiYyMVN++fTV//nz17NlTiYmJ6t+/v/r376+5c+fq+OOP14ABA3Tttdeqa9euysnJ0eLFi7V9+3b9+OOPkqQ//elPevXVV/X73/9et912m6Kjo/Xss8+qU6dO+umnnxrl8wEAAACEg5Y4r2iIq666So899pjGjBmjq6++Wrm5uZo3b5769eunoqKiRqlZ8q0Ifv7553X66aerX79+mjRpktq3b68dO3Zo0aJFiouL0wcffCBJmjFjhhYsWKATTjhBN954o6qqqvTkk0+qX79+zEeAhjABoAFeeuklU5K5adOmgPE5c+aYvXv3Nm02m5mWlmbecMMN5p49e/yvb9y40bzqqqvMbt26mU6n00xMTDRPPvlk8/PPP/fvs3DhQvOcc84x27VrZ9rtdrNdu3bmJZdcYmZlZTVa/StWrDAvueQSs2PHjqbD4TBTU1PNM88801y2bFnAfsXFxebtt99utmvXzrTZbGaPHj3MRx55xPR6vQH7STJvuummOq/1/fffm4MHDzbtdrspyZw+fbr/tQ0bNpgTJkww09PTTZvNZrZv394888wzzX//+98B5/jpp5/Mk046yXQ6nWb79u3NBx54wHzhhRfq/BoAAAAAzUVrmFds2rTJlGQ+8sgj+x2/7/zANE3zn//8p9m1a1fTbrebgwYNMj/55BNz4sSJZqdOnQL269Spkzl27Ng6z7nv3GTRokWmJHPRokUB4ytXrjTPO+88MykpyXQ4HGanTp3Miy66yFy4cGHAfl999ZV/TtO1a1dz3rx55vTp003iJKD+DNPcZ508AAAAAAAAAOCI0fMVAAAAAAAAAIKA8BUAAAAAAAAAgoDwFQAAAAAAAACCgPAVAAAAAAAAAIKA8BUAAAAAAAAAgoDwFQAAAAAAAACCICLUBYQjr9ernTt3KjY2VoZhhLocAADQAKZpqri4WO3atZPFwu+ZAYQn5hwAADRfDZlzEL7WYefOncrIyAh1GQAA4Ahs27ZNHTp0CHUZAFAn5hwAADR/9ZlzEL7WITY2VpLvExgXFxfiagAAQEMUFRUpIyPD//0cAMIRcw4AAJqvhsw5CF/rUPNnP3FxcfwgBABAM8Wf8QIIZ8w5AABo/uoz56ARGgAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAARBswhf586dq86dO8vpdGrYsGFasmTJQfefPXu2evXqpcjISGVkZOj2229XRUVFE1ULAAAAoLlhzgEAAIIh7MPX+fPna8qUKZo+fbpWrFihgQMHasyYMcrNza1z/9dff11Tp07V9OnTtWbNGr3wwguaP3++7rrrriauHAAAAEBzwJwDAAAES9iHr4899piuvfZaTZo0SX379tW8efMUFRWlF198sc79v//+e40cOVKXXnqpOnfurNNOO02XXHLJIX9zDQAAAKB1Ys4BAACCJazDV7fbreXLl2v06NH+MYvFotGjR2vx4sV1HjNixAgtX77c/4PPxo0b9fHHH+uMM8444HVcLpeKiooCNgAAAAAtH3MOAAAQTBGhLuBg8vPz5fF4lJaWFjCelpamtWvX1nnMpZdeqvz8fB1//PEyTVNVVVX6wx/+cNA/AZo1a5ZmzJjRqLUDAAAACH/MOQAAQDCF9crXw/Hll19q5syZeuqpp7RixQq98847+uijj/TAAw8c8Jhp06apsLDQv23btq0JKwYAAADQnDDnAAAA9RXWK1+Tk5NltVqVk5MTMJ6Tk6P09PQ6j7nnnnt0xRVX6JprrpEkDRgwQKWlpbruuut09913y2LZP292OBxyOByN/wYAAAAAhDXmHAAAIJjCeuWr3W7X4MGDtXDhQv+Y1+vVwoULNXz48DqPKSsr2++HHavVKkkyTTN4xQIAAABodphzAACAYArrla+SNGXKFE2cOFFDhgzRscceq9mzZ6u0tFSTJk2SJE2YMEHt27fXrFmzJElnnXWWHnvsMR199NEaNmyY1q9fr3vuuUdnnXWW/wciAAAAAKjBnAMAAARL2Iev48ePV15enu69915lZ2dr0KBBWrBggb8h/tatWwN+6/znP/9ZhmHoz3/+s3bs2KGUlBSdddZZeuihh0L1FgAAAACEMeYcAAAgWAyTv4vZT1FRkeLj41VYWKi4uLhQlwMAABqA7+MAmgP+rQIAoPlqyPfxsO75CgAAAAAAAADNFeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErAAAAAAAAAAQB4SsAAAAAAAAABAHhKwAAAAAAAAAEAeErGszjNXXZ8z/osud/kKvKE+pyAAAAAAAAgLBE+IoG25Rfqu/W79J363fpqUUbQl0OAAAAAAAAEJYIX9FgWTnF/udPfble63OLD7I3AAAAAAAA0DoRvqLBMrP3hq2VHlPT3lktr9cMYUUAAAAAAABA+CF8RYOtq17pOmlkZ0XZrVq6eY/eWLotxFUBAAAAAAAA4YXwFQ1Ws/L1d71TdcdpvSRJs/67RrlFFaEsCwAAAAAAAAgrhK9okIpKjzbvKpMk9UyL1ZUjOuuoDvEqrqjSjA9+DXF1AAAAAAAAQPggfEWDbMwrlcdrKj7SptRYh6wWQzPPHSCrxdBHq3/T57/mhLpEAAAAAAAAICwQvqJBavq99kqLlWEYkqT+7eN1zfFdJEn3vP+zSlxVIasPAAAAAAAACBeEr2iQmn6vPdNjAsYnj+6pjMRI/VZYoUc/yQxFaQAAAAAAAEBYIXxFg2TlVIevabEB45F2qx4aN0CS9I/Fm7VqW0FTlwYAAAAAAACEFcJXNEjmAcJXSTqxZ4rOPbq9TFOa+vZPqvR4m7o8AAAAAAAAIGwQvqLeytxV2ra7XFLd4ask/XlsHyVE2bQ2u1jPf7OpKcsDAAAAAAAAwgrhK+ptXU6JJCkl1qHEaHud+yTFOPTnsX0lSbM/z9KWXaVNVh8AAAAAAAAQTghfUW97Ww7EHHS/849prxHdkuSq8urud3+WaZpNUR4AAAAAAAAQVghfUW9Z2Qfu91qbYRiaee4AOSIs+nZ9vt5ZsaMpygMAAAAAAADCCuEr6i0r19d2oNchwldJ6pwcrVtP6SFJevCjX7WrxBXU2gAAAAAAAIBwQ/iKevOvfE0/dPgqSded2FW902O1p6xSD320JpilAQAAAAAAAGGH8BX1UlhWqeyiCklSj9SD93ytYbNaNOu8ATIM6Z2VO/TNurxglggAAAAAAACEFcJX1EtWrm/Va/uESMU6bfU+7uiObTRxeGdJ0t3v/qxytycY5QEAAAAAAABhh/AV9ZKVU3Ozrfqteq3tj2N6qW28U1t3l+nvC9c1dmkAAAAAAABAWCJ8Rb00tN9rbTGOCN1/Tn9J0nPfbNSvO4satTYAAAAAAAAgHBG+ol4ya1a+pjY8fJWkU/um6fT+6fJ4TU175yd5vGZjlgcAAAAAAACEHcJX1EtWTokkqddhrHytcd/Z/RTrjNCP2wv1j+83N1JlAAAAAAAAQHgifMUh5Ze4tLvULcOQuqc2vOdrjbQ4p6ae3luS9OinmdpRUN5YJQIAAAAAAABhh/AVh1TT77VzUrScNusRneuSoR01pFMblbk9uue9n2WatB8AAAAAAABAy0T4ikOq6ffa4whWvdawWAzNOm+AbFZDX6zN1cers4/4nAAAAAAAAEA4InzFIWVVh69H0u+1th5psbphVHdJ0vT//KLCsspGOS8AAAAAAAAQTghfcUg1N9vqmdY44ask3Tiqm7qmRCu/xKW/LFjbaOcFAAAAAAAAwgXhKw7KNE1/z9fGWvkqSU6bVbPOHSBJ+teSrfrfxl2Ndm4AAAAAAAAgHBC+4qB+K6xQsatKERZDnZOiG/Xcw7om6ZJjMyRJ095dLVeVp1HPDwAAAAAAAIQS4SsOquZmW11TomWPaPz/XKb+vo+SYxzamFeqpxZtaPTzAwAAAAAAAKFC+IqDWlcdvjZmv9fa4qNsuu/svpKkp75cr/W5xUG5DgAAAAAAANDUCF9xUJnZjX+zrX2NHdBWv+udqkqPqWnvrJbXawbtWgAAAAAAAEBTIXzFQWUFeeWrJBmGofvP6acou1VLN+/RG0u3Be1aAAAAAAAAQFNpFuHr3Llz1blzZzmdTg0bNkxLliw54L6jRo2SYRj7bWPHjm3CilsGr9fUuuo2AL3Sgxe+SlKHNlG647RekqRZ/12j3KKKoF4PAAAAqI05BwAACIawD1/nz5+vKVOmaPr06VqxYoUGDhyoMWPGKDc3t87933nnHf3222/+7eeff5bVatWFF17YxJU3f9v2lKmi0itHhEUdE6OCfr0rR3TWUR3iVVxRpfs++CXo1wMAAAAk5hwAACB4wj58feyxx3Tttddq0qRJ6tu3r+bNm6eoqCi9+OKLde6fmJio9PR0//bZZ58pKiqKH4QOQ2a2b9Vr99QYWS1G0K9ntRiadd4AWS2GPl6drc9+zQn6NQEAAADmHAAAIFjCOnx1u91avny5Ro8e7R+zWCwaPXq0Fi9eXK9zvPDCC7r44osVHR19wH1cLpeKiooCNuzt99oriP1e99WvXbyuOaGLJOne939Wiauqya4NAACA1oc5BwAACKawDl/z8/Pl8XiUlpYWMJ6Wlqbs7OxDHr9kyRL9/PPPuuaaaw6636xZsxQfH+/fMjIyjqjuliIzp0SS1DPI/V73NfmUnspIjNRvhRV69JPMJr02AAAAWhfmHAAAIJjCOnw9Ui+88IIGDBigY4899qD7TZs2TYWFhf5t27ZtTVRheFsXgpWvkhRpt+qhcQMkSf9YvFmrthU06fUBAACA+mLOAQAADiasw9fk5GRZrVbl5AT2/szJyVF6evpBjy0tLdUbb7yhq6+++pDXcTgciouLC9hau0qPVxvyfCtfe6TFNPn1T+yZonOPbi/TlKa+/ZMqPd4mrwEAAAAtH3MOAAAQTGEdvtrtdg0ePFgLFy70j3m9Xi1cuFDDhw8/6LFvvfWWXC6XLr/88mCX2SJtzi9VpcdUtN2q9gmRIanhz2P7KCHKprXZxXr+m00hqQEAAAAtG3MOAAAQTGEdvkrSlClT9Nxzz+kf//iH1qxZoxtuuEGlpaWaNGmSJGnChAmaNm3afse98MILGjdunJKSkpq65BYhs7rlQM/0WBmGEZIakmIc+vPYvpKk2Z9nacuu0pDUAQAAgJaNOQcAAAiWiFAXcCjjx49XXl6e7r33XmVnZ2vQoEFasGCBvyH+1q1bZbEEZsiZmZn69ttv9emnn4ai5BYhq/pmW03d73Vf5x/TXu+u3K7v1u/SXe+u1j+vHhayMBgAAAAtE3MOAAAQLIZpmmaoiwg3RUVFio+PV2FhYavtxfSHV5drwS/ZuufMvrr6+C4hrWVzfqnGzP5ariqv/nbhQJ0/uENI6wEAhDe+jwNoDvi3CgCA5qsh38fDvu0AQiOruu1AqFe+SlLn5GjdNrqHJOnBj37VrhJXiCsCAAAAAAAADo3wFfupqPRoc3V/1Z7pMSGuxufaE7qqd3qs9pRV6qGP1oS6HAAAAAAAAOCQCF+xnw15JfKaUpsom1JiHKEuR5Jks1o067wBMgzpnZU79M26vFCXBAAAAAAAABwU4Sv2U9NyoEdabFjd3Orojm00cXhnSdLd7/6scrcntAUBAAAAAAAAB0H4iv1kZpdICo9+r/v645heahvv1NbdZfr7wnWhLgcAAAAAAAA4IMJX7Kdm5WvP9PALX2McEXrgnP6SpOe+2ahfdhaGuCIAAAAAAACgboSv2E9N+BqOK18laXTfNJ0xIF0er6lp76yWx2uGuiQAAAAAAABgP4SvCFDiqtL2PeWSpJ5pMSGu5sDuO6ufYp0R+ml7of7x/eZQlwMAAAAAAADsh/AVAdZVr3pNjXUoIcoe4moOLDXOqamn95YkPfpppnYUlIe4IgAAAAAAACAQ4SsC+FsOhGG/131dMrSjhnRqozK3R/e897NMk/YDAAAAAAAACB+ErwiQlVMiSeoZpv1ea7NYDM06b4BsVkNfrM3Vx6uzQ10SAAAAAAAA4Ef4igA1K1/Dud9rbT3SYnXjqO6SpOn/+UWFZZUhrggAAAAAAADwIXxFgMzsmvA1/Fe+1rjx5G7qmhKt/BKX/rJgTajLAQAAAAAAACQRvqKWgjK3cotdknwrSpsLR4RVs84dIEn615Jt+t/GXSGuCAAAAAAAACB8RS01/V47tIlUjCMixNU0zLCuSbrk2AxJ0rR3V8tV5QlxRQAAAAAAAGjtCF/hl5nT/FoO1Db1932UHOPQxrxSPbVoQ6jLAQAAAAAAQCtH+Aq/rGbY77W2+Cib7ju7ryTpqS/Xa31ucYgrAgAAAAAAQGtG+Aq/mpWvvdJjQlzJ4Rs7oK1+1ztVlR5T095ZLa/XDHVJAAAAAAAAaKUIXyFJMk1T65p52wFJMgxDD4zrryi7VUs379EbS7eFuiQAAAAAAAC0UoSvkCTllbi0p6xSFkPqltJ8V75KUvuESP3xtF6SpFn/XaPcoooQVwQAAAAAAIDWiPAVkqSs7BJJUuekaDlt1hBXc+QmjuisozrEq7iiSvd98EuoywEAAAAAAEArRPgKSXv7vTbnlgO1WS2GZp03QFaLoY9XZ+uzX3NCXRIAAAAAAABaGcJXSNLefq/pLSN8laR+7eJ1zQldJEn3vv+zSlxVIa4IAAAAAAAArQnhKyTVXvnavPu97mvyKT3VMTFKvxVW6NFPMkNdDgAAAAAAAFoRwlfINE1lZfvC114tpO1AjUi7VQ+d21+S9I/Fm7VqW0FoCwIAAAAAAECrQfgK7SgoV6nbI5vVUOfk6FCX0+hO6JGi845uL9OUpr79kyo93lCXBAAAAAAAgFaA8BVal1MiSeqWEiObtWX+J3H32D5qE2XT2uxiPffNxlCXAwAAAAAAgFagZSZtaJCafq89WljLgdqSYhz689i+kqS/f75Om/NLQ1wRAAAAAAAAWjrCV9Tq99qybra1r/OOaa+R3ZPkqvLq7vdWyzTNUJcEAAAAAACAFozwFf6Vrz1b8MpXSTIMQw+NGyBHhEXfrd+ld1bsCHVJAAAAAAAAaMEIX1s5j9fU+lxfz9de6S07fJWkzsnRum10D0nSgx/9ql0lrhBXBAAAAAAAgJaK8LWV27q7TK4qr5w2izLaRIW6nCZx7Qld1Ts9VnvKKvXQR2tCXQ4AAAAAAABaKMLXVi6zut9rj9RYWSxGiKtpGjarRX85/ygZhvTOyh36Zl1eqEsCAAAAAABAC0T42spltZJ+r/salJGgicM7S5Luene1cosqQlsQAAAAAAAAWhzC11auJnztlR4T4kqa3h/H9FLbeKe27S7XiY8s0sML1qqwvDLUZQEAAACHraiiUre9sVKb80tDXQoAABDha6tXE772aGUrXyUpxhGhF68cqmM6Jqii0qunvtygEx9epGe/3qCKSk+oywMAAAAa7L73f9H7q3bqmleWqaiChQUAAIQa4Wsr5q7yamOe7zfivVph+CpJfdrG6e0bRujZKwarR2qMCssrNfPjtTr50S81f+lWVXm8oS4RAAAAqLepp/dWWpxD63NLNPmNVfJ4zVCXBABAq0b42optyi9VlddUrCNCbeOdoS4nZAzD0Gn90rVg8ol6+IKj1C7eqd8KK/R/b6/W7//+jRb8nC3T5IdWAAAAhL/UOKeemzBEjgiLvlibq0c+yQx1SQAAtGqEr62Y/2Zb6bEyDCPE1YSe1WLooiEZ+uKPo/TnsX2UEGXT+twS/eGfy3XuU99r8YZdoS4RAAAAOKSjOiTo4QuOkiTN+2qD3l25PcQVAQDQehG+tmL+8DWt9d1s62CcNquuOaGrvv7Tybrld90VabNq1bYCXfLcD5r44hL9srMw1CUCAAAAB3XOoPa6cVQ3SdL/vb1aq7YVhLYgAABaKcLXViwzuyZ8bZ39Xg8lzmnTHaf10ld/GqUrjuukCIuhr7LyNPaJb3Xrv1Zqyy7uIAsAAIDw9cfTeml0n1S5q7y67pVlyi6sCHVJAAC0OoSvrVjNytfWerOt+kqNdeqBcf31+ZSTdPbAdpKk//y4U6f87Svd+/7Pyi3mh1gAAACEH4vF0OPjB6lnWoxyi126/tVlqqj0hLosAABaFcLXVqqi0qMtu8sk+Xq+4tA6J0friUuO1oe3HK8Te6aoymvqlcVbNOqRL/W3TzNVXFEZ6hIBAACAALFOm56fMFQJUTb9uL1QU9/+iZvJAgDQhAhfW6n1uSUyTSkx2q7kGEeoy2lW+reP1ytXHavXrx2mgRkJKnN79OQX63Xiw4v0/DcbWU0AAACAsNIxKUpPXXaMrBZD763aqXlfbQx1SQAAtBqEr63U3n6v3GzrcI3olqz3bhyheZcfo64p0dpTVqkHP1qjU/72ld5atk0eLysKAAAAEB5GdEvWfWf1lSQ9/MlaLVyTE+KKAABoHQhfWyn6vTYOwzD0+/5t9enkE/XX8wcoPc6pHQXluvPfP+n0v3+tz37N4c+6AAAAEBauGN5Zlw3rKNOUbntjlX9OAAAAgofwtZWq+UGLfq+NI8Jq0fihHfXlnaM07fTeio+0KSunRNe+skwXzFusJZt2h7pEAAAAQPed3U/DuiSqxFWla/6xTHtK3aEuCQCAFo3wtZXKyimRJPVk5Wujctqsuv6kbvr6TyfrxlHd5LRZtHzLHl30zGJd9fJSrfmtKNQlAgAAoBWzWS16+vLB6tAmUlt3l+mm11eo0uMNdVkAALRYhK+tUHFFpXYUlEuSeqYSvgZDfKRNf/p9b31158m6dFhHWS2GvlibqzOe+Ea3z1+lbbvLQl0iAAAAWqnEaLuenzhEUXarvt+wSw9++GuoSwIAoMUifG2Fala9psc5FR9lC3E1LVtanFMzzx2gz24/UWOPaivTlN5duUO/+9uXuu8/vyi/xBXqEgEAANAK9U6P0+PjB0mS/rF4i17/39bQFgQAQAtF+NoKravu99ojLSbElbQeXVNiNPfSY/Sfm0fqhB7JqvSYevn7zTrp4UV6/LMslbiqQl0iAAAAWpkx/dL1x9N6SpLuff9n/W/jrhBXBABAy0P42gplVoevvej32uSO6pCgV68epteuGaajOsSr1O3R3xeu00kPL9JL322Sq8oT6hIBAADQitx0cnedeVRbVXlN3fDaCtpjAQDQyAhfW6Gs6vC1Zzrha6iM7J6s928aqbmXHqMuydHaVerWjA9+1Sl/+0rvrNguj9cMdYkAAABoBQzD0CMXDFT/9nHaXerWta8sUyl/lQUAQKMhfG2FMrN9PV9Z+RpahmFo7FFt9entJ2rmuQOUGuvQ9j3lmvLmjxr7xDf6Ym2OTJMQFgAAAMEVabfq2SuGKDnGobXZxZry5ip5WQwAAECjIHxtZXaXuv03eeqeSs/XcGCzWnTpsI766s6T9aff91KsM0Jrs4t11cvLdNEzi7V8y+5QlwgAAIAWrl1CpJ65YrDsVos++SVHsxeuC3VJAAC0CISvrUxNy4GMxEhFOyJCXA1qi7RbdeOo7vrmTyfr+pO6yhFh0dLNe3T+04t1zT+W+b92AAAAQDAM7tRGM88bIEl6YuE6ffTTbyGuCACA5o/wtZXJ4mZbYS8hyq5pp/fRl3eO0iXHZshqMfT5mhyNmf217njzR23fw00QAAAAEBwXDO6ga47vIkm6461V+nlHYYgrAgCgeWsW4evcuXPVuXNnOZ1ODRs2TEuWLDno/gUFBbrpppvUtm1bORwO9ezZUx9//HETVRveMrOrb7ZF+Br22sZHatZ5R+mTySfq9P7pMk3p7RXb9btHv9L9H/yq3aXuUJcIAADQYjDn2GvaGX10Us8UVVR6dd0ry5RX7Ap1SQAANFthH77Onz9fU6ZM0fTp07VixQoNHDhQY8aMUW5ubp37u91unXrqqdq8ebP+/e9/KzMzU88995zat2/fxJWHp3U5vpttEb42H91TY/T05YP13k0jNbxrktwer178bpNOfHiRnli4jrvRAgAAHCHmHIGsFkNPXHK0uqZEa2dhhf7wz+VyVXlCXRYAAM2SYYb57dSHDRumoUOHas6cOZIkr9erjIwM3XLLLZo6dep++8+bN0+PPPKI1q5dK5vNdljXLCoqUnx8vAoLCxUXF3dE9YcT0zQ16P7PVFheqY9vPUF927Wc99ZamKapb9bl668L1uqXnUWSpOQYh249pbsuHtpR9oiw/30KAARdS/0+DiB4mHPUbWNeic6Z+52KK6p04eAOeviCo2QYRqjLAgAg5BryfTyskxq3263ly5dr9OjR/jGLxaLRo0dr8eLFdR7zn//8R8OHD9dNN92ktLQ09e/fXzNnzpTHc+Df1LpcLhUVFQVsLVFusUuF5ZWyWgx1TYkOdTk4DIZh6MSeKfrg5uP1xCVHq1NSlPJLXLr3/V90ymNf6q53V2vOF+v0zort+mHjLm3dVSZ3lTfUZQMAAIQt5hwH1jUlRnMuPUYWQ3pr+Xa9+N3mUJcEAECzE9a3u8/Pz5fH41FaWlrAeFpamtauXVvnMRs3btQXX3yhyy67TB9//LHWr1+vG2+8UZWVlZo+fXqdx8yaNUszZsxo9PrDTU2/185JUXLarCGuBkfCYjF09sB2Or1/ut5Yuk1//3ydtu0u1+v/27rfvoYhpcQ41DYhUu0TnGobH6l2+zxPjrGzigEAALRKzDkO7qSeKbrrjD568KM1euijX9UjNUYn9kwJdVkAADQbYR2+Hg6v16vU1FQ9++yzslqtGjx4sHbs2KFHHnnkgD8ITZs2TVOmTPF/XFRUpIyMjKYquclk5XCzrZbGZrXoiuM66fxj2uu/q7O1ZXeZdhaU67fCcu0sqNCOgnK5q7zKLXYpt9ilH7fVfR57hEVt451qVx3GtktwVj9Gql2873m0o8X9cwEAAHBYWtuc4+rjuygzu1hvLd+um19fofduGqmuKTGhLgsAgGYhrNOU5ORkWa1W5eTkBIzn5OQoPT29zmPatm0rm80mq3Xvys4+ffooOztbbrdbdrt9v2McDoccDkfjFh+GCF9brih7hM4f3GG/cdM0tbvUrZ0FFdpZWK6dBdVbYYX/eW6xS+4qr7bsKtOWXWUHvEZ8pE1t451qXx3Ktk3wPfetnnUqLc4pmzWsO5kAAADshznHoRmGoQfP7a+N+aVavmWPrnllmd69caTiIw+v3y0AAK1JWIevdrtdgwcP1sKFCzVu3DhJvt8yL1y4UDfffHOdx4wcOVKvv/66vF6vLBZfEJSVlaW2bdvW+UNQa5KZUyJJ6pVO+NpaGIahpBiHkmIcGtAhvs593FVe5RRVVK+Y9a2WrXleE9AWVVSpsLxSheWVWlvdvmJfFkNKjXXWuWq2ZmsTZaO9AQAACCstcs7x249S24GNekpHhFXzLh+ss+d8q415pbr1Xyv14pVDZbXwsx0AAAcT1uGrJE2ZMkUTJ07UkCFDdOyxx2r27NkqLS3VpEmTJEkTJkxQ+/btNWvWLEnSDTfcoDlz5ui2227TLbfconXr1mnmzJm69dZbQ/k2Qs7rNbWOla+ogz3CoozEKGUkRh1wn+KKylphbPWjfyVthbILK+T2eJVdVKHsogqt2FpQ53mcNktAa4O28ZG+1bM1gW18pCLt9CMGAABNq0XNOdZ8IM2/XBpytTRmpmRzNtqpU2Idem7CEF0w73t9lZWnv/x3je4e27fRzg8AQEvU4PDV7XarqqoqGLXU6dxzz1VeXp7uvfdeZWdna9CgQVqwYIG/If7WrVv9v22WpIyMDH3yySe6/fbbddRRR6l9+/a67bbb9H//939NVnM42lFQrjK3R3arRZ2TDhyyAXWJddoU67QdMLj3ek3ll7q0s6BCvxWUa8c+K2d3FlYor9ilikqvNuaXamN+6QGv1SbKpuQYh+IjbYqLtCnOGVH9aFNcZITinLZar+0di3VGKIK2BwAAhL2mnk9EREQccjXq+PHjW86cI3+d73HZC9KOZdKF/5ASuzTa6fu3j9ejFw7Uza+v1HPfbFKv9DhdUEf7KwAA4GOYpmnWd2e3261fflkir7ckmDUFsFhi1K/fsU365ztFRUWKj49XYWGh4uLimuy6wbRwTY6u/scy9U6P1YLJJ4a6HLRCriqPsgsr9q6crdV79rfCcu3YU65St+eIrhFtt9YZzAaEuHUFuU6bYpwR/Nkc0EK0xO/jQEvRWuYT9RHUf6vWfS69c61UvltyxEvjnpL6nNmol3js00w98cV62a0W/eu64zS4U5tGPT8AAOGsId/HG7TytaqqSl5vibp0scvpDH6z+IoKlzZtKlFVVVXY/bDU3GRWtxyg3ytCxRFhVaekaHVKiq7zddM0VVRRpd8Ky7W7xK2iikoVlVdVP1b6+876nge+VhPalro9KnV7tLOwosH1GYYU46gjrK0OauP9z+t+LdoeIQvhLQAAB8V8oon0GC394RvprUnS9iXS/Muk4TdLo++TrI1zk6zJo3sqM6dYn/ySo+tfXa4PbhmptvGRjXJuAABaksPq+ep0OhQV1Xi9gw7O3UTXadmysun3ivBmGIbiI22HddfcSo9XxRVV+wWzBwpr9w1yKyq9Mk2puKJKxRVV2lFQ3uAaLIavPYN/ta3TpsRou/8GZO0TItW+TaQ6JEQpLjKCG48BAFo15hNNIL6DNOlj6fP7pMVzfNu2JdKFL/leO0IWi6HHLhqk85/+Xmuzi3XdK8v15vXD6d8PAMA+wv6GW2gcWTm+P+0ifEVLZLNalBhtV2L04a1ocVV5/OFtYXU4e+Agd5/Xyivl9njlNaXC6uOlg4e3MY4ItUtw+gPZ9glR1Y9OtU+IUmqsg1W0AADgyFlt0piHpE4jpHdv8K2CnXeCdN6zUo9Tj/j00Y4IPTdhiM6Z+51W7yjUn97+SU9cPIhfMgMAUAvhaytQ5fFqfZ4vfO1F+ArsxxFhlSPGquSYhv/5o2maclV5/YFsYa0Vtvklbu0s8PWz3VF9I7LdpW6VuKqUlVPi/6XIvmxWQ23jfatl2/lXzNYEtZFqm+CUI4JVJQAAoJ56j5Wu/0p660rpt1XSaxdIJ9whjbpLsh7ZlDAjMUpPXXaMLn/+f/rgx53qnR6rm07u3ihlAwDQEhC+tgJbdpfJXeVVpM2qDm3owwQ0JsMw5LRZ5bRZlRp36D+fLHNXaWdBhS+M3VOuHQVlvo+rA9rsogpVekxt3V2mrbvLDnielFhHrVYGvsd28dUBbZtIxTkbp58bAABoIRK7SFd/Kn1yl7T0eembv/naEJz/vBSbfkSnPq5rku4/p7/uene1HvkkUz1SY3RavyM7JwAALQXhayuwt99rDH/KDIRYlD1C3VNj1D01ps7XqzxeZRdVVAe0Zf5Qdvuect8q2oJyVVR6lVfsUl6xS6u2FdR5nlhnhC+crbVitn0b30raDgmRSo6htQEAAK1OhEMa+zep43Dpg9ukzd/42hBc8ILU5cQjOvWlwzpqbXaRXlm8RbfPX6W3bxyh3ukHv/szAACtAeFrK1Dzp809aDkAhL0Iq0Ud2kSpQ5soSYn7vW6apnaXumutnC3f73lBWaWKK6q0NrtYa6t/+bIvu9Xi6zvbplZ7g1o3BUuPd8oeYQnyuwUAACEx4AKp7UDpzYlS7i/SK+f4WhCccIdkOfzv//ec2Vfrc0v0/YZduvaVZXr/puMPuyc/AAAtBeFrK5CV4wtf6PcKNH+GYSgpxqGkGIeO6pBQ5z6lrirtLCjX9upQduc+AW1OUYXcHq827yrT5l11tzYwDCm1urVBu+qVsknRdiXG2JUY5bu5WVKMXYnRDsVH2mRlFS0AAM1Lcg/pms+l/94prfyntOhBaeti3824opMP65Q2q0VzLz1G4576Tlt2lenG15br1auHyWblF7oAgNaL8LUVyKwOX3umE74CrUG0I0I90mIPuNq90uNVdmFF4IrZPeXaWbj3Y1eVVzlFLuUUubRia8FBr2cxpITqQDYxujqcjbH7wtp9tqRoh9pE27hhGAAA4cAeJZ0zV+o4QvroDmnDQl8bggtfkjoed1inbBNt13MThui8p77XDxt3a8YHv+jBcQMauXAAAJoPwtcWzlXl0ab8UkmsfAXgY7NalJEYpYzEqDpfN01T+SW+1gY7q7ddpW7tLnFrV6lbe8rc2l3q1q4Sl4oqquQ1pd2lvrH6inFE1BHM+h7b1HpeE9bGOCJkGKyuBQAgKI6+TGo3yNeGYNc66aUzpFNnSMNv9v05TAP1TIvV7PGDdO2ry/TPH7aqV3qcrjiuU+PXDQBAMxC08PUf//hQyckJGjv2eEnSn/70dz377Lvq27er/vWvh9SpU9tgXRq1bMovlcdrKtYZobQ4R6jLAdAMGIahlFiHUmIdGpSRcNB9Kz1efxi7u8St3f5gtnqsrHq8dG9w6/GaKnFVqcRVpa276257sC+71bJfWFsT2AaEtTF2tYmyKyHKTisEAGjmmE80sbR+0nWLpA8mSz//W/r0z9KWxdK4uVJkmwafbnTfNN05ppceXpCpGf/5Rd1SojWi2+G1MwAAoDkLWvg6c+aLevrpaZKkxYt/0ty5b+nxx6foww+/0e23P6Z33nkkWJdGLZnZe/u9smoMQGOzWS1KjXUqNdZZr/1N01RReZV2lbr8q2X9wWyt57VfK6/0yO3xKruoQtlFFfW6TkArhJrHGLvSYp0BNxpLj3fSAgEAwhTziRBwxErnPy91GiEtmCplfiQ9s1q68B9S+2MafLobTuqmzOxivb9qp256bYXev+l4dUyq+y9vAABoqYIWvm7blqPu3TMkSe+996XOP/93uu668zRy5ECNGnV9sC6LfWTR7xVAGDEMQ/FRNsVH2dQ1pX7HlLs92lXq0p7Syv1C27qC28Lyyga1QkiJdahdQqTaJzj9Nxjzfex7bBNl45dXABACzCdCxDCkoVdL7QdLb02U9myWXhwjjZkpDb2mQW0IDMPQX88/SpvzS/Xj9kJd+8oyvX3jCMU46H4HAGg9gvZdLyYmSrt2Fahjx3R9+ukPmjLlMkmS0+lQebkrWJfFPjKzSyTR7xVA8xVpt6qDPUod6vkXj5UerwrKKqvDWF9ou7vUpfwSt3KKKvy9bHcUlKui0qu8Ypfyil36cdsBrm+zql2CMyCQrXles3rWHsFdnAGgsTGfCLF2g6TrvpLev0la+6H08R+lLd9JZz0hOePqfRqnzapnrhiis+d8q8ycYk1+Y5WevWKwLLQHAgC0EkELX089dZiuueZBHX10L2VlbdUZZ4yUJP3yywZ17kx/pqayLte38rVHWkyIKwGApmGzWvw9a6UD/+LJNE0VlFVqR3UQu9O/VWh79fO8YpfKKz3akFeqDXmldZ7HMKSUGMfeQLZNpNrFOwNC2gRWzwJAgzGfCAORCdL4f0o/PC19do/0y7vSbz9JF70ipfev92nS45165orBGv/sD/p8TY7+9lmm7hzTO3h1AwAQRoIWvs6d+3/685+f0rZtOXr77YeVlJQgSVq+fK0uuWRMsC6LWsrce29mw8pXAAhkGIbaVN+wq3/7+Dr3cVV5lF1Ys1q2Qjv2VAe0hXsD24pKr3KLXcotdmnVtoI6zxNlt9YKY51qF1/9vLr3bFocq2cBYF/MJ8KEYUjDb5Q6DJHeulLavUF6/hTpjEeloy+vdxuCozu20V/PH6Db5/+ouYs2qGdarM4Z1D64tQMAEAaCFr4mJMRqzpz/2298xgz6MzWV9bklMk0pOcaupBhHqMsBgGbHEWFVp6RodUqKrvN10zS1u9TtC2ZrrZ7d29qgQvklLpW5PVqfW6L1uSV1nscwpNTq3rPtEiLVoVZ7g3bVvWjjI1k9C6B1YT4RZjKOla7/Rnr3emn9Z9J/bpa2fC+NfVSy1/19cl/nHt1Ba7OL9cxXG/Wnf/+krskxGtCh7l+AAgDQUgQtfF2w4HvFxETp+OMHSZLmzn1Tzz33nvr27aK5c/9PbdrUv08QDk9mdvXNtlj1CgBBYRiGkmIcSopxHHDyWFHpWz27s6Dc386gpr1BTVDrqvIqp8ilnCKXVm4tqPM80bVWz6bFOdQmyq74KJsSIu1KiLL5tlrPI21WwloAzRrziTAUnSRd+qb03ePSFw9KP74u7VwpXfQPKaVXvU7xpzG9tS6nRF+szdW1ryzTf24eqdQ4Z5ALBwAgdIIWvt5559/117/eIklavXq97rhjtqZMuUyLFi3TlCmP66WXpgfr0qi2rnqFFeErAISO02ZV5+RodU4+8OrZXaXuWqtmA9sb7CwoV36JW6Vuj9bllvj/bT8Uu9VSHc5WB7NR9oDn8ZH7B7YJUXZF2wltAYQH5hNhymKRTrhD6nCs9PbVUt4a6dmTpbP+Lh114SEPt1oM/f3iQTr3qe+1PrdE1726XG9cd5ycNmsTFA8AQNMLWvi6adNO9e3bVZL09tsLdeaZx2vmzJu0YsVanXHGbcG6LGph5SsAhD/DMJQc41ByjENHdUioc5+KSk/Aatnc4goVlFWqoLxSBWWVKix31/rYrUqPKbfHq7xil/KKG3ZH8AiLoYQoW3U46wtsa1bYtqkOaeNrB7mRvhW4sY4I7lwNoFExnwhzXU6Q/vCtL4Dd9LX0zjXS1u+lMbMk28FXssY6bXp+whCdM/c7rdpWoLveXa2/XTiQX/4BAFqkoIWvdrtNZWUVkqTPP1+iCRPGSpISE+NUVFS/VTs4Mlk5vvC1V3pMiCsBABwJp82qrikx6ppy6H/PTdNUmdvjD2ILa4W0BeW+j/eU7Q1rC6vH95RVyl3lVZXXVH6JW/klbkml9a7RYmifsHbfFbbVYe4+q3BjnTZZCW0B1IH5RDMQkypd8Z705V+krx+Rlr0obV/ma0OQ2PWgh3ZOjtZTlx2jCS8u0TsrdqhPepyuPfHgxwAA0BwFLXw9/viBmjLlcY0cOVBLlvyi+fNnSZKysraqQ4e0YF0W1QrLK/Vboe+H1R6sfAWAVsMwDEU7IhTtiFD7hMgGHVtR6VFBrXA2cEXt3o/3vu4bL6/0yGtKu0vd2l3qbtA1rzuxq+46o0+DjgHQOjCfaCYsVul3d0sdh0nvXCdl/yQ9c5J0zlyp79kHPXRk92TdM7aP7vvgV8367xp1T4vRyb1Sm6hwAACaRtDC1zlz/k833vgX/fvfC/X001PVvr3vm+h///udfv/74cG6LKqtz/Wtem0b71Sc0xbiagAAzYHTZlV6vFXp8Q278UlFpUdF5XtD2j3+Fbf7r7AtKKtehVvm62ObEMX3KAB1Yz7RzHQfLV3/jfTvq6RtP0hvXiEdd6M0eoYUYT/gYRNHdNba7GK9sXSbbn19pd69aaS6p/KXewCAliNo4WvHjun68MPZ+40//vgdwbokasnM5mZbAICm4bRZ5bRZG3y3aneVV17TDFJVAJo75hPNUHx76coPpYX3S98/If3wlLRtiXThy1JCRp2HGIah+8/prw15JVq6eY+ufWWZ3rtxpOL55RwAoIUIWvgqSR6PR++996XWrNkkSerXr5vOPvtEWa3cyTLY9vZ7JXwFAIQne4Ql1CUACHPMJ5ohq0067QGp43DpvT9IO5ZJz5wgnfus1PO0Og+xR1j09OWDdc6c77Qpv1Q3/2uFXrpyqCKsfJ8AADR/Qftutn79NvXpc6EmTJiud95ZpHfeWaTLL79H/fpdpA0btgfrsqiWme0LX1n5CgAAgOaI+UQz1/sM6fqvpXZHS+V7pNcvlD6fIXmq6tw9Ocah5yYMUaTNqm/W5Wvmx2ubuGAAAIIjaOHrrbc+om7dOmjbto+0YsVrWrHiNW3d+qG6dGmvW299JFiXRbV1uTXhK/2SAAAA0Pwwn2gB2nSWrvpEOvY638ffPia9crZU9Fudu/dtF6fHLhooSXrxu016c+m2JioUAIDgCVr4+tVXK/Tww7cqMTHeP5aUlKC//OVmffXVimBdFpLyS1zKL3HLMESzegAAADRLzCdaiAiHdMYj0gUvSfZYact3vjYEG7+sc/fTB7TV5NE9JEl3v7daSzbtbsJiAQBofEELXx0Ou4qLS/cbLykpk91O8/Rgqun32jExSlH2oLb1BQAAAIKC+UQL0/886bovpbT+Umme9Mo46cu/Sl7Pfrve+rseOmNAuio9psY/u1hXvPA/ffjTTrmq9t8XAIBwF7Tw9cwzj9d11z2k//3vZ5mmKdM09cMPq/WHP8zS2WefGKzLQlIW/V4BAADQzDGfaIGSu0vXfC4dfYUkU/pypvTP86WSvIDdLBZDj144UKf2TZNpSt+sy9fNr6/UcTMX6v4PfvXf3wIAgOYgaOHrE0/cqW7dOmj48ElyOkfI6RyhESOuUvfuGZo9+45gXRaSsnJLJNHvFQAAAM0X84kWyhYpnTNHGjdPskVJGxf52hBsWRywW5Q9Qs9NGKKv7zxZt/yuu9LjnNpTVqkXv9ukMbO/1ri53+mNJVtV4qr7Bl4AAISLoP1NekJCrN5//zGtX79Na9ZskiT16dNF3btnBOuSqMbKVwAAADR3zCdauEGXSO0GSW9OkPKzpJfHSqOnS8NvkSx71wh1TIrSHaf10uTRPfV1Vp7eWLpVC9fkatW2Aq3aVqD7P/xVZx7VVuOHZuiYjm1kGEbo3hMAAHVo1PB1ypTHDvr6okXL/M8fe2xKY14a1UzTVGZ1z9de6YSvAAAAaD6YT7QyqX2kaxdJH06WVr8lfXavbwXsuKekqMSAXa0WQyf3TtXJvVOVV+zSOyu2a/6ybdqYV6o3l23Xm8u2q3tqjC4emqFzj26vpBhHaN4TAAD7aNTwdeXKzHrtx28jgye7qELFFVWKsBjqmkzbAQAAADQfzCdaIUeMdN5zUqeR0n//T8r6r/TMSdKFL0sdBtd5SEqsQ9ef1E3XndhVy7bs0fyl2/TRT79pfW6JHvxojf66YK1O7Zumi4Zk6IQeKbJa+O8FABA6jRq+Llr0TGOeDochK8fX77VzcrTsEUFr6QsAAAA0OuYTrZRhSEMmSe2Pkd6cKO3ZJL04RhrzkHTsdb7X6zzM0NDOiRraOVHTz+qrD378TfOXbtWP2wv18epsfbw6W+3inbpwSIYuHNJBHdpENfEbAwAgiD1fERo1/V570e8VAAAAQHPSdqB0/VfS+zdJaz6Q/vsn6cd/Se2HSOkDpPT+Umpf30279hHrtOnSYR116bCOWvNbkeYv3aZ3V+7QzsIK/X3hOj3xxTod3z1Z44dm6NS+aXJEWEPwBgEArRHhawtT0++Vm20BAAAAaHac8dJFr0r/myd9+mdp50rfVsOwSEk9fEFsWn8p/Sjf85g0/wrZPm3jdN/Z/TT19N769NcczV+6Vd+t36Vv1uXrm3X5ahNl03nHdND4oRnMmwAAQUf42sJk+W+2Rb9XAAAAAM2QYUjH3SD1Huu7AVf2T1LOz1L2z1JZvpSf6dt+fnvvMVHJe1fHpg2Q0gfImdxDZw9sp7MHttPWXWV6a/k2vbVsu7KLKvTCt5v0wrebdHTHBI0fkqEzB7ZTjIPpMQCg8fHdpQXxek2tq+752oPf4AIAAABozhI6+raB430fm6ZUnF0dxK7e+7hrvS+U3bjIt9Ww2qWU3lL6UeqY3l939OivySOP0dfbKvXG0q1auCZXK7cWaOXWAt3/4a8666h2umhoho7pmMBN3QAAjeawwteKCldj1xHS67QU2/eUq7zSI3uERZ0SaSYPAACA8MR8AofFMKS4tr6tx6l7x91lUt4aXxCbXRPM/iK5i32rZrN/8u9qlXRyfIZOTh+g0hN767vitnplU6y+2x2j+cu2af6ybeqRGqPxQzN07tHtlRTjaPr3CQBoURoUvkZERMhiidGmTSWS3EEqKZDFEqOICBbo1kdNv9fuKTGKsFpCXA0AAAAQiPkEgsIeJbUf7NtqeL1SwZZaK2R/lnJWSwVbpcJtUuE2RetjnSbpNEmemGhttXXR96Xt9POuDH34cSf9fUFHndi3ky4amqHjuyfLamE1LACg4Rr0U4jdble/fseqqqoqWPXsJyIiQna7vcmu15zt7fdKywEAAACEH+YTaDIWi5TYxbf1PXvveHmBb1Vszs/Vq2J/lnLXyFpVqi5VP6uL5Wepeh2LxzS0OTNda9Z20nOO7krrMUTDRpyodh26+m/uBQDAoTT4V8B2u50fXsJUTfjaI42bbQEAACA8MZ9ASEUmSJ1H+rYanipp17q9q2Or2xdYS3PVzfhN3fSbVPWDtOaf0hqpyBIvd1JfJXQ9WhHtBkpp/aWUXpLVFrK3BQAIX/z9TQuSmV298pWbbQEAAABA/VgjpNQ+vk0X7h0vzpFyVqty52plZy2Vkb1a6ZXbFectlPIW+7YaFpuU2ltKGyCl95fSB/hC2ajEJn87AIDwQvjaQlR6vNqYVypJ6kn4CgAAAABHJjZNik2TrftoZZzoG9qas1tfffu1tvz6g9q7NqqvZYv6GFsV5y2rXjG7Wvqx1jni2vuC2OQekiNecsRKjpjqx1jJHhs4Zo+RLNaQvF0AQHAQvrYQW3aVyu3xKspuVfuEyFCXAwAAAAAtTse0RF1x/jhVjTtbX6/L04tLt2nhmhylm3nqY2zRwIitGpWQqx7mZjmKt0pFO3xb1oL6X8QWvTec9YeysfUbc8T5AlxHrGSPpjctAIQBwtcWIiunRJLUIy1WFu7CCQAAAABBE2G16He90/S73mnKK3bpnRXbNX9pZ32WX6pHc337DEq16OoeZfpdQq6iy7ZLrmLf5i7Z+7z25q30HVhZ6ttKso+wSqPWCtuYA4e0/vF9x2oda4skyAWAw0T42kLs7ffKzbYAAAAAoKmkxDp0/UnddN2JXbVsyx69sWSbPlq9U6tyvbol1ymbtZNO6jlU/dvHqWeXWPVMi1XnpChFWC2BJ6py7R/I1hXUukskV1H1xyX77FfkGzM9kszqj4uO/E0aVl8Q64yT2nSWknpISd197RSSuksJHWmXAAAHQPjaQmTl+MJX+r0CAAAAQNMzDENDOydqaOdETT+7rz74cafmL92mn7YX6vM1Ofp8TY5/X7vVoq4p0eqZFqueaTHVj7HKSEySNTr5yAoxTamy/CBBbe0wt1Zo6w9wSwKDXpm+MLeiwLcVbJU2fR14TatdSuzqC2Jrh7JJPaTopCN7PwDQzBG+thCZhK8AAAAAEBbinDZdNqyTLhvWSb/uLNJ36/OVlVOsrJxircstUZnbo7XZxVpb/ReMNZw2i7qnxqhnaqx6pvuC2R6psWqfEFn/9nKGIdmjfJvSjuyNeL2+Fgg1oWz5bmn3RmnXeil/ne9x1wbJ45Ly1vq2fUW22RvEJnff+zyxq2RzHll9ANAMEL62ABWVHm3ZVSZJ6pVO+AoAAAAA4aJvuzj1bRfn/9jrNbWjoLw6jC3xh7Lrc0tUUenVzzuK9POOwFYB0XaruqfFqmdqjHqlx6pHWqx6pcUqLc4hI5i9WC2Wvb1fa3Q8LnAfr1cq3FYdxNYOZdf7xsv3SNuX+rYAhpSQsX8Lg6TuUlx737UBoAUgfG0BNuaVyuM1FR9pU2qsI9TlAAAAAAAOwGIxlJEYpYzEKJ3SZ+/KVI/X1NbdZb4wNrtYWbklWpdTrA15JSp1e/TjtgL9uK0g4Fyxzgh/y4La7QuSY+zBDWUD35DUppNv635K4GvusuqVsuuk/OpAtua5q9DXwqBgq7RhYeBxEZHVQWy36lC2JqDtLjnjm+Z9AUAjIXxtAWr6vfZKi226b7AAAAAAgEZjtRjqkhytLsnRGtMv3T9e6fFqy65SZWaXVLctKFZmdrE27ypTcUWVlm/Zo+Vb9gScq02Urc5Qtk20vWnflD1KSu/v22ozTak03xfE7rtadvcmqapcylnt2/YVnbJ/C4PkHr4bgVltTfK2AKAhCF9bgJp+rz3SYkJcCQAAAACgMdmsFnVPjVX31FiNVVv/uKvKo415pb5ANqdEmTnFWpdTrC27y7SnrFL/27Rb/9u0O+BcKbEOfx/ZXjU9ZdNiFeds4tDSMKSYFN/WaUTga54qqWBLrVB2na+vbP46qSRbKs3zbVu/3+ecVl8AW7t9Qc3zmDTfNQEgBAhfW4B1NStf6fcKAAAAAK2CI8KqPm3j1KdtXMB4udujDXm+VbKZ1cFsVk6xtu8pV16xS3nFLn23flfAMW3jndV9ZH1hbM+0WPVIjVG0IwSRgTXC124gqZvUc0zgaxVF0u4N+7QwqA5nK0t9r+3esP85HXHV56xeKZvUzRfI2qMke4xki5Ls0b7NaieoBdCoCF9bgJqVrz3TCF8BAAAAoDWLtFvVv328+rcP7I1a4qrS+twSXz/ZHF9P2azsYmUXVei3Qt/2dVZewDEd2kQGtC/okeq7yVebaLts1hDcEMsZJ7U72rfVZppS8W+B7QtqnhdskVxF0s6Vvu1QLBGSrTqItdeEsvsEtPbo6o9j9u4TcEwd+0c4CXWBVorwtZkrdVVp2+5ySYSvAAAAAIC6xTgiNCgjQYMyEgLGC8srtS6nWFnVK2Szqp/nl7i0fU+5tu8p1xdrc/c7X5wzQkkxDiVG29Umyq6kaLsSY6ofq7ekaId/zGmzBu/NGYYU1863dT0p8LUql7Rnc60WBut9K2XLdkvuUt+KWXep5HH79vdW+W4G5ips5Bot1QHtvmHtQQLb+gS8tijfTc8AhK1mEb7OnTtXjzzyiLKzszVw4EA9+eSTOvbYY+vc9+WXX9akSZMCxhwOhyoqKpqi1Ca3LrdEkpRc/U0PAAAAQMMx50BrFR9p05DOiRrSOTFgfHepu7qfrK99QVZOiTbmlWh3qVteUyqqqFJRRZU25ZfW6zpRdmt1IOsLZtv4nzv2Bra1wtsYR0Tj3FA6wiGl9PJtB+OprA5jy3yPNVtlmeQukdxlgWGtu3rcv/8B9qnyLZaS6ZXcxb6tsUU4JVukFBHpe7RFVT869z73v1bzel2v1YxH1v2atVlESEDYCfv/c+bPn68pU6Zo3rx5GjZsmGbPnq0xY8YoMzNTqampdR4TFxenzMxM/8eN8g92mMry93vlZlsAAADA4WDOAewvMdqu47om6biuSQHjHq+pwvJK7S51aVeJW7tL3dpV6nvc+9yl3aWV1Y9uVXpMlbk9KnP7VtLWh91q2buCNmbvatrEqNohrcMf6MZH2mSxHMH/h1abFJng2xqT19OAQHfffQ5xTI2qCt+mPY1b+74sEbVC2Vrh7b6hbcBr+4S+db5WPW61+wJeq12y2HxfE/5tRQsQ9uHrY489pmuvvdb/m+V58+bpo48+0osvvqipU6fWeYxhGEpPT2/KMkMmK5t+rwAAAMCRYM4B1J/VYviD0O51/24igGmaKnZVaXdJ7ZDW5XteZ3jrUkWlV26PV9lFFcouqt+KcqvFUJsoW2DLg1rhbe3WCDUhbkRT9K21WCVHrG9rTF6vb1VtzeraynJfQFtZUf1Y7gtka55X1t6nvNYx+7xWVRG4X2W5JLP6mlW+/rmuosZ9Lwdj2SeMrdkstrrDWmv1eM1x++3f0OPre41aH0c4CI0RIKzDV7fbreXLl2vatGn+MYvFotGjR2vx4sUHPK6kpESdOnWS1+vVMccco5kzZ6pfv34H3N/lcsnlcvk/Lipqwn9IjlDNzbZ6Eb4CAAAADcacAwguwzAU57QpzmlT5+Toeh1T7vZoV/Wq2f1DWldgYFviVrGrSh6vqfwSt/JL3PWsS0qItCkpxqHkGLvvMdr3mBTjC29rxpNi7IptrDYIjcVi2dsbNphM09c3d79gtnawW7ZP0FtRR8hbc8xBAmBv5f7X91b5tubEqAnc43yPzri9AXzNmCOu7nFn/N4xbtLWYoR1+Jqfny+Px6O0tLSA8bS0NK1du7bOY3r16qUXX3xRRx11lAoLC/Xoo49qxIgR+uWXX9ShQ4c6j5k1a5ZmzJjR6PU3hZq2Az0IXwEAAIAGY84BhJ9Iu1Ud7FHq0CaqXvu7qjzaU1qpXaUu/2NAG4SSvatqd5e6VVBeKdOU9pRVak9Zpdbvfz+x/dgjLAcIZ33Pk2LsSo5x+O/HYo9oITfBMozqlgLO4F/LNH1Bq8ft68HrqfQFsh635Kke91bufa3Ojxt6fCPtb3prvQ+PVFHg246ExbY3iHXGBQa3Bxt31v44Torg/kChFtbh6+EYPny4hg8f7v94xIgR6tOnj5555hk98MADdR4zbdo0TZkyxf9xUVGRMjIygl7rkSosq1ROke+35z3T6PkKAAAANIXWNOcAmgNHhFXp8Valx9cvIKzyeFVQXqldJW7tKnEpr8TXv3ZXdR/b/FrPd5W4VOr2yF3l1c7CCu0srF8bhDhnhJJrBbVJNatr91lVmxxjV5zzCPvVthSGsffP+psbr2dvQOsulVzF1Vuh77GiqNZYdeuGOsern8v0Bbvlu33bkbA66g5lAwLcmhW4tVbe2qNrtV6w1f3c0kJ+yRBkYR2+Jicny2q1KicnJ2A8Jyen3v2VbDabjj76aK1fv/6A+zgcDjkcjiOqNRSycn2rXtsnRCrW2Qz/cQIAAABCjDkH0PpEWC3+VarSof+KtKYNQn51GLurxK38WuHsrlL33tdK3fJ4TRVVVKmookob80sPef6I6j66e8PZvStsk2sFtjUfO23WRvgsoFFZrL7N5vSFmWp7+Ofyen03VwsIZIsOENQeaLzYdw5J8rikMpdUlt8obzWAYa0jlD1AUFvv1w/yPOIwj7NEhLSFQ1iHr3a7XYMHD9bChQs1btw4SZLX69XChQt188031+scHo9Hq1ev1hlnnBHESkMj03+zLVa9AgAAAIeDOQeAQ2lIGwSv11RheWWtsNa9X3Bb83F+iUvFFVWq8prKLXYpt9h1yPNLUrTdGtD+IKn6pmJJMXufJ0b7gto2US2oBUJrYbH4AlxnnKT2h38er6fuULaisI6xosBA11Xsu5mbvw1DrbYLtZkeX+/eqvIjestBN2amNPymkF0+rMNXSZoyZYomTpyoIUOG6Nhjj9Xs2bNVWlrqvxPphAkT1L59e82aNUuSdP/99+u4445T9+7dVVBQoEceeURbtmzRNddcE8q3ERQ1/V570u8VAAAAOGzMOQA0FovFUJtou9pE29U99dD7u6o8vn601WHsvu0P8ktctVoguOX2eFXq9qh0d5m27i6rV01xzgh/MFuzwnbfkDax5uMouyKshLUtgsUqRSb4tsbi9e4fyNZ+XuWqe7zBz490331+kWENbd/bsA9fx48fr7y8PN17773Kzs7WoEGDtGDBAn9D/K1bt8pSq8fEnj17dO211yo7O1tt2rTR4MGD9f3336tv376hegtBQ/gKAAAAHDnmHABCxRFhVdv4SLWNjzzkvqZpqthV5W93UBPO1txcrOa5L8B1a3epS15T/hYIm+rRAkGSEqJsvlA2em8oW9MKYe/HvpW3baLsstKvtvWwWCSLQ4oI8zY6plndh7c6lA1xvYZpmmZIKwhDRUVFio+PV2FhoeLi4kJdTp1M09QxD3ymPWWV+vCW49W/fXyoSwIAICw0h+/jAMC/VQCCrXYLhJpAdlepL7jdG9L6Xttd6tbuMrcamhAZhtQmyu5fVZscU3dgmxxjV2K0QwmR3FwMLUNDvo+H/cpX1C2/xK09ZZUyDKl7Kj1fAQAAAADAXg1tgeDxmiooqwlofcHs7uqbie2uHeBWh7d7yiplmvKvvK1XTYZ84ew+q2oTox2KcUYoxmFVlD1CMY4IRTsiFO2w+p/HOCLkiLDICOGNk4DDQfjaTNW0HOiUGMWdDgEAAAAAwBGxWozqG3k5pLRD71/l8WpPmW9l7e4St/JL3dpd4jrAClu3Cssr5TVV3S6hfmFtXTVG2/cGslEOX2AbHRDYVo/VCm2jHRGKtlsDP3ZY5YggT0HwEb42U/R7BQAAAAAAoRJhtSgl1qGU2Pr106z0eLXHv5LWXasdgku7SytV6qpSqatKJa4qlbqrVOry+J67qlTm9kjyrc6t6WHbGGxWozqYrQllrf6PDxTi1h7z7bc3DLZxszLUgfC1maoJX3ulE74CAAAAAIDwZrNalBrnVGqcs8HHer1mQCBb5q4OaV2evYGtP7ytHnNXB7c1Ia57b7hbUemVJFV6TBWUVaqgrLJR3qM9wqJIm1VOm0WOiMBHp80qR4RFjupHp80qZ4RVDpul1mP1fgFjvkffefYeW/OazWrQiiHMEb42U5nZrHwFAAAAAAAtn8ViKNZpU6zT1ijnq/J4Ver2BK62rRXsBoS4NcGue+9YzXiZ23eMu8oX5rqrvHJXeVVY3ihl1ovFUL2CXn/ge5BguPZxkTarouy+HrxRdl9rh0i7VfYIVvc2FOFrM2SaptbllEgifAUAAAAAAGiICKtF8ZEWxUc2TpjrrvL6V+NWVHpVUemRq8ojV6VXFbUeKyq9clV6VFHlrTXmkavK63901fq4otIrV1XgY81+NbymVF7pUXmlR1LjrOA9GJvV8AeyAeGswxfORtcaq/16VHXf3cjqILdmLMpmVZTDKru15d5MjfC1GfqtsELFripFWAx1SY4OdTkAAAAAAACtlj3CInuEXQlR9ia5nmmavqC2AWFt7ceaQPiAAXGVR2Vuj8rdvscyd5UqPaYkX6uGwvJKFZY3btAbYTH2CWatirJFKMqxd9WtL7ytFeI6IvwBb+2xSNve1xwRoQ91CV+boczqfq9dU6JZ7g0AAAAAANCKGIZR3ULAKjXS6t1DcVd5Ve72qNRd5Q9kAx5ddbzm8qis0qMy197x0upQt2bfmpYNVV5TxRVVKm6km6nVsBjS3WP76urjuzTqeRuC8LUZyqLfKwAAAAAAAJqIb3WvRfFRjRv2Vnm81QGtL5Atr+7FWzNWE+Tufc2j8kpfj97AQLeq+jXf8TWtGbymQr5wkfC1GapZ+Ur4CgAAAAAAgOYqwmpRnNWiuEa6mVoNj9f0B7fRjtDGn4SvzRA32wIAAAAAAADqZrUYinXaFNvIoe7hoGFoM+PxmlqX61v52iud8BUAAAAAAAAIV4Svzcy23WWqqPTKEWFRx8SoUJcDAAAAAAAA4AAIX5uZmn6v3VNjZLUYIa4GAAAAAAAAwIEQvjYz66rD1170ewUAAAAAAADCGuFrM5NZc7Mt+r0CAAAAAAAAYY3wtZnJymblKwAAAAAAANAcEL42I5Uerzbm+1a+9kiLCXE1AAAAAAAAAA6G8LUZ2ZxfqkqPqWi7Ve0TIkNdDgAAAAAAAICDIHxtRjKrb7bVMz1WhmGEuBoAAAAAAAAAB0P42ozQ7xUAAAAAAABoPghfm5Gala89CF8BAAAAAACAsEf42oysy/HdbIuVrwAAAAAAAED4I3xtJioqPdq8q1SS1DM9JsTVAAAAAAAAADgUwtdmYn1uibym1CbKppQYR6jLAQAAAAAAAHAIhK/NRFatfq+GYYS4GgAAAAAAAACHQvjaTGTR7xUAAAAAAABoVghfm4mala890wlfAQAAAAAAgOaA8LWZyMz2ha+sfAUAAAAAAACaB8LXZqDEVaUdBeWSpJ5pMSGuBgAAAAAAAEB9EL42A+uqWw6kxjqUEGUPcTUAAAAAAAAA6oPwtRmo6ffai36vAAAAAAAAQLNB+NoMZGaXSJJ60u8VAAAAAAAAaDYIX5uBmpWv9HsFAAAAAAAAmg/C12Zgb/jKylcAAAAAAACguSB8DXN7St3KLXZJknoQvgIAAAAAAADNBuFrmKtZ9dqhTaRiHBEhrgYAAAAAAABAfRG+hjlaDgAAAAAAAADNE+FrmMvKKZFE+AoAAAAAAAA0N4SvYS6zeuVrr/SYEFcCAAAAAAAAoCEIX8OYaZq0HQAAAAAAAACaKcLXMJZX7FJBWaUshtQthZWvAAAAAAAAQHNC+BrGavq9dk6KltNmDXE1AAAAAAAAABqC8DWMZdJyAAAAAAAAAGi2CF/DWFZ2TfhKywEAAAAAAACguSF8DWP+la/prHwFAAAAAAAAmhvC1zBlmqbWVYevvWg7AAAAAAAAADQ7hK9hakdBuUrdHtmshjonR4e6HAAAAAAAAAANRPgaprKqV712TY6RzcqXCQAAAAAAAGhuSPXCVGZ2iST6vQIAAAAAAADNFeFrmNrb7zUmxJUAAAAAAAAAOByEr2Eqszp87cnNtgAAAAAAAIBmifA1DHm8ptblVrcdIHwFAAAAAAAAmiXC1zC0ZVep3FVeOW0WZSRGhbocAAAAAAAAAIeB8DUMZeX4Vr32SI2V1WKEuBoAAAAAAAAAh4PwNQxl0e8VAAAAAAAAaPYIX8PQ3pttxYS4EgAAAAAAAACHq1mEr3PnzlXnzp3ldDo1bNgwLVmypF7HvfHGGzIMQ+PGjQtugY0sK7s6fE1n5SsAAADQFFrbnAMAADSNsA9f58+frylTpmj69OlasWKFBg4cqDFjxig3N/egx23evFl//OMfdcIJJzRRpY3DXeXVpvxSSVIv2g4AAAAAQdfa5hwAAKDphH34+thjj+naa6/VpEmT1LdvX82bN09RUVF68cUXD3iMx+PRZZddphkzZqhr165NWO2R25RfqiqvqVhHhNrGO0NdDgAAANDitbY5BwAAaDphHb663W4tX75co0eP9o9ZLBaNHj1aixcvPuBx999/v1JTU3X11VfX6zoul0tFRUUBW6jU9HvtkRYjwzBCVgcAAADQGrTGOQcAAGg6YR2+5ufny+PxKC0tLWA8LS1N2dnZdR7z7bff6oUXXtBzzz1X7+vMmjVL8fHx/i0jI+OI6j4SNf1ee9HvFQAAAAi61jjnAAAATSesw9eGKi4u1hVXXKHnnntOycnJ9T5u2rRpKiws9G/btm0LYpUHl1W98rUn/V4BAACAsNMS5hwAAKDpRIS6gINJTk6W1WpVTk5OwHhOTo7S09P323/Dhg3avHmzzjrrLP+Y1+uVJEVERCgzM1PdunXb7ziHwyGHw9HI1R+emvCVm20BAAAAwdca5xwAAKDphPXKV7vdrsGDB2vhwoX+Ma/Xq4ULF2r48OH77d+7d2+tXr1aq1at8m9nn322Tj75ZK1atSrs/7Sn3O3Rlt1lkqQehK8AAABA0LW2OQcAAGhaYb3yVZKmTJmiiRMnasiQITr22GM1e/ZslZaWatKkSZKkCRMmqH379po1a5acTqf69+8fcHxCQoIk7Tcejtbnlsg0pcRou5Jj7KEuBwAAAGgVWtOcAwAANK2wD1/Hjx+vvLw83XvvvcrOztagQYO0YMECf0P8rVu3ymIJ6wW89ba332uMDMMIcTUAAABA69Ca5hwAAKBpGaZpmqEuItwUFRUpPj5ehYWFiouLa7Lrzvp4jZ75eqMmDu+kGefwW3MAAA5HqL6PA0BD8G8VAADNV0O+j/Pr2zCSWb3ylX6vAAAAAAAAQPNH+BpGsrJ94WuvdMJXAAAAAAAAoLkjfA0TxRWV2llYIUnqmUr4CgAAAAAAADR3hK9hIiunRJKUHudUfJQtxNUAAAAAAAAAOFKEr2Eiy9/vNSbElQAAAAAAAABoDISvYSKzpt8rN9sCAAAAAAAAWgTC1zCxLtcXvvbkZlsAAAAAAABAi0D4GiYys309X1n5CgAAAAAAALQMhK9hYFeJS/klLklS91R6vgIAAAAAAAAtAeFrGMjK8a16zUiMVLQjIsTVAAAAAAAAAGgMhK9hoKbfKy0HAAAAAAAAgJaD8DUMZGZX32yL8BUAAAAAAABoMQhfw0BWDuErAAAAAAAA0NIQvoaYaZqsfAUAAAAAAABaIMLXEMstdqmookpWi6GuKdGhLgcAAAAAAABAIyF8DbGaVa+dk6LktFlDXA0AAAAAAACAxkL4GmL0ewUAAAAAAABaJsLXEKPfKwAAAAAAANAyEb6GWFZuiSSpVzrhKwAAAAAAANCSEL6GkNdrah1tBwAAAAAAAIAWifA1hHYUlKvM7ZHdalHnpKhQlwMAAAAAAACgERG+hlBNv9euKdGKsPKlAAAAAAAAAFoSEr8Qysr1ha/0ewUAAAAAAABaHsLXEMrKpt8rAAAAAAAA0FIRvoZQZk6JJMJXAAAAAAAAoCUifA2RKo9XG3J94WsvwlcAAAAAAACgxSF8DZEtu8vk9ngVabOqQ5vIUJcDAAAAAAAAoJERvobI3n6vMbJYjBBXAwAAAAAAAKCxEb6GSGaOL3ztQcsBAAAAAAAAoEUifA2RrOrwlX6vAAAAAAAAQMtE+BoiWTm+m231TCd8BQAAAAAAAFoiwtcQcFV5tCm/VBIrXwEAAAAAAICWivA1BDbmlcrjNRXrjFBanCPU5QAAAAAAAAAIAsLXEKjd79UwjBBXAwAAAAAAACAYCF9DoCZ8pd8rAAAAAAAA0HIRvoZAZrbvZlv0ewUAAAAAAABaLsLXEKhZ+dojLSbElQAAAAAAAAAIFsLXJlbmrtLW3WWSWPkKAAAAAAAAtGSEr01sfa6v5UByjF1JMY4QVwMAAAAAAAAgWAhfm1hmdnXLgVRWvQIAAAAAAAAtGeFrE6vp99ornfAVAAAAAAAAaMkIX5tYZo6v7UBP+r0CAAAAAAAALRrhaxNb51/5GhPiSgAAAAAAAAAEE+FrEyosr9RvhRWSpO70fAUAAAAAAABaNMLXJlSz6rVtvFPxkbYQVwMAAAAAAAAgmAhfm1BmdfhKv1cAAAAAAACg5YsIdQGtyTEd2+jOMb3UoU1kqEsBAAAAAAAAEGSEr02oT9s49WkbF+oyAAAAAAAAADQB2g4AAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEDSL8HXu3Lnq3LmznE6nhg0bpiVLlhxw33feeUdDhgxRQkKCoqOjNWjQIL366qtNWC0AAACA5oY5BwAACIawD1/nz5+vKVOmaPr06VqxYoUGDhyoMWPGKDc3t879ExMTdffdd2vx4sX66aefNGnSJE2aNEmffPJJE1cOAAAAoDlgzgEAAILFME3TDHURBzNs2DANHTpUc+bMkSR5vV5lZGTolltu0dSpU+t1jmOOOUZjx47VAw88UK/9i4qKFB8fr8LCQsXFxR127QAAoOnxfRxAQzHnAAAADdGQ7+NhvfLV7XZr+fLlGj16tH/MYrFo9OjRWrx48SGPN01TCxcuVGZmpk488cQD7udyuVRUVBSwAQAAAGj5mHMAAIBgCuvwNT8/Xx6PR2lpaQHjaWlpys7OPuBxhYWFiomJkd1u19ixY/Xkk0/q1FNPPeD+s2bNUnx8vH/LyMhotPcAAAAAIHwx5wAAAMEU1uHr4YqNjdWqVau0dOlSPfTQQ5oyZYq+/PLLA+4/bdo0FRYW+rdt27Y1XbEAAAAAmh3mHAAAoD4iQl3AwSQnJ8tqtSonJydgPCcnR+np6Qc8zmKxqHv37pKkQYMGac2aNZo1a5ZGjRpV5/4Oh0MOh6PR6gYAAADQPDDnAAAAwRTWK1/tdrsGDx6shQsX+se8Xq8WLlyo4cOH1/s8Xq9XLpcrGCUCAAAAaMaYcwAAgGAK65WvkjRlyhRNnDhRQ4YM0bHHHqvZs2ertLRUkyZNkiRNmDBB7du316xZsyT5eikNGTJE3bp1k8vl0scff6xXX31VTz/9dCjfBgAAAIAwxZwDAAAES9iHr+PHj1deXp7uvfdeZWdna9CgQVqwYIG/If7WrVtlsexdwFtaWqobb7xR27dvV2RkpHr37q1//vOfGj9+fKjeAgAAAIAwxpwDAAAEi2GaphnqIsJNYWGhEhIStG3bNsXFxYW6HAAA0ABFRUXKyMhQQUGB4uPjQ10OANSJOQcAAM1XQ+YcYb/yNRSKi4slSRkZGSGuBAAAHK7i4mLCVwBhizkHAADNX33mHKx8rYPX69XOnTsVGxsrwzAa9dw1yTi/4Q4PfD3CD1+T8MPXJLzw9Tg00zRVXFysdu3aBfyZMACEE+YcrQdfj/DD1yS88PUIP3xNDq0hcw5WvtbBYrGoQ4cOQb1GXFwc/wGHEb4e4YevSfjhaxJe+HocHCteAYQ75hytD1+P8MPXJLzw9Qg/fE0Orr5zDpaDAAAAAAAAAEAQEL4CAAAAAAAAQBAQvjYxh8Oh6dOny+FwhLoUiK9HOOJrEn74moQXvh4AgEPhe0V44esRfviahBe+HuGHr0nj4oZbAAAAAAAAABAErHwFAAAAAAAAgCAgfAUAAAAAAACAICB8BQAAAAAAAIAgIHwFAAAAAAAAgCAgfAUAAAAAAACAICB8bUJz585V586d5XQ6NWzYMC1ZsiTUJbVas2bN0tChQxUbG6vU1FSNGzdOmZmZoS4L1f7yl7/IMAxNnjw51KW0ajt27NDll1+upKQkRUZGasCAAVq2bFmoy2q1PB6P7rnnHnXp0kWRkZHq1q2bHnjgAZmmGerSAABhhDlH+GDOEd6Yc4QH5hzhhTlHcBC+NpH58+drypQpmj59ulasWKGBAwdqzJgxys3NDXVprdJXX32lm266ST/88IM+++wzVVZW6rTTTlNpaWmoS2v1li5dqmeeeUZHHXVUqEtp1fbs2aORI0fKZrPpv//9r3799Vf97W9/U5s2bUJdWqv117/+VU8//bTmzJmjNWvW6K9//asefvhhPfnkk6EuDQAQJphzhBfmHOGLOUd4YM4RfphzBIdhEl83iWHDhmno0KGaM2eOJMnr9SojI0O33HKLpk6dGuLqkJeXp9TUVH311Vc68cQTQ11Oq1VSUqJjjjlGTz31lB588EENGjRIs2fPDnVZrdLUqVP13Xff6Ztvvgl1Kah25plnKi0tTS+88IJ/7Pzzz1dkZKT++c9/hrAyAEC4YM4R3phzhAfmHOGDOUf4Yc4RHKx8bQJut1vLly/X6NGj/WMWi0WjR4/W4sWLQ1gZahQWFkqSEhMTQ1xJ63bTTTdp7NixAf+vIDT+85//aMiQIbrwwguVmpqqo48+Ws8991yoy2rVRowYoYULFyorK0uS9OOPP+rbb7/V6aefHuLKAADhgDlH+GPOER6Yc4QP5hzhhzlHcESEuoDWID8/Xx6PR2lpaQHjaWlpWrt2bYiqQg2v16vJkydr5MiR6t+/f6jLabXeeOMNrVixQkuXLg11KZC0ceNGPf3005oyZYruuusuLV26VLfeeqvsdrsmTpwY6vJapalTp6qoqEi9e/eW1WqVx+PRQw89pMsuuyzUpQEAwgBzjvDGnCM8MOcIL8w5wg9zjuAgfEWrd9NNN+nnn3/Wt99+G+pSWq1t27bptttu02effSan0xnqciDfBGHIkCGaOXOmJOnoo4/Wzz//rHnz5vGDUIi8+eabeu211/T666+rX79+WrVqlSZPnqx27drxNQEAIMwx5wg95hzhhzlH+GHOERyEr00gOTlZVqtVOTk5AeM5OTlKT08PUVWQpJtvvlkffvihvv76a3Xo0CHU5bRay5cvV25uro455hj/mMfj0ddff605c+bI5XLJarWGsMLWp23bturbt2/AWJ8+ffT222+HqCLceeedmjp1qi6++GJJ0oABA7RlyxbNmjWLH4QAAMw5whhzjvDAnCP8MOcIP8w5goOer03Abrdr8ODBWrhwoX/M6/Vq4cKFGj58eAgra71M09TNN9+sd999V1988YW6dOkS6pJatVNOOUWrV6/WqlWr/NuQIUN02WWXadWqVfwQFAIjR45UZmZmwFhWVpY6deoUoopQVlYmiyXw27bVapXX6w1RRQCAcMKcI/ww5wgvzDnCD3OO8MOcIzhY+dpEpkyZookTJ2rIkCE69thjNXv2bJWWlmrSpEmhLq1Vuummm/T666/r/fffV2xsrLKzsyVJ8fHxioyMDHF1rU9sbOx+va+io6OVlJRET6wQuf322zVixAjNnDlTF110kZYsWaJnn31Wzz77bKhLa7XOOussPfTQQ+rYsaP69eunlStX6rHHHtNVV10V6tIAAGGCOUd4Yc4RXphzhB/mHOGHOUdwGKZpmqEuorWYM2eOHnnkEWVnZ2vQoEF64oknNGzYsFCX1SoZhlHn+EsvvaQrr7yyaYtBnUaNGqVBgwZp9uzZoS6l1frwww81bdo0rVu3Tl26dNGUKVN07bXXhrqsVqu4uFj33HOP3n33XeXm5qpdu3a65JJLdO+998put4e6PABAmGDOET6Yc4Q/5hyhx5wjvDDnCA7CVwAAAAAAAAAIAnq+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAAAAAAABAEBC+AgAAAAAAAEAQEL4CAAAAAAAAQBAQvgIAJEmjRo3SqFGjQl0GAAAAgIO47777ZBiG8vPzG/W8S5cu1YgRIxQdHS3DMLRq1SpJ0oIFCzRo0CA5nU4ZhqGCggJdeeWV6ty5c6Nev3Pnzrryyisb9ZxAOIgIdQEAgPr7/vvv9emnn2ry5MlKSEgIdTkAAAAAWoDKykpdeOGFcjqdevzxxxUVFaVOnTpp165duuiii9SvXz/NnTtXDodD0dHRoS4XaFYIXwGgGfn+++81Y8YMXXnllYSvAAAAABrFhg0btGXLFj333HO65ppr/OMLFixQcXGxHnjgAY0ePTqEFQLNF20HAISN0tLSUJcQtvjcAAAAAPXDz84Nl5ubK0n7LfA40DiA+iN8BVqwLVu26MYbb1SvXr0UGRmppKQkXXjhhdq8efN++xYUFOj2229X586d5XA41KFDB02YMCGgj1BFRYXuu+8+9ezZU06nU23bttV5552nDRs2SJK+/PJLGYahL7/8MuDcmzdvlmEYevnll/1jV155pWJiYrRhwwadccYZio2N1WWXXSZJ+uabb3ThhReqY8eOcjgcysjI0O23367y8vL96l67dq0uuugipaSkKDIyUr169dLdd98tSVq0aJEMw9C7776733Gvv/66DMPQ4sWLG/ppPaAnn3xS/fr1U1RUlNq0aaMhQ4bo9ddfD9hn5cqVOv300xUXF6eYmBidcsop+uGHHwL2efnll2UYhr766ivdeOONSk1NVYcOHXTffffpzjvvlCR16dJFhmHIMIyAr+c///lPDR48WJGRkUpMTNTFF1+sbdu27Vfrs88+q27duikyMlLHHnusvvnmm0b7PAAAAKBlYV4RfvMKSf7eqwkJCYqPj9ekSZNUVlbmf72uz1cNwzB03333SfJ9Dk866SRJ0oUXXijDMPz3g5g4caIkaejQoTIM46A9Wb1er2bPnq1+/frJ6XQqLS1N119/vfbs2ROwn2maevDBB9WhQwdFRUXp5JNP1i+//NLAzxLQfNB2AGjBli5dqu+//14XX3yxOnTooM2bN+vpp5/WqFGj9OuvvyoqKkqSVFJSohNOOEFr1qzRVVddpWOOOUb5+fn6z3/+o+3btys5OVkej0dnnnmmFi5cqIsvvli33XabiouL9dlnn+nnn39Wt27dGlxfVVWVxowZo+OPP16PPvqov5633npLZWVluuGGG5SUlKQlS5boySef1Pbt2/XWW2/5j//pp590wgknyGaz6brrrlPnzp21YcMGffDBB3rooYc0atQoZWRk6LXXXtO5554bcO3XXntN3bp10/Dhw4/gM7zXc889p1tvvVUXXHCBbrvtNlVUVOinn37S//73P1166aWSpF9++UUnnHCC4uLi9Kc//Uk2m03PPPOMRo0apa+++krDhg0LOOeNN96olJQU3XvvvSotLdXpp5+urKws/etf/9Ljjz+u5ORkSVJKSook6aGHHtI999yjiy66SNdcc43y8vL05JNP6sQTT9TKlSv9v61+4YUXdP3112vEiBGaPHmyNm7cqLPPPluJiYnKyMholM8HAAAAWg7mFeE1r6hx0UUXqUuXLpo1a5ZWrFih559/XqmpqfrrX//aoGtef/31at++vWbOnKlbb71VQ4cOVVpamiSpV69eevbZZ3X//ferS5cuB/36XH/99Xr55Zc1adIk3Xrrrdq0aZPmzJmjlStX6rvvvpPNZpMk3XvvvXrwwQd1xhln6IwzztCKFSt02mmnye12N/CzBTQTJoAWq6ysbL+xxYsXm5LMV155xT927733mpLMd955Z7/9vV6vaZqm+eKLL5qSzMcee+yA+yxatMiUZC5atCjg9U2bNpmSzJdeesk/NnHiRFOSOXXq1HrVPWvWLNMwDHPLli3+sRNPPNGMjY0NGKtdj2ma5rRp00yHw2EWFBT4x3Jzc82IiAhz+vTp+13ncJ1zzjlmv379DrrPuHHjTLvdbm7YsME/tnPnTjM2NtY88cQT/WMvvfSSKck8/vjjzaqqqoBzPPLII6Ykc9OmTQHjmzdvNq1Wq/nQQw8FjK9evdqMiIjwj7vdbjM1NdUcNGiQ6XK5/Ps9++yzpiTzpJNOasjbBgAAQCvAvCK85hXTp083JZlXXXVVwPi5555rJiUl+T+u6/NVQ1JA3TWf87feeitgv5q5ydKlSwPGJ06caHbq1Mn/8TfffGNKMl977bWA/RYsWBAwnpuba9rtdnPs2LEBn9+77rrLlGROnDjxoO8daI5oOwC0YJGRkf7nlZWV2rVrl7p3766EhAStWLHC/9rbb7+tgQMH7vdbXMn35yg1+yQnJ+uWW2454D6H44Ybbjho3aWlpcrPz9eIESNkmqZWrlwpScrLy9PXX3+tq666Sh07djxgPRMmTJDL5dK///1v/9j8+fNVVVWlyy+//LDr3ldCQoK2b9+upUuX1vm6x+PRp59+qnHjxqlr167+8bZt2+rSSy/Vt99+q6KiooBjrr32Wlmt1npd/5133pHX69VFF12k/Px8/5aenq4ePXpo0aJFkqRly5YpNzdXf/jDH2S32/3HX3nllYqPj2/o2wYAAEArwLwifOYVtf3hD38I+PiEE07Qrl279ptXNIW33npL8fHxOvXUUwPmI4MHD1ZMTIx/PvL555/L7XbrlltuCfj8Tp48uclrBpoK4SvQgpWXl+vee+9VRkaGHA6HkpOTlZKSooKCAhUWFvr327Bhg/r373/Qc23YsEG9evVSRETjdSuJiIhQhw4d9hvfunWrrrzySiUmJiomJkYpKSn+HkQ1dW/cuFGSDll37969NXToUL322mv+sddee03/3969hzdZ3/8ffyVpk54LPR8o0HIGOQlSETxMUTyxeRziNhibujl0aucBlMM8MnUynKjofp6+25wH1M15YNPOw1RolZMyzrSc6RmaHugpuX9/pA2ElkNL0zttn4/rypXkzue+8w7VJu9XP/ncZ555pvr373/M/VwulwoKCnwux/sazD333KOIiAiNGzdOAwYM0KxZs/Tll196Hy8uLlZ1dbUGDRrUbN8hQ4bI7XY3W5s1PT39uK/tSFu3bpVhGBowYIDi4+N9Lhs3bvQulL9z505J0oABA3z2Dw4O9gmFAQAAgCb0FYHTVxzp6LC4Z8+ektRsjdWOsHXrVpWXlyshIaFZP1JZWXnCfiQ+Pt5bP9DVsOYr0IXdeuuteumll3T77bdr/Pjxio6OlsVi0XXXXSe3293uz3esv1S7XK4WtzscDlmt1mZjL7zwQpWVlemee+7R4MGDFR4err179+qnP/1pm+qePn26brvtNu3Zs0e1tbVauXKllixZctx9du/e3Sz8/OSTT3Teeee1OH7IkCHavHmz3nvvPS1fvlxvvfWWnnnmGc2fP1/3339/q2uWfP9SfyJut1sWi0Uffvhhi7NlIyIi2lQDAAAAQF/hEWh9xbG+JWcYhqTW/zueCrfbrYSEBJ9w+khN56kAuiPCV6ALW7ZsmWbMmKEnnnjCu62mpkYHDx70GdevXz+tX7/+uMfq16+fcnJyVF9f710o/WhNf6k8+vhNf908Gd999522bNmiV155RdOnT/du/+ijj3zGNc3SPFHdknTdddcpKytLf/vb33To0CEFBwdr6tSpx90nKSmp2XOOHDnyuPuEh4dr6tSpmjp1qurq6nTVVVfp4Ycf1pw5cxQfH6+wsDBt3ry52X6bNm2S1Wo9qZNdHesDVL9+/WQYhtLT0zVw4MBj7t+nTx9Jnr9Mn3/++d7t9fX1ys/PP+FrBAAAQPdDX+ERCH1FSEjICets0h7/jierX79++vjjjzVhwoTjTiI5sh858pt3xcXFpszYBToCyw4AXZjNZvP+1bPJU0891ewvnVdffbXWrVund955p9kxmva/+uqrVVJS0uJfdpvG9OnTRzabTZ9//rnP488880yraj7ymE23n3zySZ9x8fHxOuecc/Tiiy9q165dLdbTJC4uTpdccon+8pe/6K9//asuvvhixcXFHbeOkJAQTZo0yedyvK/BlJaW+ty32+0aOnSoDMNQfX29bDabLrroIv3jH//Qjh07vOMKCwv16quvauLEiYqKijpuTZLng5jU/APUVVddJZvNpvvvv7/Z6zcMw1vf2LFjFR8fr6VLl/p83enll19udkwAAABAoq9oEgh9RWtERUUpLi7ulP4dT9YPf/hDuVwuPfjgg80ea2ho8PYakyZNUnBwsJ566imff9/Fixe3e01AoGDmK9CFXX755frzn/+s6OhoDR06VCtWrNDHH3+s2NhYn3F33XWXli1bpmuvvVY/+9nPNGbMGJWVlendd9/V0qVLNXLkSE2fPl3/93//p6ysLOXm5urss89WVVWVPv74Y/3qV7/SD37wA0VHR+vaa6/VU089JYvFon79+um9997zru9zMgYPHqx+/frpzjvv1N69exUVFaW33nqrxb+C/vGPf9TEiRN1+umn66abblJ6erp27Nih999/X2vXrvUZO336dF1zzTWS1OIHglN10UUXKSkpSRMmTFBiYqI2btyoJUuW6LLLLlNkZKQk6aGHHtJHH32kiRMn6le/+pWCgoL03HPPqba2Vo899thJPc+YMWMkSffdd5+uu+46BQcHa8qUKerXr58eeughzZkzRzt27NAVV1yhyMhI5efn65133tFNN92kO++8U8HBwXrooYf0i1/8Queff76mTp2q/Px8vfTSS6z5CgAAgBbRVxwWCH1Fa9xwww363e9+pxtuuEFjx47V559/ri1btrR73eeee65+8YtfaOHChVq7dq0uuugiBQcHa+vWrXrzzTf15JNP6pprrlF8fLzuvPNOLVy4UJdffrkuvfRSrVmzRh9++OEJg2yg0zIAdFkHDhwwZs6cacTFxRkRERHG5MmTjU2bNhl9+vQxZsyY4TO2tLTUuOWWW4zU1FTDbrcbvXr1MmbMmGGUlJR4x1RXVxv33XefkZ6ebgQHBxtJSUnGNddcY2zfvt07pri42Lj66quNsLAwo2fPnsYvfvELY/369YYk46WXXvKOmzFjhhEeHt5i3Rs2bDAmTZpkREREGHFxccaNN95orFu3rtkxDMMw1q9fb1x55ZVGjx49jJCQEGPQoEHGvHnzmh2ztrbW6NmzpxEdHW0cOnSo9f+YJ/Dcc88Z55xzjhEbG2s4HA6jX79+xl133WWUl5f7jFu9erUxefJkIyIiwggLCzO+973vGV999ZXPmJdeesmQZHz99dctPteDDz5opKamGlar1ZBk5Ofnex976623jIkTJxrh4eFGeHi4MXjwYGPWrFnG5s2bfY7xzDPPGOnp6YbD4TDGjh1rfP7558a5555rnHvuue3y7wEAAICug77isEDoKxYsWGBIMoqLi332beojjuwPqqurjZ///OdGdHS0ERkZafzwhz80ioqKDEnGggULvOM++eQTQ5Lx5ptvtnjMo3uTGTNmGH369GlW//PPP2+MGTPGCA0NNSIjI43hw4cbd999t7Fv3z7vGJfLZdx///1GcnKyERoaapx33nnG+vXrW/zvCegKLIZx1Dx6AOiCGhoalJKSoilTpuiFF14wuxwAAAAAnRB9BYDWYs1XAN3C3//+dxUXF/sstg8AAAAArUFfAaC1mPkKoEvLycnRt99+qwcffFBxcXFavXq12SUBAAAA6GToKwC0FTNfAXRpzz77rG6++WYlJCTo//7v/8wuBwAAAEAnRF8BoK2Y+QoAAAAAAAAAfsDMVwAAAAAAAADwA8JXAAAAAAAAAPCDILMLCERut1v79u1TZGSkLBaL2eUAAIBWMAxDFRUVSklJkdXK35kBBCZ6DgAAOq/W9ByEry3Yt2+f0tLSzC4DAACcgt27d6tXr15mlwEALaLnAACg8zuZnoPwtQWRkZGSPP+AUVFRJlcDAABaw+l0Ki0tzft+DgCBiJ4DAIDOqzU9B+FrC5q+9hMVFcUHIQAAOim+xgsgkNFzAADQ+Z1Mz8FCaAAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB6aHr08//bT69u2rkJAQZWZmKjc395hj6+vr9cADD6hfv34KCQnRyJEjtXz5cp8xv/3tb2WxWHwugwcP9vfLAAAAABCg6DkAAIBZTA1fX3/9dWVlZWnBggVavXq1Ro4cqcmTJ6uoqKjF8XPnztVzzz2np556Shs2bNAvf/lLXXnllVqzZo3PuGHDhmn//v3eyxdffNERLwcAAABAgKHnAAAAZjI1fF20aJFuvPFGzZw5U0OHDtXSpUsVFhamF198scXxf/7zn3Xvvffq0ksvVUZGhm6++WZdeumleuKJJ3zGBQUFKSkpyXuJi4vriJcDAAAAIMDQcwAAADOZFr7W1dVp1apVmjRp0uFirFZNmjRJK1asaHGf2tpahYSE+GwLDQ1t9lfmrVu3KiUlRRkZGfrRj36kXbt2HbeW2tpaOZ1OnwsAAACAzo2eAwAAmM208LWkpEQul0uJiYk+2xMTE1VQUNDiPpMnT9aiRYu0detWud1uffTRR3r77be1f/9+75jMzEy9/PLLWr58uZ599lnl5+fr7LPPVkVFxTFrWbhwoaKjo72XtLS09nmRAAAAAExDzwEAAMxm+gm3WuPJJ5/UgAEDNHjwYNntdt1yyy2aOXOmrNbDL+OSSy7RtddeqxEjRmjy5Mn64IMPdPDgQb3xxhvHPO6cOXNUXl7uvezevbsjXg4AAACAAEPPAQAA2pNp4WtcXJxsNpsKCwt9thcWFiopKanFfeLj4/X3v/9dVVVV2rlzpzZt2qSIiAhlZGQc83l69OihgQMHatu2bccc43A4FBUV5XMBAAAA0LnRcwAAALOZFr7a7XaNGTNG2dnZ3m1ut1vZ2dkaP378cfcNCQlRamqqGhoa9NZbb+kHP/jBMcdWVlZq+/btSk5ObrfaAQAAAAQ+eg4AAGA2U5cdyMrK0p/+9Ce98sor2rhxo26++WZVVVVp5syZkqTp06drzpw53vE5OTl6++23lZeXp//+97+6+OKL5Xa7dffdd3vH3Hnnnfrss8+0Y8cOffXVV7ryyitls9k0bdq0Dn99AAAAAMxFzwEAAMwUZOaTT506VcXFxZo/f74KCgo0atQoLV++3Lsg/q5du3zWVqqpqdHcuXOVl5eniIgIXXrppfrzn/+sHj16eMfs2bNH06ZNU2lpqeLj4zVx4kStXLlS8fHxHf3yAAAAAJiMngMAAJjJYhiGYXYRgcbpdCo6Olrl5eWsxQQAQCfD+ziAzoDfVQAAdF6teR83ddkBAAAAAAAAAOiqCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXwEAAAAAAADADwhfAQAAAAAAAMAPCF8BAAAAAAAAwA8IXztYbYNLZVV1ZpcBAAAAAAAAwM8IXzvQn1fu1Ijf/luPLd9kdikAAAAAAAAA/IzwtQOlRIeotsGtnPwys0sBAAAAAAAA4Gemh69PP/20+vbtq5CQEGVmZio3N/eYY+vr6/XAAw+oX79+CgkJ0ciRI7V8+fJTOmZHGts3RhaLlF9SpSJnjdnlAAAAAN1Cd+o5AABAYDE1fH399deVlZWlBQsWaPXq1Ro5cqQmT56soqKiFsfPnTtXzz33nJ566ilt2LBBv/zlL3XllVdqzZo1bT5mR4oODdaQpChJYvYrAAAA0AG6W88BAAACi8UwDMOsJ8/MzNQZZ5yhJUuWSJLcbrfS0tJ06623avbs2c3Gp6Sk6L777tOsWbO8266++mqFhobqL3/5S5uO2RKn06no6GiVl5crKirqVF+mj/v/+T+99OUO/fjM3nroiuHtemwAAODf93EAnU937DkAAIB/teZ93LSZr3V1dVq1apUmTZp0uBirVZMmTdKKFSta3Ke2tlYhISE+20JDQ/XFF1+0+ZhNx3U6nT4Xf8lMj5Uk5eQx8xUAAADwp+7acwAAgMBhWvhaUlIil8ulxMREn+2JiYkqKChocZ/Jkydr0aJF2rp1q9xutz766CO9/fbb2r9/f5uPKUkLFy5UdHS095KWlnaKr+7YxqXHSJK2FlWqtLLWb88DAAAAdHfdtecAAACBw/QTbrXGk08+qQEDBmjw4MGy2+265ZZbNHPmTFmtp/Yy5syZo/Lycu9l9+7d7VRxczHhdg1KjJQk5bLuKwAAABBQukLPAQAAAodp4WtcXJxsNpsKCwt9thcWFiopKanFfeLj4/X3v/9dVVVV2rlzpzZt2qSIiAhlZGS0+ZiS5HA4FBUV5XPxp6bZr5x0CwAAAPCf7txzAACAwGBa+Gq32zVmzBhlZ2d7t7ndbmVnZ2v8+PHH3TckJESpqalqaGjQW2+9pR/84AenfMyOlJlB+AoAAAD4W3fuOQAAQGAIMvPJs7KyNGPGDI0dO1bjxo3T4sWLVVVVpZkzZ0qSpk+frtTUVC1cuFCSlJOTo71792rUqFHau3evfvvb38rtduvuu+8+6WMGgqaZr5sKnCqvrld0WLDJFQEAAABdU3ftOQAAQGAwNXydOnWqiouLNX/+fBUUFGjUqFFavny5d/H6Xbt2+aytVFNTo7lz5yovL08RERG69NJL9ec//1k9evQ46WMGgoTIEGXEhyuvuEq5O8p04dDAqQ0AAADoSrprzwEAAAKDxTAMw+wiAo3T6VR0dLTKy8v9thbTnLe/099yd+mGiemae/lQvzwHAADdUUe8jwPAqeJ3FQAAnVdr3sdNW/O1u8tsXHogdwfrvgIAAAAAAABdEeGrSZpOurV+b7kqaupNrgYAAAAAAABAeyN8NUlydKh6x4TJbUjf7DxgdjkAAAAAAAAA2hnhq4malh7IyWPpAQAAAAAAAKCrIXw1UWZGrCQpJ7/U5EoAAAAAAAAAtDfCVxM1zXz9bk+5qusaTK4GAAAAAAAAQHsifDVRr56hSokOUYPb0OqdB80uBwAAAAAAAEA7Inw1kcViYekBAAAAAAAAoIsifDUZJ90CAAAAAAAAuibCV5M1zXxdu/ugaupdJlcDAAAAAAAAoL0Qvpqsb2yYEiIdqnO5tWbXQbPLAQAAAAAAANBOCF9NZrFYNK5x6YHcfJYeAAAAAAAAALoKwtcAwEm3AAAAAAAAgK6H8DUAnNk483X1rgOqa3CbXA0AAAAAAACA9kD4GgD6J0QoNtyumnq3vt1z0OxyAAAAAAAAALQDwtcAcOS6rzms+woAAAAAAAB0CYSvASKzMXxdmce6rwAAAAAAAEBXQPgaIMale066tWrnATW4WPcVAAAAAAAA6OwIXwPE4KRIRYcGq7rOpfX7nGaXAwAAAAAAAOAUEb4GCKvVojP6Nq77ytIDAAAAAAAAQKdH+BpAzszgpFsAAAAAAABAV0H4GkAyG9d9/Tq/TC63YXI1AAAAAAAAAE4F4WsAGZIcqQhHkCpqG7RxP+u+AgAAAAAAAJ0Z4WsACbJZNbZvT0ksPQAAAAAAAAB0doSvAaZp6QFOugUAAAAAAAB0boSvASaz8aRbuTvK5GbdVwAAAAAAAKDTInwNMMNToxVmt+lgdb22FFWYXQ4AAAAAAACANiJ8DTDBNqvG9Glc9zWPdV8BAAAAAACAzorwNQCN69u49AAn3QIAAAAAAAA6LcLXAJSZ0XjSrfxSGQbrvgIAAAAAAACdEeFrABqZFi1HkFUllXXaXlxldjkAAAAAAAAA2oDwNQA5gmwa3buHJM/sVwAAAAAAAACdD+FrgMpMb1x6gJNuAQAAAAAAAJ0S4WuAykw/fNIt1n0FAAAAAAAAOh/Tw9enn35affv2VUhIiDIzM5Wbm3vc8YsXL9agQYMUGhqqtLQ03XHHHaqpqfE+/tvf/lYWi8XnMnjwYH+/jHY3undPBdssKnDWaFdZtdnlAAAAAJ0WPQcAADBLkJlP/vrrrysrK0tLly5VZmamFi9erMmTJ2vz5s1KSEhoNv7VV1/V7Nmz9eKLL+qss87Sli1b9NOf/lQWi0WLFi3yjhs2bJg+/vhj7/2gIFNfZpuE2m0a2auHvtl5QDl5ZeoTG252SQAAAECnQ88BAADMZOrM10WLFunGG2/UzJkzNXToUC1dulRhYWF68cUXWxz/1VdfacKECbr++uvVt29fXXTRRZo2bVqzv1wHBQUpKSnJe4mLi+uIl9PuMjM8Sw+s5KRbAAAAQJvQcwAAADOZFr7W1dVp1apVmjRp0uFirFZNmjRJK1asaHGfs846S6tWrfJ+8MnLy9MHH3ygSy+91Gfc1q1blZKSooyMDP3oRz/Srl27jltLbW2tnE6nzyUQcNItAAAAoO3oOQAAgNlMC19LSkrkcrmUmJjosz0xMVEFBQUt7nP99dfrgQce0MSJExUcHKx+/frpvPPO07333usdk5mZqZdfflnLly/Xs88+q/z8fJ199tmqqKg4Zi0LFy5UdHS095KWltY+L/IUjenTUzarRXsPHtKeA6z7CgAAALQGPQcAADCb6Sfcao1PP/1UjzzyiJ555hmtXr1ab7/9tt5//309+OCD3jGXXHKJrr32Wo0YMUKTJ0/WBx98oIMHD+qNN9445nHnzJmj8vJy72X37t0d8XJOKNwRpNNSoyVJufnMfgUAAAD8rbv1HAAAwL9MWxU+Li5ONptNhYWFPtsLCwuVlJTU4j7z5s3TT37yE91www2SpOHDh6uqqko33XST7rvvPlmtzbPkHj16aODAgdq2bdsxa3E4HHI4HKfwavznzPQYrdt9UDl5Zbrq9F5mlwMAAAB0GvQcAADAbKbNfLXb7RozZoyys7O929xut7KzszV+/PgW96murm72Ycdms0mSDMNocZ/Kykpt375dycnJ7VR5x2o66VYOJ90CAAAAWoWeAwAAmM20ma+SlJWVpRkzZmjs2LEaN26cFi9erKqqKs2cOVOSNH36dKWmpmrhwoWSpClTpmjRokUaPXq0MjMztW3bNs2bN09TpkzxfiC68847NWXKFPXp00f79u3TggULZLPZNG3aNNNe56kY2zdGVou0o7Rahc4aJUaFmF0SAAAA0GnQcwAAADOZGr5OnTpVxcXFmj9/vgoKCjRq1CgtX77cuyD+rl27fP7qPHfuXFksFs2dO1d79+5VfHy8pkyZoocfftg7Zs+ePZo2bZpKS0sVHx+viRMnauXKlYqPj+/w19ceokKCNTQlSuv3OrUyr1Q/GJVqdkkAAABAp0HPAQAAzGQxjvXdmW7M6XQqOjpa5eXlioqKMrscPfDPDXrxy3z9KLO3Hr5yuNnlAAAQ0ALtfRwAWsLvKgAAOq/WvI+btuYrTt7hdV/LTK4EAAAAAAAAwMkifO0ExvX1hK/biipVUllrcjUAAAAAAAAATgbhayfQM9yuwUmRkqRcZr8CAAAAAAAAnQLhayeRmd649EBeqcmVAAAAAAAAADgZhK+dRGZGrCTWfQUAAAAAAAA6i6Aj79TV1amhoaHjnjwoSHa7vcOerzM7o3Hd100FFTpYXaceYfy7AQAAoPOh5wAAAN2JN3ytq6vT//6XK7e7ssOe3GqN0LBh4/gwdBLiIx3qFx+u7cVVys0v00XDkswuCQAAAGgVeg4AANDdeMPXhoYGud2VSk+3KyTE4fcnrqmpVX5+pRoaGvggdJIyM2K1vbhKOYSvAAAA6IToOQAAQHcTdPSGkBCHwsJCOujp6zroebqGzPQYvZqzSzn5nHQLAAAAnRc9BwAA6C444VYnkpnuOenWhn1OOWvqTa4GAAAAAAAAwPEQvnYiSdEh6hMbJrchfbOjzOxyAAAAAAAAABwH4Wsnk5keI0nKySd8BQAAAAAAAAIZ4Wsn07T0QE4e4SsAAAAAAAAQyAhfO5nMDM/M1+/2lquqtsHkagAAAAAAAAAcC+FrJ9OrZ5hSe4TK5Ta0aucBs8sBAAAAAAAAcAyEr51Q0+zXnPxSkysBAAAAAAAAcCxtCl8/+eSb9q4DrdB00q1cTroFAACALoqeAwAAdAVtCl8vvvhW9ev3Az300P/T7t0F7V0TTqDppFvrdperpt5lcjUAAABA+6PnAAAAXUGbwte9ez/ULbf8UMuW/UcZGT/Q5Mm36I03PlJdXX1714cW9IkNU2KUQ3Uut1bvYt1XAAAAdD30HAAAoCtoU/gaF9dDd9zxI61d+6pycl7RwIG99atf/U4pKRfr179+XOvWbWnvOnEEi8Xinf2ak8fSAwAAAOh66DkAAEBXcMon3Dr99MGaM2embrnlh6qsPKQXX3xXY8b8RGeffYP+97/t7VEjWsBJtwAAANBd0HMAAIDOqs3ha319g5Yt+1iXXvpr9elzuf71r5VasuRuFRb+W9u2vaM+fZJ17bWz27NWHKHppFtrdh1UbQPrvgIAAKDroecAAACdXVBbdrr11sf0t7/9S4Yh/eQnl+qxx36t007r7308PDxUv//9bUpJuaTdCoWvfvERiouwq6SyTt/uKdcZfWPMLgkAAABoN/QcAACgK2hT+LphQ76eeuouXXXV+XI47C2OiYvroU8+WXpKxeHYLBaLxqXH6IPvCpSTV0r4CgAAgC6FngMAAHQFbQpfs7OfPfGBg4J07rlj2nJ4nKTM9FhP+JpfplvMLgYAAABoR/QcAACgK2jTmq8LF76kF1/8R7PtL774Dz366MunWhNOUtNJt1btPKB6l9vkagAAAID2Q88BAAC6gjaFr88997YGD+7bbPuwYf20dOnbp1oTTtLAhEj1CAtWdZ1L3+0tN7scAAAAoN3QcwAAgK6gTeFrQUGpkpPjmm2Pj++h/ftLTrkonByr1eJd6zU3v8zkagAAAID2Q88BAAC6gjaFr2lpifryy3XNtn/55TqlpMSfclE4eZnpnvA1J6/U5EoAAACA9kPPAQAAuoI2nXDrxhuv0O23P6H6+gadf/4ZkqTs7Fzdffcf9Zvf/LhdC8TxnZkRK0n6ZscBudyGbFaLyRUBAAAAp46eAwAAdAVtCl/vumu6SkvL9atfPaq6unpJUkiIXffcM0Nz5sxs1wJxfEOSoxQZEqSKmgZt2OfU8F7RZpcEAAAAnDJ6DgAA0BW0KXy1WCx69NFfa968G7RxY75CQx0aMKC3HA57e9eHE7A1rvv6n01FyskvJXwFAABAl0DPAQAAuoI2rfnaJCIiTGecMUynndafD0EmGte47uvKPE66BQAAgK6FngMAAHRmbZr5KknffLNBb7zxkXbtKlBdXYPPY2+//fgpF4aT13TSra93lMntNmRl3VcAAAB0AfQcAACgs2vTzNfXXvuXzjrrZ9q4cYfeeedT1dc36H//267//OdrRUdHtHeNOIHTUqMVZrep/FC9NhdWmF0OAAAAcMroOQAAQFfQpvD1kUde0h/+kKV//vMPstuD9eSTv9GmTW/phz+8UL17J7XqWM8995z69u2rkJAQZWZmKjc397jjFy9erEGDBik0NFRpaWm64447VFNT4zPm6aefbtUxO7tgm1Vj+vSUJOXklZpcDQAAAHDq6DkAAEBX0Kbwdfv2PbrssomSJLs9WFVVNbJYLLrjjuv1/PNvn/RxPv30M91zzz1asGCBVq9erZEjR2ry5MkqKipqcfyrr76q2bNna8GCBdq4caNeeOEFvf7667r33nu9Y15//XVlZWWd9DG7ijMzYiVJOfms+woAAIDOj54DAAB0BW0KX3v2jFJFRbUkKTU1XuvXb5MkHTxYoerqmuPt6uOdd97WzJkzNXPmTA0dOlRLly5VWFiYXnzxxRbHf/XVV5owYYKuv/569e3bVxdddJGmTZvm81fmRYsW6cYbbzzpY3YVTeu+5uaXyTAMk6sBAAAATg09BwAA6AraFL6ec85offTRSknStddO0m23PaEbb3xI06bdpwsuGHdSx6ivr9fWrVt1/vnnHy7GatWkSZO0YsWKFvc566yztGrVKu8Hn7y8PH3wwQe69NJLJUl1dXVatWqVJk2adNLHlKTa2lo5nU6fS2czvFe0HEFWlVbVaXtxpdnlAAAAAKeEngMAAHQFQW3ZacmSu1VTUydJuu++nyk4OEhffbVOV199vubO/flJHePgwYNyuw0lJCT4bE9MTNSmTZta3Of6669XSUmJJk6cKMMw1NDQoF/+8pferwCVlJTI5XIpMTHxpI8pSQsXLtT9999/UnUHKkeQTaf37qkVeaVamVem/gmRZpcEAAAAtBk9BwAA6ApaPfO1oaFB7733X9lsnl2tVqtmz/6p3n33D3riiTvUs2dUuxfZ5NNPP9UjjzyiZ555RqtXr9bbb7+t999/Xw8++OApHXfOnDkqLy/3Xnbv3t1OFXeszAzP0gOs+woAAIDOjJ4DAAB0Fa2e+RoUFKRf/nKhNm5cdkpP3KNHD1mtlmaL0hcWFiopqeWzl86bN08/+clPdMMNN0iShg8frqqqKt1000267777FBcXJ5vNpsLCwpM+piQ5HA45HI5Tej2BIDM9VtJW5eSVyjAMWSwWs0sCAAAAWo2eAwAAdBVtWvN13LhhWrt28yk9cXBwsAYMGKBPPvnEu83tdis7O1vjx49vcZ/q6mpZrb4l22w2SZJhGLLb7RozZoyys7NP+phdyejePWS3WVVUUasdpdVmlwMAAAC0GT0HAADoCtq05uuvfnWtsrL+oN27CzVmzBCFh4f6PD5ixICTOs6VV16le+9dpPHjx2vcuHFavHixqqqqNHPmTEnS9OnTlZqaqoULF0qSpkyZokWLFmn06NHKzMzUtm3bNG/ePE2ZMsX7gSgrK0szZszQ2LFjWzxmVxYSbNPItGh9veOAcvJKlR4XbnZJAAAAQJvQcwAAgK6gTeHrddd5Fpv/9a9/791msVi8X3V3uXJP6jjnnXeuFi6M1/z581VQUKBRo0Zp+fLl3sXrd+3a5fNX57lz58pisWju3Lnau3ev4uPjNWXKFD388MPeMVOnTlVxcfExj9nVZabH6usdB5SbX6brxvU2uxwAAACgTeg5AABAV2AxDMOQPF+v2bjxcw0ZEqmwsJDj7rRz5/7jPt6nT/IJn7i6ukYbN1ZoyJBzFBYW1oqS/c/pdCo6Olrl5eWKivLfYv7+8N+txfrJC7lK7RGqL2efb3Y5AAB0uM78Pg50dfQch/G7CgCAzqs17+Ntmvl6Mh90YI4xfXoqyGrR3oOHtLusWmkxgfUhEwAAADgZ9BwAAKAraFP4+n//995xH58+/fI2FYNTF2YP0vBe0Vqz66By8ssIXwEAANAp0XMAAICuoE3h6223PeFzv76+QdXVNbLbgxUWFsIHIZNlpsd6wte8Ul0zppfZ5QAAAACtRs8BAAC6gjaFrwcOfNJs29atu3Tzzb/TXXf95JSLwqnJTI/R0s+2K3dHmdmlAAAAAG1CzwEAALoC64mHnJwBA3rrd7+7Rbfd9vsTD4Zfje3bU1aLtLO0WgXlNWaXAwAAALQLeg4AANDZtFv4KklBQTbt21fSnodEG0SGBGtYSrQkKSe/1ORqAAAAgPZDzwEAADqTNi078O67n/ncNwxD+/eXaMmSNzRhwsh2KQynJjM9Rt/tLdfKvDL9YFSq2eUAAAAArULPAQAAuoI2ha9XXHGnz32LxaL4+J46//yxeuKJO9qlMJyazIxY/b8v8pn5CgAAgE6JngMAAHQFbQpf3e6v27sOtLMz+vaUxSLlFVepqKJGCZEhZpcEAAAAnDR6DgAA0BW065qvCBw9wuwalBgpSfo6/4DJ1QAAAAAAAADdT5vC16uvvkuPPvpys+2PPfaKrr32nlOtCe3kzIxYSZx0CwAAAJ0PPQcAAOgK2hS+fv75Gl166cRm2y+5ZII+/3zNKReF9pGZHiNJyskrM7kSAAAAoHXoOQAAQFfQpvC1svKQ7Pbmy8UGBwfJ6aw65aLQPsY1hq+bCytUVlVncjUAAADAyaPnAAAAXUGbwtfhw/vp9dc/arb9tdf+paFD00+5KLSP2AiHBiRESJJy85n9CgAAgM6DngMAAHQFzf+UfBLmzbtBV111l7Zv36Pzzz9DkpSdnau//e1fevPNR9u1QJyacekx2lpUqdz8Ml18WpLZ5QAAAAAnhZ4DAAB0BW0KX6dMOUd///sTeuSRF7VsWbZCQx0aMWKAPv74GZ177pj2rhGnIDMjVn/N2cVJtwAAANCp0HMAAICuoE3hqyRddtlEXXZZ8wXwEVjObFz3dcN+p8oP1Ss6NNjkigAAAICTQ88BAAA6uzat+fr11/9TTs76Zttzctbrm282nHJRaD8JUSFKjwuXYUjf7GDdVwAAAHQO9BwAAKAraNPM11mzHtXdd09XZuZpPtv37i3So4++opycV9qlOLSPzPQY5ZdUKSe/TBcMSTS7HAAAAOCE6DkAAOh+DMNQg9uQy22o3uVuvDbU4HarweV5rMHlbrw2VO92+4xtcB2xX+PYEb16qH/jCenN0KbwdcOGfJ1++uBm20ePHqQNG/JPuSi0r3HpMXrt693KyWPdVwAAAHQO9BwAgEDndhsqq65TkbNWhRU1Kq6oVb3L7X3cMJrv02zTUYOOfvzoYxgtHPSE+zR7vIXCjuJyG96As8Ht9oae9S6j8bHDt48OSZu2ecPSYwSnh497RHjqPnFtrfXbKUM7X/jqcNhVWFimjIxePtv37y9RUJCtXQpD+8nMiJUkrd/nVGVtgyIcbV7qFwAAAOgQ9BwAALM0uNwqrWoMVZ01KqqoVVFF47Wz8bazViWVtX4JC9FckNWiIJtFQVbr4evGbcE2q2xWi8+YYJtFNqvnsaToUHNrb8tOF12UqTlzlugf/1ik6GhPcnzwYIXuvfdpXXhhZrsWiFOX2iNUvXqGas+BQ1q184DOHRhvdkkAAADAcdFzAADaW12DW8WVtSpy1qjQWaviowLVQmetiipqVVpV2+Ks1ZZYLFJsuF3xkSGKj3QoNLj56ZUssjTb57j31WzD8e42HsNy3DHNn+ME+1ukYKtVNptFwVaLgmyHw06b1dpsW1Cz+00BqPXw40eFop5rz/ambc2OZfWMP7q+zqRN4evvf3+7zjnnRvXpc7lGjx4kSVq7dosSE2P05z8/0K4Fon1kpsdqz4E9yskrJXwFAABAwKPnAACcrJp611EBavNZqkUVNTpQXX/Sx7RZLYqLsCshMkQJkQ4lRDk8t5uuG7fFRTgUbGvT+ezRTbQpfE1NTdC3376mv/71Q61bt1WhoQ7NnDlF06ZdrOBgvtIeiDIzYvTW6j3KyS8zuxQAAADghOg5AACVtQ0qavzaf6HTs6aqJ1Q9vK2oolYVNQ0nfcxgm0UJjbNUEyIdSozyDVfjG2/Hhjtks3be2ZYIHG3+1BIeHqqJE0epd+8k1dV5/nLw4YdfSpK+//1z26c6tJsz0z3rvn6756AO1bkUamedLAAAAAQ2eg4ACHxut6E6l9tzafBc6htv1x5xu8519HajcbxLh+rdjcGqJ0wtbgxWq+tcJ11HSLD1hLNUEyND1CMsuFN/hR2dT5vC17y8Pbryyrv03XfbZLFYZBiGz3+4LlduuxWI9pEWE6qkqBAVOGu0ZtcBndU/zuySAAAAgGOi5wCAY3O5DR2srpOzpqHloPOIsLP2qEC06XZtC2GpJ0BtDFMbXEcEpEeEqy7fffx9wqkIR5ASIh2Kb2GWque2J2SNdAQRqiIgtSl8ve22J5SenqLs7GeVnv595eS8rLIyp37zmz/o97+/vZ1LRHuwWCzKzIjRP9bu08r8MsJXAAAABDR6DgDdiWEYctY0qLSyVqVVdSqtrFNpVa3n+qhtZVV1Kquqk58zzzaz2zwnVbIHWb2XYJtVdptVjqbbTY/ZrAoOsiokyKa4yCPWV20KWqMcCrOz1Aw6tzb9F7xixbf6z3+WKi6uh6xWi2w2qyZOHKWFC2/Rr3/9uNasebW960Q7yEyP1T/W7lNOXqnZpQAAAADHRc8BoLOrrmtQaWWdSio9IWpZVZ1KjhOo1rtan6aG221yBNsaQ0yL7Dar7EE22Y8MP23NA88jr5se8wlGj3jM0UKI6nMc72Oe52f2KeCrTeGry+VWZGSYJCkurof27SvRoEF91adPsjZv3tmuBaL9ZGbESJLW7D6omnqXQoJZ9xUAAACBiZ4DQKCpbXCprDEwbSlQ9TxWq5LGQLWm3t3q54hwBCk2wq6YcLtiwx2Ki7A33m+8He5QbIRdseF29Qy3K9hm9cMrBdCe2hS+nnZaP61bt1Xp6anKzDxNjz32f7Lbg/T88+8oIyO1vWtEO8mIC1dchEMllbVat/ugMjNizS4JAAAAaBE9BwB/a3C5VVZd12KgWlrlCVGbAtXSyjpV1Da0+jkcQVbFRTiOGajGRtgVF+5QTGOgyiQp4DgMQ3I3SPWHpIZaqaHmiEvtMbbXSL3HS4nDTCu7TeHr3Lk/V1XVIUnSAw/8UpdffrvOPvtGxcZG6/XXF7ZrgWg/FotFmekxev+7/crJLyN8BQAAQMCi5wACR4PLrZ1l1covrlKdyy23YcjlNhqvPWe7dxuGXIYht7vpMXnHNW13GzpiP892o3Fb0/Yjj3l4P0MuQ95je/Zrun3E8/scW4fHHLHdMCSXYch5qF4Hqutb/W8RZLV4QtQIh2LD7Y2zUA/PRo2N8A1Uw+02voaPrsft8oSa9TXNg85jhqDttN1o/YxyXfJ45wtfJ08e773dv3+aNm16S2Vl5erZM4pfKgEuM8MTvubml5ldCgAAAHBM9BxAxzMMQ/vLa7S5sEKbCyq0paBCmwoqtK24UnUNbQg8OgGLReoZZm8hSG0eqMaG2xUVEiyrld9BCABut+Q6TnDpE47WSg2HWrn9WEHoIc/s00Bgc0jBIVJQiBTkkIJCG69DfLf37GNqme12yriYmOj2OhT8KDPdM9t11c4Dqne5WR8GAAAAnQY9B9B+DlTVaXNhhbYUegLWLQUV2lxYoYqalkOV0GCb+iWEKyw4SBaLZLNaZLNaZLVYZG28b7Ucsc1qkc2ixusjtlklm8UiS+PYYx/j8L7NxrRwTKvF9/lPdIzIkGDFRtjVIzRYQe3VF7vdkuHyzMxzuzy33Y33j9zm83jjY96xTdeG77YT7WO4Dz9/s31a+9wnqrel12ic+DjebS0dxy1ZbZI1yHOxWA/f9m63SRbbUduP3MfmO/bIa5/9rMfYp7XP1dJ2qyeYbNNMzxOFn0cFpq669vnv9lRZg5uHnccKQdtzu83u+Vl2Au0WvqJzGJAQoZ5hwTpQXa9v95RrTJ+eZpcEAAAAAPCT6roGbS2sPDybtfG6qKK2xfFBVosy4sM1MDFSg5MiNTAxUoOSIpXWM6xzz/g8dEDas0ra87W0d5Xnfouh5dG3WxGkAmaxWA8HlcFHBJZBRwSWrd3uE3gePf6IMVbWKT6RgAhfn376aT3++OMqKCjQyJEj9dRTT2ncuHEtjj3vvPP02WefNdt+6aWX6v3335ck/fSnP9Urr7zi8/jkyZO1fPny9i++k7FaLRqXHqN//a9QOfmlhK8AAADo8p577jk9+eST9Bvo0updbu0oqfLMYm2azVpYoV1l1TKMlvfp1TPUJ2AdlBSpjLgI2YM6x2yyY3K7pKKNnqC16VKyxeyqJFkaZ1RaD8+4tNg8s/eO3maxNm63HbVPS9uabluPOGYL23wet7aw7WT2Obp2m2fthma1H3V9wtdmPSLkbvDcdjfdbjgcfnsfazh82zjyvvuofRpa2O/ofY449jH3aWgM2lvY5+j9rEHtF4D6bD/y9lGP2YLN/o8bx2F6+Lps2TJlZWVp6dKlyszM1OLFizV58mRt3rxZCQkJzca//fbbqqs7PLW6tLRUI0eO1LXXXusz7uKLL9ZLL73kve9wOPz3IjqZzPRYT/iaV6ZfnWd2NQAAAID/fPrpZ5oz5wk999xz9BvoEtxuQ3sPHvIJWDcXVGh7caXqXS2nrHER9sMBa+P1gMRIRThMjwTaR1WJtOcbaU9u48zW1VJdZfNxMf2kXmdIvcZKUaknDgmPGYq2dZ9OPHMYQJs1+01bU9PyVw/aW9Pz/PGPf9SNN96omTNnSpKWLl2q999/Xy+++KJmz57dbL+YmBif+6+99prCwsKafRhyOBxKSkryU/Wd27h0z7/hqp0H1OByt9/6NgAAAMBJ6Mie45133tbMmTPpN9AplVTWek96taXQsybrloIKVdW1/BX3cLtNA48IWAclRmpgUqTiIrrQHwdc9VLh/3xntZblNR9nj5RST5fSxnkC19SxUnhsx9cLoNvzhq9BQUGyWiOUn18pqWMW7XW5HFqzZo3mzp3r3Wa1WjVp0iStWLHipI7xwgsv6LrrrlN4eLjP9k8//VQJCQnq2bOnzj//fD300EOKjW35F21tba1qaw9/AHQ6nW14NZ3HkOQoRYYEqaKmQRv2OzWiVw+zSwIAdEMHq+tksVgUHcrXpIDuoqN7jvr6eq1bt1W33faQd5sZ/YbU/XoOtE5lbYO2NAarTUHrlsIKlVS2/P9JsM2ifvERGtS4ZEDT0gGpPUI797qsLakobAxZcz2zW/eu9pxs6Ghxg6S0Mxpntp4hxQ9mLUoAAcEbvtrtdg0bNk4NDS2f2dAfioqK5Ha7lZiY6LM9MTFRmzZtOuH+ubm5Wr9+vV544QWf7RdffLGuuuoqpaena/v27br33nt1ySWXaMWKFbLZmv/yXbhwoe6///5TezGdiM1q0bi+McreVKScvDLCVwCAX1XU1GtrUaW2FlZoc0GlthYdPtHH/MuH6mcT080uEUAH6eieY//+/aqsNNSrVy+f7R3db0jdr+dAy+oa3NpeXOldKmBzgWc2654DLYSJ8nxLvXdMmHcma1PQ2jcuXMFd8RuMDXVSwXeHlw/Y/bVUvqv5uJBoz0zWtHGeJQRSx0qhPTq8XAA4GT7LDtjtdtnt9g578lN9rhdeeEHDhw9vtlj+dddd5709fPhwjRgxQv369dOnn36qCy64oNlx5syZo6ysLO99p9OptLS0U6ot0GVmNIav+aW68ZwMs8sBAHQBh+pc2lbkOZvy1savRm4trNTegy03lJK0v/zYjwHomjqy5wgNDT2l/dur35C6Z8/R3RmGoa1Flfpia4lW7zqgzQUVyi+pUoO75XVZEyIdPksFDEqM1IDECIXZu8i6rC0p33t4RuvuXGn/Osl19LIkFilh6BGzWsdJsf09a6sCQCdg6m/xuLg42Ww2FRYW+mwvLCw84fpJVVVVeu211/TAAw+c8HkyMjIUFxenbdu2tfhhyOFwdLsF8jPTPV+Jys0vk8ttyNbVvpoCAPCb2gaX8oqrvF+JbJrNeryzKTc1lAMSIjUoKUIDEiM1ICFCkSEsOQDAfwKl35C6Z8/RHRVX1OrLbSX679YSfbGtWIXO5usbRzqCPLNYkw4vFzAoMVI9wztuIpQp6muk/WsPr9O6+2upYl/zcaExh2e09jpDSh0jOSI7vFwAaC+mhq92u11jxoxRdna2rrjiCkmS2+1Wdna2brnlluPu++abb6q2tlY//vGPT/g8e/bsUWlpqZKTk9uj7C5hWEqUwu02OWsatKnAqWEp0WaXBAAIMPUut3aWVmlzQaU3aN1SWKEdpdVyHWPWTky4XQMTIzQwMfKIS4R6hHXxhhJAQKLfgL8dqnMpd0eZvtharP9uLdGmggqfxx1BVo1Lj9GZGbEamhylQUmRSo4OkaWrn/XeMKSDOz0zWvd87ZnVWvCd5K73HWexSUmnHV6ntdcZUkyGZ70FAOgiTP/+QlZWlmbMmKGxY8dq3LhxWrx4saqqqrxnI50+fbpSU1O1cOFCn/1eeOEFXXHFFc0Wta+srNT999+vq6++WklJSdq+fbvuvvtu9e/fX5MnT+6w1xXogmxWjekbo8+3FCs3v4zwFQC6MZfb0K6yau+JPrY0rs+6vbhS9a6WQ9aokCANTIzUgMRIDWoKW7va2ZQBdAn0G2hPbrehDfud3pmtX+84oLoGt8+YYSlRmjggTmf3j9fYvj0VEtwNTvpUVyXtW3N4Ruuer6WqoubjwhOOmNU6TkoZJdnDm48DgC7E9PB16tSpKi4u1vz581VQUKBRo0Zp+fLl3pNw7dq1S9aj1nLZvHmzvvjiC/373/9udjybzaZvv/1Wr7zyig4ePKiUlBRddNFFevDBB/maz1Ey0z3ha05emWZO4GQnANDVud2G9h481HjCq0rvuqzbiipVe1Tj2CTMbvMNWBsviVGOrj9rB0CXQL+BU7X34CHvzNavtpeqrKrO5/Hk6BCdPSBOEwfEa0K/WMV29T9EGoZUlnd4Ruuer6XC/0mGy3ecNVhKHuE7q7VHb2a1Auh2LIZxrNXZui+n06no6GiVl5crKirK7HL8ZtXOMl397ArFhNu1au4kmmgA6CIMw1Chs9ZnqYDNhZXaVlihqjpXi/s4gqzqnxDReHKPxnVZEyKV2iNU1k62Lnh3eR8H0LnxuypwVdTUa2Ve41IC20qUV1zl83i43abx/WI1sb8ncO0XH968l3K7JeceqXS7VLZdqiyWDLckwxNetnQtHbXt6PtHX+v4x/Ne6xjHP9F1C8evr5b2rZUOlTX/h4tM8T0pVvIIKfjUTnoHAIGqNe/jps98hXmGp/ZQSLBVZVV12lpUqYGJLGIOAJ1NSWWtz3IBWwo8YauzpqHF8cE2izLiIjQwKVIDExqvEyPVOyaMky8CALqlBpdb6/Yc9CwlsLVEa3Yf9Fnb3GqRRqX10MQB8Tp7QJxGpfVQsM3qCVgr9ks7Vh8OWUvzPNdl+ZKr+cm2ugybw7NkwJGzWqNTza4KAAIS4Ws3Zg+yakyfnvpyW6ly8koJXwGgAxmGoXqXoZoGl2rqXaqtd6um3qWaerd3W413m0s1DW7VNt4uqvAErlsLK1V61Fcfm9isFvWNDTtiXVbPia/6xoV7GkYAALopwzC0o7Tau5TAiu2lqqj1/aNl39gwTRwQp4n94nRWUoOiqnZJZSulrdulnKaQNU9qOHTsJ7IGSz37SrH9pMhkyWqTZGn82v2xrnXE1/JPNPY4x5BFsrTxGN4ajq7JItmCpYRhnpNkBXXx5RUAoJ0QvnZz4/rGesLX/DL9ZHxfs8sBANMYhqHaBrdv6NnguX2oznO7toVA9MjxtQ1HPe4NUg8Hp4f3cemISTVtZrFIvWPCNCDBs1RA05qsGfHhcgR1gxN8AABwEg5U1enL7Z6Zrf/dWqK9B48MTQ2lhx7S5anVmhBTrqH2YkVV75IKtksb8qW6ymMf2GKTevaRYvp5QtaYflJshuc6Ok2y0XIDQHfHO0E3l5kRI0nKyS+TYRis+wqgSzAMQ8WVtcovrlJ+yeHLwUP1hwPUo2aXHuuEUx3BYpFCgmwKCbYqJNimkGCbHEFNtxuvj3g8OizYE7YmRqp/QoRC7YSsAAAcqbbBpVU7D+iLrSX6YluJvttbrmijQumWAo2zFKhfcKFGR5RqgK1IsbV7ZKuvkPbIczmaxeoJUr3h6hHXPXp7ZoMCAHAMhK/d3Ki0HrIHWVVcUav8kiplxEeYXRIAnLSKmnrtKKlWXkmlN2DNawxcK2tbXvP0ZARZLd7g03FUKBoSbG0MQm1ytBCMHhmWOoKtCvXu1zxIdTRus9us/PELAIBTYBiGNhdWKHdDnvI3f6vK/ZuV6t6ngZYCXWQpUF97oXpYfE+cJZ8VAyxSdC8pJqN5yNqzD1+xBwC0GeFrNxcSbNOotB7KzS9TTn4Z4SuAgFPX4NausurGcLXSG7DmlVSpuOLYJ7KwWqRePcOUHheujPhwpceFKzbc4ROSOoKOCkWDbQoJsiqINVEBAAhcNU6pbLvK92zS3u3rVVWwRQ7nDvVy79N0S+MSAdbGy9EiUxpD1aNC1p7pUnBIR74KAEA3QfgKnZke4wlf80o1bVxvs8sB0A253YYKnDWeYLWkSvnFVd7ZrLvLqo+7NmpchEMZceE+IWtGfLjSYsJY8xQAgM7s4G5p7zdS6XapLE+ukm1yFW+TvbZUkhTdePFq/BJJtT1Ois1QaOJAWWKPmMEakyHZwzr6VQAAujnCV2hceqykbaz7CsDvDlbXecNVT9BaqbziKu0orVJN/bHXXA2325QeH670uAilx4WrX2PI2jcuXFEhrLMGAECX4nZLK56Skf2ALO7DywjZGi+SVGJEaYeRpPLQNAUnDFBi36FKHzRC9vj+CnNEmlI2AAAtIXyFTu/TQ0FWi/aX12jPgUNKi+GvwQDarqbepR2lTbNXm9Zg9cxiPVBdf8z9gqwW9Y4N885iTY+LUEZ8uDLiwhUf6eAPQwAAdAeVRdI7v5S2Z8si6X/uPtpspGmHO0k7jCRVRfRRr/6n6YzBfTWhX5x6htvNrhgAgOMifIXC7EEa0Staq3cd1Mq8UsJXACfkchvae+CQd2mAppNc5ZdUae/BQ8fdNzk6pDFcPbxEQEZchHr1DGWtVQAAurNt2dI7v5CqilVrcWhB3U/0XtCFOqt/nM4eEKcrB8Srb2wYf5AFAHQqhK+QJGVmxGr1roPKyS/TtWPTzC4HQAA4VOdSUUWNCsprtKP08Emu8kuqtKu0WnWuYy8TEBUSpIz4iMOzWOMPh61hdt56AADAERrqpP88KH31R0lSaXh/TS27SXuD+uifsyaofwLLCAAAOi86YEiSMtNj9Oyn25WTX2p2KQD8rLquQUXOWhU6a1RU4bkurvC9X1RRq4qahuMexx5kVXqsb7jaFLbGhNuZlQIAAE6sLF966+fS3lWSpKLBP9F5305StRGsx74/jOAVANDpEb5CkjS2b4ysFml32SHtO3hIKT1CzS4JQCtV1jaoyFmjQmetiipaDlSLnLWqrD1+qHqkkGCrEiJD1Cc2TP3iI3yWC0jpESqblYAVAAC00XfLpH/eLtVVSCE9VHXJk7rywyhVuw/p+yNTdO3YXmZXCADAKSN8hSQpwhGk01Kj9e2ecuXml+mK0almlwRAkmEYqqxt8AaqRUdcF1bUqsgbqtaoqs510scNDbYpMcqhhMgQJTReJ0Y5fG7HR4YoKiSIGawAAKB91VZKH94jrf2L537v8TKuel53vlesvQcL1DsmTA9feRqfQQAAXQLhK7wy02P07Z5y5eSXEr4CfmYYhpw1DSquODxTtdBZ2xiq1qj4iG2H6k8+VA2325QYFaL4SIcSokKUGOkJVJu2JUaFKCHSoQgHoSoAADDB/m+lZT+TSrdKFqt0zl3SOXfr1W/26sP1BQq2WbTk+tGKDAk2u1IAANoF4Su8MtNj9af/5isnr8zsUgC/MgxDbkNyG4bn4m75tsswZHjHSW634b3tchve47gatxuG5DKabhuqrnM1zlRtvq5qUUWNauqPfcKqo0U6ghQf5VBi40zVphA1oem68XaEg1/rAAAgABmGlPOc9NE8yVUnRaZIV/9J6jtRmwqceuCfGyRJ91w8WCN69TC3VgAA2hFdOrzOSI+RxSLllVSpyFmjhKgQs0tCAHO5DdXUuzyXBrdq612qqXerpsGzrbberdqGxm2N42ob3D5jahrH1DaNabxd3xhsekJNHXHbN+B0uxsfM44e1xiYuo8KWRtvG4bZ/3qHRYUEeQPUowPVw/cdCrPz6xoAAHRSVaXSP2ZJWz703B90qfSDp6WwGFXXNeiWV9eotsGt7w2K188mpJtbKwAA7YxuHl7RocEakhSlDfudyskv05SRKWaXhFaoqXepsrbBJ9SsqW8MRRt8tzWNORyIug6PPyow9TnWEY81uAMowfQTi0WyWiyyWSyHb1t9b1stksXiufaMs8hq9dy2B1l91lT1BqpHzGANCbaZ/TIBAAD8J/+/0ts3ShX7JZtDuughadyNng9akh745wZtK6pUQqRDv792pKyczBMA0MUQvsLHuPSYxvC1lPA1QBmGof3lNdqwz6kN+53e611l1abVZLdZ5Qi2KiTYJkeQ5zok2KqQIJvPtqYxIUGNt4MaxzWOdzTeD7ZZZW0MPD0BZ8uBp81iaTbOapGsR962WBrvN94+xvYjQ9WmMayJCgAA0EauBumz30mf/16SIcUNlK55UUoa7h3y7rp9eu3r3bJYpMXXjVJshMO8egEA8BPCV/g4MyNGL3+1Q7n5rPsaCOpdbuUVV2nD/nJt2OfU/xqD1oPV9cfcxx5kVUiQVY6jAtAjw01HYwB65DZvYHpkWBrUfNuRwaqjcX8bMxQAAADQ5OAu6a0bpd0rPfdH/0S65FHJHu4dsqu0Wve+/Z0k6dbv9ddZ/eLMqBQAAL8jfIWPcemxkqQthZUqq6pTTLjd5Iq6j4qaem0qqPDMZG0MWTcXVqiuoflJmWxWiwYkRGhocpSGpkRpaHKUBidHqUdoMF/VAgAAgHk2vCu9e4tUUy45oqTL/yANv8ZnSF2DW7f+bbUqaxt0Rt+e+vUFA0wqFgAA/yN8hY+YcLsGJkZoS2GlcvNLdfFpyWaX1OUYhqFCZ6027C/X//Y2Lh2w36mdpS0vGxDhCPIJWYemRKl/QgRrhQIAACBw1B+S/nWv9M2LnvupY6Wr/58U0/wEWr//92at21Ou6NBgPXndaAXZrB1cLAAAHYfwFc1kpsdqS2GlVuaVEb6eogaXW3klVc3WZy2rqmtxfHJ0SLOgNa1nGLNZAQAAELiKNkpvzpSKN3ruT7hdOn+uZAtuNvSTzUV6/vM8SdJj14xQSo/QDiwUAICOR/iKZjIzYvTnlTuVw7qvrVJZ26BN+31D1k0Fx142oH98hE/IOiQ5imUeAAAA0HkYhrTqJWn5HKmhRopIlK5cKvU7v8Xhhc4a/eaNdZKkGeP7aPKwpI6sFgAAUxC+oplx6TGSpE0FTpVX1ys6rPlfrLszwzBUVFHbbDbrjtIqGUbz8eF2m4YcNZt1YGIkywYAAACg8zp0QHr319LGdz33+0+SrlgqRcS3ONzlNnTH62tVVlWnIclRmnPpkA4sFgAA8xC+opmEyBBlxIUrr6RKX+8o06ShiWaXZJoGl1v5JVU+IeuGfU6VHmPZgKSoEJ+QdWhylHrHsGwAAAAAupBdK6W3bpDKd0vWYGnSAunMWZL12Gu3Lv1su77aXqowu01Lrh/NRAQAQLdB+IoWZWbEKK+kSjn5pd0mfK2qbdCmgorDQeu+cm0qqFBtC8sGWC1SvyOWDRiWEq0hyZGKjXCYUDkAAADQAdwu6YtF0icLJcMl9UyXrnlRSj39uLt9s6NMiz7aIkl64AenqV98REdUCwBAQCB8RYsy02P1t9zdXXrdV8Mw9PHGIv197V5t3OdU/jGWDQhrWjbgiNmsg5JYNgAAAADdiHOf9PZN0o7/eu4P/6F02RNSSNRxdztYXafbXlsrl9vQlaNTdfXpqR1QLAAAgYPwFS1qWvd1/d5yVdTUKzKka637umGfUw++t0Er8kp9tidEOjQ0JUrDUqI0NDlaQ1Oi1IdlAwAAANCdbV4u/f1m6VCZFBzuCV1HTTvhboZh6J63vtXeg4fUNzZMD15xmiwWPlcDALoXwle0KKVHqNJiQrW77JC+2XlA3xuUYHZJ7aK4olaLPtqs177eLcOQ7EFW/fSsvprYP05DkqMUH8myAQAAAIAkqaFW+mi+lLPUcz9phHTNS1Jc/5Pa/S8rd+pf/ytUsM2iJdefrggH7ScAoPvh3Q/HlJkeq91le5SbX9bpw9eaepde+nKHnv5kmyprGyRJl49I1j0XD1ZaTJjJ1QEAAAABpmSrtGymVPCd5/6Zv5Im/VYKOrnJChv2OfXg+xslSXMuGaLTUqP9VCgAAIGN8BXHlJkeo2Wr9ijnqK/mdyaGYejD9QVa+OFG7S47JEka2Sta8y4fqrF9Y0yuDgAAAAgwhiGtfVX64C6pvkoKi5WueFYaOPmkD1Fd16Bb/rZadQ1uXTA4QTMn9PVfvQAABDjCVxzTmRmxkqRv95Sruq5BYfbO9Z/L+r3leuCfG5S7w3PSsMQoh+65eLCuGJXKGq4AAADA0Wqc0vtZ0ndveu6nnyNd+bwUldyqwyz4x/+UV1ylpKgQPX7tSNZ5BQB0a50rTUOH6tUzVCnRIdpXXqPVOw9q4oA4s0s6KUXOGj3+r81atnqPDEMKCbbqF+f00y/Ozeh0ATIAAADQIfaukpb9TDqwQ7LYpO/dK028Q7LaWnWYv6/ZqzdX7ZHVIi2+bpRiwu3+qRcAgE6CJArHZLFYlJkRq3fW7FVOfmnAh6819S79v//m6ZlPt6u6ziVJumJUiu6+eLBSeoSaXB0AAAAQgNxuacVTUvYDkrtBiu4tXf3/pN6ZrT7UjpIq3feOZ43YX18wwPtNOgAAujOr2QVI0tNPP62+ffsqJCREmZmZys3NPebY8847TxaLpdnlsssu844xDEPz589XcnKyQkNDNWnSJG3durUjXkqXMy7dsy5qTl6ZyZUcm2EYenfdPl3wxGf6/b+3qLrOpdG9e+idX52lxdeNJngFAADo5ug3jqGySPrrNdJH8z3B69AfSL/8b5uC19oGl27522pV1bmUmR6jW88f4IeCAQDofEwPX19//XVlZWVpwYIFWr16tUaOHKnJkyerqKioxfFvv/229u/f772sX79eNptN1157rXfMY489pj/+8Y9aunSpcnJyFB4ersmTJ6umpqajXlaXkdkYvq7dfVA19S6Tq2lu7e6DumbpCv36b2u09+AhpUSH6I/TRuvtm8/S6N49zS4PAAAAJqPfOIZt2dKzE6Tt2VJQqDTlSenaV6TQHm063GPLN2v9Xqd6hgVr8XWjZOMcCwAASJIshmEYZhaQmZmpM844Q0uWLJEkud1upaWl6dZbb9Xs2bNPuP/ixYs1f/587d+/X+Hh4TIMQykpKfrNb36jO++8U5JUXl6uxMREvfzyy7ruuutOeEyn06no6GiVl5crKirq1F5gJ2cYhsY9kq3iilq9dtOZAfPVof3lh/TY8s16Z81eSVJosE2/Oq+fbjg7Q6H21q1LBQDoWngfB3CkQOw3JBN/VzXUSZ88JH35pOd+wlDpmpekhMFtPmT2xkL9/JVvJEkvzBirC4YktkelAAAErNa8j5s687Wurk6rVq3SpEmTvNusVqsmTZqkFStWnNQxXnjhBV133XUKDw+XJOXn56ugoMDnmNHR0crMzDzmMWtra+V0On0u8LBYLN7Zr4Gw9EB1XYP+8NEWfe/3n3qD16tP76VP7zpPt14wgOAVAAAAXoHSb0gB0nOU5UsvXXw4eB37c+nG/5xS8FpQXqM731wnSfrZhHSCVwAAjmJq+FpSUiKXy6XERN836MTERBUUFJxw/9zcXK1fv1433HCDd1vTfq055sKFCxUdHe29pKWltfaldGmZjbNdc/JLTavB7Tb0zpo9Ov/3n+nJ7K2qqXfrjL499e4tE/TED0cqMSrEtNoAAAAQmAKl35ACoOf4bpm09Gxp7yopJFr64Z+lyxdJwW0/P4LLbei219boQHW9TkuN0j2XDGrHggEA6BqCzC7gVLzwwgsaPny4xo0bd0rHmTNnjrKysrz3nU4nAewRmma+rt51QHUNbtmDOjazX7WzTA/8c4PW7SmXJPXqGap7Lx2iS05LksXCWlIAAADwj/bqNyQTe47aSunDe6S1f/Hc7z1euupPUo9Tf+4l/9mmnPwyhdttemra6XIE8S00AACOZmr4GhcXJ5vNpsLCQp/thYWFSkpKOu6+VVVVeu211/TAAw/4bG/ar7CwUMnJyT7HHDVqVIvHcjgccjgcbXgF3cOAhAjFhNtVVlWn7/Ye1Jg+MR3yvHsOVOvR5Zv1z3X7JEnhdptmnd9fP5uQrpBgPtgBAADg+AKl35BM6jn2fyst+5lUulWyWKVz7pLOuVuynXobmJNXqiezt0iSHr5yuNLjwk/5mAAAdEWmLjtgt9s1ZswYZWdne7e53W5lZ2dr/Pjxx933zTffVG1trX784x/7bE9PT1dSUpLPMZ1Op3Jyck54TLTMYrFoXF9P4LqyA9Z9rapt0O//tVkXPPGZ/rlunywW6boz0vTJXefpV+f1J3gFAADASem2/YZhSCuXSv/vAk/wGpkizfin9L172yV4PVBVp9teWyu3IV0zppeuGJ3aDkUDANA1mb7sQFZWlmbMmKGxY8dq3LhxWrx4saqqqjRz5kxJ0vTp05WamqqFCxf67PfCCy/oiiuuUGxsrM92i8Wi22+/XQ899JAGDBig9PR0zZs3TykpKbriiis66mV1OZkZMVr+vwLl5Jdp1vf88xxut6Flq/fo8X9tVnFFrSTpzIwYzbt8qIalRPvnSQEAANCldbt+o6pU+scsacuHnvsDL5GueEYKa59vrxmGobuWrVOBs0YZ8eG6//vD2uW4AAB0VaaHr1OnTlVxcbHmz5+vgoICjRo1SsuXL/cuYL9r1y5Zrb4TdDdv3qwvvvhC//73v1s85t13362qqirddNNNOnjwoCZOnKjly5crJISTMrVVZrrnQ+eqHWVqcLkVZGvfSdM5eaV68P0NWr/Xc9bXPrFhuvfSIbpoaCLrugIAAKDNul2/8e+5nuDVZpcuekgad5PUjp+nX/5qhz7eWCS7zaqnpo1WuMP0lhIAgIBmMQzDMLuIQON0OhUdHa3y8nJFRUWZXU5AcLsNjXrg33LWNOjvsyZoVFqPdjnu7rJqLfxwoz74znNm2EhHkH59wQBNP6sPC/YDANqE93EAnYHffldVFnnWeb14oZQ0vP2OK2n93nJd9cxXqnO5df/3h2nGWX3b9fgAAHQWrXkf58+UOClWq0Xj0mP08cYi5eSVnnL4WlFTr6c/2a4Xv8hXncstq0WaNq637rhwoOIiOPkZAAAA0CYRCdJP32v3w1bWNujWv61RncutC4cmavr4Pu3+HAAAdEWErzhpmemx+nhjkXLzy/SLc/u16Rgut6E3vtmtJ/69WSWVdZKkif3jNPfyIRqcxOwkAAAAIBDN//t65ZdUKSU6RI9fM4KlwQAAOEmErzhpmRmeRfpzd5TJ5TZks7buA9dX20r0wHsbtKmgQpKUEReu+y4bovMHJ/DhDQAAAAhQb63ao7fX7JXVIj05bbR6hNnNLgkAgE6D8BUnbWhylCIcQaqoadDG/U6dlhp9Uvvll1TpkQ826qMNhZKkqJAg3T5poH58Zh/Zg9r3xF0AAAAA2k9ecaXm/WO9JOmOSQN1Rt8YkysCAKBzIXzFSQuyWTW2b099urlYOfllJwxfyw/V66nsrXplxQ7VuzwzZX9yZh/ddsEA9Qznr+UAAABAIKttcOmWV9eous6l8Rmx+tX3+ptdEgAAnQ7hK1plXHqMJ3zNK9XPJ6a3OKbB5dbfvt6tP3y0RWVVnnVdzxsUr7mXDVH/hMiOLBcAAABAGy38YJM27HcqJtyuxdeNavWyYwAAgPAVrZSZHitJ+npHmdxuQ9ajPoB9vqVYD72/QVsKKyVJ/RMiNPeyITpvUEKH1woAAACgbT7aUKiXv9ohSXri2pFKjAoxtyAAADopwle0yohe0QoNtulAdb22FlVqUJJnJuu2oko98sFG/WdTkSSpZ1iw7rhwoK4f11tBNtZ1BQAAADqLfQcP6a5l6yRJN56dru8NZiIFAABtRfiKVgm2WTWmT099sa1EOfmlSoxyaPHHW/WXlTvV4DYUZLVoxll99evzByg6LNjscgEAAAC0QoPLrdtfW6uD1fUa0Stad00ebHZJAAB0aoSvaLXM9Bh9sa1Er3y1Q0/8e4vKD9VLkiYNSdC9lw5RRnyEyRUCAAAAaIs//mebcneUKcIRpKemjZY9iG+xAQBwKghf0WqZGZ51X7cXV0mSBiVGat7lQzVxQJyZZQEAAAA4BV9tL9FT/9kqSXrkquHqExtuckUAAHR+hK9otZFp0RqYGKGyqjrdceFATR2bxrquAAAAQCdWWlmrO15fK8OQpo5N0/dHpphdEgAAXQLhK1rNEWTT8tvOkdVqMbsUAAAAAKfIMAzd+eY6FTpr1T8hQgu+P9TskgAA6DKYrog2IXgFAAAAuoYXvsjXJ5uLZQ+yasn1oxVmZ44OAADthfAVAAAAALqpb/cc1KPLN0mS5l8+VIOTokyuCACAroXwFQAAAAC6oYqaet36tzWqdxm65LQk/Sizt9klAQDQ5RC+AgAAAEA3YxiG7ntnvXaWViu1R6h+d9UIWSwsLQYAQHsjfAUAAACAbubNVXv07rp9slkt+uO00YoOCza7JAAAuiTCVwAAAADoRrYVVWjBP/4nScq6cKDG9OlpckUAAHRdhK8AAAAA0E3U1Lt0y6trdKjepYn943Tzuf3MLgkAgC6N8BUAAAAAuomH39+oTQUViouwa9HUkbJaWecVAAB/InwFAAAAgG5g+fr9+vPKnZKkJ344SgmRISZXBABA10f4CgAAAABd3J4D1bp72beSpF+cm6FzB8abXBEAAN0D4SsAAAAAdGENLrdue22tnDUNGpXWQ3deNMjskgAA6DYIXwEAAACgC1v88Vat2nlAkY4gPTVttIJttIEAAHQU3nUBAAAAoIv6cluJnv50myTpd1ePUFpMmMkVAQDQvRC+AgAAAEAXVFJZq9tfXyvDkKaN663LRiSbXRIAAN0O4SsAAAAAdDFut6HfvLFOxRW1GpgYofmXDzW7JAAAuiXCVwAAAADoYv7fF3n6bEuxQoKtWnL96Qq128wuCQCAbonwFQAAAAC6kLW7D+qx5ZslSQumDNPAxEiTKwIAoPsifAUAAACALsJZU69b/7ZaDW5Dl41I1nVnpJldEgAA3RrhKwAAAAB0EQ+/t1G7yw6pV89QLbxquCwWi9klAQDQrQWZXQAAAAAAoH3ccn5/5ZdWac4lgxUVEmx2OQAAdHuErwAAAADQRaTFhOn1m85kxisAAAHC9GUHnn76afXt21chISHKzMxUbm7ucccfPHhQs2bNUnJyshwOhwYOHKgPPvjA+/hvf/tbWSwWn8vgwYP9/TIAAAAABKju1nMQvAIAEDhMnfn6+uuvKysrS0uXLlVmZqYWL16syZMna/PmzUpISGg2vq6uThdeeKESEhK0bNkypaamaufOnerRo4fPuGHDhunjjz/23g8KYoIvAAAA0B3RcwAAADOZ+glh0aJFuvHGGzVz5kxJ0tKlS/X+++/rxRdf1OzZs5uNf/HFF1VWVqavvvpKwcGe9Yv69u3bbFxQUJCSkpL8WjsAAACAwEfPAQAAzGTasgN1dXVatWqVJk2adLgYq1WTJk3SihUrWtzn3Xff1fjx4zVr1iwlJibqtNNO0yOPPCKXy+UzbuvWrUpJSVFGRoZ+9KMfadeuXcetpba2Vk6n0+cCAAAAoHOj5wAAAGYzLXwtKSmRy+VSYmKiz/bExEQVFBS0uE9eXp6WLVsml8ulDz74QPPmzdMTTzyhhx56yDsmMzNTL7/8spYvX65nn31W+fn5Ovvss1VRUXHMWhYuXKjo6GjvJS0trX1eJAAAAADT0HMAAACzdaqFidxutxISEvT888/LZrNpzJgx2rt3rx5//HEtWLBAknTJJZd4x48YMUKZmZnq06eP3njjDf385z9v8bhz5sxRVlaW977T6eTDEAAAANAN0XMAAID2ZFr4GhcXJ5vNpsLCQp/thYWFx1w7KTk5WcHBwbLZbN5tQ4YMUUFBgerq6mS325vt06NHDw0cOFDbtm07Zi0Oh0MOh6ONrwQAAABAIKLnAAAAZjNt2QG73a4xY8YoOzvbu83tdis7O1vjx49vcZ8JEyZo27Ztcrvd3m1btmxRcnJyix+CJKmyslLbt29XcnJy+74AAAAAAAGNngMAAJjNtPBVkrKysvSnP/1Jr7zyijZu3Kibb75ZVVVV3jORTp8+XXPmzPGOv/nmm1VWVqbbbrtNW7Zs0fvvv69HHnlEs2bN8o6588479dlnn2nHjh366quvdOWVV8pms2natGkd/voAAAAAmIueAwAAmMnUNV+nTp2q4uJizZ8/XwUFBRo1apSWL1/uXRB/165dsloP58NpaWn617/+pTvuuEMjRoxQamqqbrvtNt1zzz3eMXv27NG0adNUWlqq+Ph4TZw4UStXrlR8fHyHvz4AAAAA5qLnAAAAZrIYhmGYXUSgcTqdio6OVnl5uaKioswuBwAAtALv4wA6A35XAQDQebXmfdzUZQcAAAAAAAAAoKsifAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8gfAUAAAAAAAAAPyB8BQAAAAAAAAA/IHwFAAAAAAAAAD8wPXx9+umn1bdvX4WEhCgzM1O5ubnHHX/w4EHNmjVLycnJcjgcGjhwoD744INTOiYAAACAroueAwAAmMXU8PX1119XVlaWFixYoNWrV2vkyJGaPHmyioqKWhxfV1enCy+8UDt27NCyZcu0efNm/elPf1JqamqbjwkAAACg66LnAAAAZrIYhmGY9eSZmZk644wztGTJEkmS2+1WWlqabr31Vs2ePbvZ+KVLl+rxxx/Xpk2bFBwc3C7HbInT6VR0dLTKy8sVFRXVxlcHAADMwPs4gCPRcwAAgPbWmvdx02a+1tXVadWqVZo0adLhYqxWTZo0SStWrGhxn3fffVfjx4/XrFmzlJiYqNNOO02PPPKIXC5Xm48pSbW1tXI6nT4XAAAAAJ0bPQcAADCbaeFrSUmJXC6XEhMTfbYnJiaqoKCgxX3y8vK0bNkyuVwuffDBB5o3b56eeOIJPfTQQ20+piQtXLhQ0dHR3ktaWtopvjoAAAAAZqPnAAAAZjP9hFut4Xa7lZCQoOeff15jxozR1KlTdd9992np0qWndNw5c+aovLzce9m9e3c7VQwAAACgM6HnAAAA7SnIrCeOi4uTzWZTYWGhz/bCwkIlJSW1uE9ycrKCg4Nls9m824YMGaKCggLV1dW16ZiS5HA45HA4TuHVAAAAAAg09BwAAMBsps18tdvtGjNmjLKzs73b3G63srOzNX78+Bb3mTBhgrZt2ya32+3dtmXLFiUnJ8tut7fpmAAAAAC6JnoOAABgNlOXHcjKytKf/vQnvfLKK9q4caNuvvlmVVVVaebMmZKk6dOna86cOd7xN998s8rKynTbbbdpy5Ytev/99/XII49o1qxZJ31MAAAAAN0HPQcAADCTacsOSNLUqVNVXFys+fPnq6CgQKNGjdLy5cu9i9fv2rVLVuvhfDgtLU3/+te/dMcdd2jEiBFKTU3VbbfdpnvuueekjwkAAACg+6DnAAAAZrIYhmGYXUSgcTqdio6OVnl5uaKioswuBwAAtALv4wA6A35XAQDQebXmfdzUma+BqimPdjqdJlcCAABaq+n9m78vAwhk9BwAAHRerek5CF9bUFFRIcnzlSMAANA5VVRUKDo62uwyAKBF9BwAAHR+J9NzsOxAC9xut/bt26fIyEhZLJZ2PbbT6VRaWpp2797N14sCAD+PwMPPJPDwMwks/DxOzDAMVVRUKCUlxWcdRwAIJPQc3Qc/j8DDzySw8PMIPPxMTqw1PQczX1tgtVrVq1cvvz5HVFQU/wEHEH4egYefSeDhZxJY+HkcHzNeAQQ6eo7uh59H4OFnElj4eQQefibHd7I9B9NBAAAAAAAAAMAPCF8BAAAAAAAAwA8IXzuYw+HQggUL5HA4zC4F4ucRiPiZBB5+JoGFnwcA4ER4rwgs/DwCDz+TwMLPI/DwM2lfnHALAAAAAAAAAPyAma8AAAAAAAAA4AeErwAAAAAAAADgB4SvAAAAAAAAAOAHhK8AAAAAAAAA4AeErwAAAAAAAADgB4SvHejpp59W3759FRISoszMTOXm5ppdUre1cOFCnXHGGYqMjFRCQoKuuOIKbd682eyy0Oh3v/udLBaLbr/9drNL6db27t2rH//4x4qNjVVoaKiGDx+ub775xuyyui2Xy6V58+YpPT1doaGh6tevnx588EEZhmF2aQCAAELPETjoOQIbPUdgoOcILPQc/kH42kFef/11ZWVlacGCBVq9erVGjhypyZMnq6ioyOzSuqXPPvtMs2bN0sqVK/XRRx+pvr5eF110kaqqqswurdv7+uuv9dxzz2nEiBFml9KtHThwQBMmTFBwcLA+/PBDbdiwQU888YR69uxpdmnd1qOPPqpnn31WS5Ys0caNG/Xoo4/qscce01NPPWV2aQCAAEHPEVjoOQIXPUdgoOcIPPQc/mExiK87RGZmps444wwtWbJEkuR2u5WWlqZbb71Vs2fPNrk6FBcXKyEhQZ999pnOOeccs8vptiorK3X66afrmWee0UMPPaRRo0Zp8eLFZpfVLc2ePVtffvml/vvf/5pdChpdfvnlSkxM1AsvvODddvXVVys0NFR/+ctfTKwMABAo6DkCGz1HYKDnCBz0HIGHnsM/mPnaAerq6rRq1SpNmjTJu81qtWrSpElasWKFiZWhSXl5uSQpJibG5Eq6t1mzZumyyy7z+X8F5nj33Xc1duxYXXvttUpISNDo0aP1pz/9yeyyurWzzjpL2dnZ2rJliyRp3bp1+uKLL3TJJZeYXBkAIBDQcwQ+eo7AQM8ROOg5Ag89h38EmV1Ad1BSUiKXy6XExESf7YmJidq0aZNJVaGJ2+3W7bffrgkTJui0004zu5xu67XXXtPq1av19ddfm10KJOXl5enZZ59VVlaW7r33Xn399df69a9/LbvdrhkzZphdXrc0e/ZsOZ1ODR48WDabTS6XSw8//LB+9KMfmV0aACAA0HMENnqOwEDPEVjoOQIPPYd/EL6i25s1a5bWr1+vL774wuxSuq3du3frtttu00cffaSQkBCzy4E8DcLYsWP1yCOPSJJGjx6t9evXa+nSpXwQMskbb7yhv/71r3r11Vc1bNgwrV27VrfffrtSUlL4mQAAEODoOcxHzxF46DkCDz2HfxC+doC4uDjZbDYVFhb6bC8sLFRSUpJJVUGSbrnlFr333nv6/PPP1atXL7PL6bZWrVqloqIinX766d5tLpdLn3/+uZYsWaLa2lrZbDYTK+x+kpOTNXToUJ9tQ4YM0VtvvWVSRbjrrrs0e/ZsXXfddZKk4cOHa+fOnVq4cCEfhAAA9BwBjJ4jMNBzBB56jsBDz+EfrPnaAex2u8aMGaPs7GzvNrfbrezsbI0fP97EyrovwzB0yy236J133tF//vMfpaenm11St3bBBRfou+++09q1a72XsWPH6kc/+pHWrl3LhyATTJgwQZs3b/bZtmXLFvXp08ekilBdXS2r1fdt22azye12m1QRACCQ0HMEHnqOwELPEXjoOQIPPYd/MPO1g2RlZWnGjBkaO3asxo0bp8WLF6uqqkozZ840u7RuadasWXr11Vf1j3/8Q5GRkSooKJAkRUdHKzQ01OTqup/IyMhma1+Fh4crNjaWNbFMcscdd+iss87SI488oh/+8IfKzc3V888/r+eff97s0rqtKVOm6OGHH1bv3r01bNgwrVmzRosWLdLPfvYzs0sDAAQIeo7AQs8RWOg5Ag89R+Ch5/APi2EYhtlFdBdLlizR448/roKCAo0aNUp//OMflZmZaXZZ3ZLFYmlx+0svvaSf/vSnHVsMWnTeeedp1KhRWrx4sdmldFvvvfee5syZo61btyo9PV1ZWVm68cYbzS6r26qoqNC8efP0zjvvqKioSCkpKZo2bZrmz58vu91udnkAgABBzxE46DkCHz2H+eg5Ags9h38QvgIAAAAAAACAH7DmKwAAAAAAAAD4AeErAAAAAAAAAPgB4SsAAAAAAAAA+AHhKwAAAAAAAAD4AeErAAAAAAAAAPgB4SsAAAAAAAAA+AHhKwAAAAAAAAD4AeErAAAAAAAAAPgB4SsAAAAAAAAA+AHhKwAAAAAAAAD4AeErAAAAAAAAAPjB/wfRKk9YFYIb0QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, axs = plt.subplots(2, 2)\n",
        "fig.subplots_adjust(left=0.2, wspace=0.6)\n",
        "make_plot(axs, \n",
        "          dan_sorted_history,\n",
        "          dan_shuffled_history, \n",
        "          model_1_name='sorted',\n",
        "         model_2_name='shuffled')\n",
        "\n",
        "fig.align_ylabels(axs[:, 1])\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWLsg0WNdcE"
      },
      "source": [
        "### 2.2 DAN vs Weighted Averaging Models using Attention \n",
        "\n",
        "#### 2.2.1. Warm-Up: Manual Attention Calculation\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.2.1.a Calculate the context vector for the following query and key/value vectors. You can do this manually, or you can use \n",
        "\n",
        "\n",
        "```\n",
        "tf.keras.layers.Attention()\n",
        "```\n",
        "\n",
        "2.2.1.b What are the weights for the key/value vectors?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "fpLZyRImNdz5"
      },
      "outputs": [],
      "source": [
        "q = [1, 2., 1]\n",
        "\n",
        "k1 = v1 = [-1, -1, 3.]\n",
        "k2 = v2 = [1, 2, -5.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSbKYfjEOmlh",
        "outputId": "193c8ff5-705a-4682-ed63-31fddf91a848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The context vector:\n",
            "[ 0.   0.5 -1. ]\n",
            "\n",
            "The attention weights:\n",
            "[0.5 0.5]\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "q1 = np.array([q])\n",
        "kv = np.array([k1,k2])\n",
        "context = tf.keras.layers.Attention()([q1, kv], return_attention_scores=True)\n",
        "\n",
        "print(f\"The context vector:\\n{context[0][0].numpy()}\\n\")\n",
        "print(f\"The attention weights:\\n{context[1][0].numpy()}\")\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing tuple unpacking on attention layer\n",
        "context, weights = tf.keras.layers.Attention()([q1, kv], return_attention_scores=True)\n",
        "\n",
        "print(context.shape)\n",
        "print(weights.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyC0eX6GlJx3",
        "outputId": "b852eb60-08ff-45a4-959f-33f5f99547ee"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3)\n",
            "(1, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68YFNDesI0Cv"
      },
      "source": [
        "#### 2.2.2 The 'WAN' Model\n",
        "\n",
        "\n",
        "Next, we would like to improve our DAN by attempting to train a neural net that learns to put more weight on some words than others. How could we do that? **Attention** is the answer!\n",
        "\n",
        "Here, we will build a model that you can call \"Weighted Averaging Models using Attention\". You should construct a network that uses attention to weight the input tokens for a given example.\n",
        "\n",
        "The core structure is the same as for the DAN network, so remember to re-use the embedding matrix you initialized earlier with word2vec embedding weights.\n",
        "\n",
        "However, there are obviously some critical changes from the DAN:\n",
        "\n",
        "1) How do I create a learnable query vector for the attention calculation that is supposed to generate the suitable token probabilities? And what is its size?\n",
        "\n",
        "2) What are the key vectors for the attention calculation?\n",
        "\n",
        "3) How does the averaging change? \n",
        "\n",
        "\n",
        "First, the key vectors should be the incoming word vectors.\n",
        "\n",
        "The query vector needs to have the size of the word vectors, as it needs to attend to them. A good way to create the query vector is to generate an embedding like vector easily by getting a single row of trained weights from a Dense layer if we pass in a value of one to multiply by that weight matrix in the usual way:\n",
        "\n",
        "\n",
        "```\n",
        "wan_query_layer = tf.keras.layers.Dense(embedding_matrix.shape[1])\n",
        "```\n",
        "\n",
        "That sounds great... but how do I use this to have a vector available in my calculation? And... make this vector available to all examples in the batch?\n",
        "\n",
        "What you can use is a 'fake input-like layer' that creates for each incoming batch example a '1', that then the query layer can get applied to.\n",
        "Assuming that the input layer for your network is **wan_input_layer**, this could be done with\n",
        "\n",
        "```\n",
        "wan_one_vector = tf.Variable(tf.ones((1, 1, 1)))\n",
        "wan_batch_of_ones = tf.tile(wan_one_vector, (tf.shape(wan_input_layer)[0], 1, 1)) \n",
        "```\n",
        "\n",
        "You could then have the query vector available for each example through:\n",
        "\n",
        "```\n",
        "wan_query_vector = wan_query_layer(wan_batch_of_ones)\n",
        "\n",
        "```\n",
        "\n",
        "You will see that this structure is essentially the same as what we did for word vectors, except that we had to replace the input layer with our fake layer, as there is no actual input. We will also have **2 outputs** (discussed in a bit.)\n",
        "\n",
        "How does the averaging change? You should use:\n",
        "\n",
        "```\n",
        "tf.keras.layers.Attention()\n",
        "```\n",
        "\n",
        "and make sure you consider the proper inputs and outputs for that calculation.\n",
        "\n",
        "So why 2 outputs, and how do we do that? First off, we need the output that makes the classification, as always. What is the second output? We also would like our model to provide us with the attention weights it calculated. This will tell us which words were considered how much for the context creation.\n",
        "\n",
        "Can we implement 2 outputs? You need to have a list of the two outputs. But note that you may also want to have a list of 2 cost function and 2 metrics. You can use 'None' both times to account for our new second output, and you can ignore the corresponding values that the model report. (In general, the total loss will be a sum of the individual losses. So one would rather construct a loss that always returns zero for the second loss, but as it is very small we can ignore this here.)\n",
        "\n",
        "Finally, you may want to reshape the output after the Attention layer, because the Attention layer will still give a sequence of vectors for each example. It will just be a sequence of one weighted average vector for each example. You may want to remove that middle dimension of size one so you just have a single vector for each example. You can do that with layers.Reshape():\n",
        "\n",
        "```\n",
        "wan_attention_output = tf.keras.layers.Reshape((wan_attention_output.shape[-1],))(wan_attention_output)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "_9F8zY4WG3Mg"
      },
      "outputs": [],
      "source": [
        "# def create_wan_model(retrain_embeddings=False, \n",
        "#                      max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "#                      hidden_dim=100,\n",
        "#                      dropout=0.3,\n",
        "#                      embedding_initializer='word2vec',\n",
        "#                      learning_rate=0.001):\n",
        "#   \"\"\"\n",
        "#   Construct the WAN model including the compilation and return it. Parametrize it using the arguments.\n",
        "#   :param retrain_embeddings: boolean, indicating whether the word embeddings are trainable\n",
        "#   :param hidden_dim: dimension of the hidden layer\n",
        "#   :param dropout: dropout applied to the hidden layer\n",
        "\n",
        "#   :returns: the compiled model\n",
        "#   \"\"\"\n",
        "\n",
        "#   if embedding_initializer == 'word2vec':\n",
        "#     embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)\n",
        "#   else:\n",
        "#     embeddings_initializer='uniform'\n",
        "  \n",
        "#   ### YOUR CODE HERE\n",
        "  \n",
        "#   # inputs/embeddings\n",
        "#   wan_input_layer = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64', name='input_layer')\n",
        "#   wan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "#                                     embedding_matrix.shape[1],\n",
        "#                                     embeddings_initializer=embeddings_initializer,\n",
        "#                                     input_length=MAX_SEQUENCE_LENGTH,\n",
        "#                                     trainable=retrain_embeddings)\n",
        "#   wan_embeddings = wan_embedding_layer(wan_input_layer)\n",
        "  \n",
        "#   # creating query vector\n",
        "#   wan_query_layer = tf.keras.layers.Dense(embedding_matrix.shape[1], name='query_vector') \n",
        "#   wan_one_vector = tf.Variable(tf.ones((1, 1, 1)))\n",
        "#   wan_batch_of_ones = tf.tile(wan_one_vector, (tf.shape(wan_input_layer)[0], 1, 1)) \n",
        "#   wan_query_vector = wan_query_layer(wan_batch_of_ones)\n",
        "  \n",
        "#   # adding attention layer\n",
        "#   wan_context_output, wan_weights_output = tf.keras.layers.Attention(name='attention')([wan_query_vector, wan_embeddings], return_attention_scores=True)\n",
        "#   wan_attention_output = tf.keras.layers.Reshape((wan_context_output.shape[-1],), name='context_vectror')(wan_context_output)\n",
        "\n",
        "#   # hidden layers\n",
        "#   last_hidden_output = tf.keras.layers.Dense(hidden_dim, activation='relu',\n",
        "#                                              name='wan_hidden_1')(wan_attention_output)\n",
        "#   last_hidden_output = tf.keras.layers.Dropout(dropout)(last_hidden_output)\n",
        "\n",
        "#   wan_classification = tf.keras.layers.Dense(1, activation='sigmoid',\n",
        "#                                              name='wan_classification')(last_hidden_output)\n",
        "\n",
        "#   ## creating model/compiling\n",
        "#   wan_model = tf.keras.models.Model(inputs=wan_input_layer, outputs=[wan_classification, wan_weights_output])\n",
        "#   wan_model.compile(loss=['binary_crossentropy', None],\n",
        "#                     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,\n",
        "#                                                          beta_1=0.9,\n",
        "#                                                          beta_2=0.999,\n",
        "#                                                          epsilon=1e-07,\n",
        "#                                                          amsgrad=False,\n",
        "#                                                          name='Adam'),\n",
        "#                     metrics=['accuracy', None])\n",
        "\n",
        "\n",
        "#   ### END YOUR CODE\n",
        "    \n",
        "#   return wan_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5gnW7j8QHUo"
      },
      "source": [
        "Now train the model for the same dataset as we did for the DAN model (shuffled data) and save its history in a variable named 'wan_history'. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing model layers\n",
        "# wan_model.summary()"
      ],
      "metadata": {
        "id": "YbZrZEnpSR8m"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "A3WYcv0CNMGC"
      },
      "outputs": [],
      "source": [
        "# ### YOUR CODE HERE\n",
        "\n",
        "# # WAN model with shuffled data\n",
        "# wan_model = create_wan_model()\n",
        "\n",
        "# wan_history = wan_model.fit(train_input_ids,\n",
        "#                             train_input_labels,\n",
        "#                             validation_data=(test_input_ids, test_input_labels),\n",
        "#                             batch_size=32,\n",
        "#                             epochs=10,\n",
        "#                             shuffle=True\n",
        "#                             )\n",
        "\n",
        "# ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P0r4zH4k59o"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.2.2.a What is the highest validation accuracy that you observed for the wan training after 10 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAgsZiy8Nx1U"
      },
      "source": [
        "Now compare the results of the initial dan_model training and the wan_model training:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wan_history"
      ],
      "metadata": {
        "id": "i5O2VeYKRSgN"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "O-MMBGCWLwe1"
      },
      "outputs": [],
      "source": [
        "# fig, axs = plt.subplots(2, 2)\n",
        "# fig.subplots_adjust(left=0.2, wspace=0.6)\n",
        "# make_plot(axs, \n",
        "#           dan_shuffled_history,\n",
        "#           wan_history, \n",
        "#           model_1_name='dan',\n",
        "#          model_2_name='wan')\n",
        "\n",
        "# fig.align_ylabels(axs[:, 1])\n",
        "# fig.set_size_inches(18.5, 10.5)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILMLcnuZOWoT"
      },
      "source": [
        "Next, let us see for the wan_model which words matter most for the classification prediction and which ones did less so. How can we tell? We can look at the attention weights!\n",
        "\n",
        "Let's look at the first training example.  We'll need to convert the input_ids back into the associated strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "53pdy0pwU91Z",
        "outputId": "27b39255-8f5b-4200-d839-28d284ba8702"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "train_examples[0].numpy().decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6CFujCuX37x"
      },
      "source": [
        "The corresponding list of input ids that are suitably formatted, i.e. with sequence length 100, are these:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5U6X-fcVD88",
        "outputId": "b68ce37a-1d7f-4326-c873-c03e0efe60a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21531, 25272, 12291,  7427, 37254, 43981,  6891, 12917, 38232,\n",
              "       16915, 12929, 16182, 43981, 20526, 23487, 43981, 23807, 42958,\n",
              "       35058, 43981, 19123, 35029, 41270, 29275, 12917, 32597, 20659,\n",
              "         638, 16915, 43981,   174, 32597, 35058, 39971,  2326,  3636,\n",
              "       22434, 35029, 43981, 33922, 43981, 21531, 34710, 16908, 12291,\n",
              "       36880, 28137,  5376, 28038, 43981, 15402, 29155, 18063, 24951,\n",
              "       17433, 17595,  8856, 14193, 43981, 43248, 17433,  6290, 32597,\n",
              "        9001, 11511, 43981, 21807, 39168, 43981, 16856, 43981, 43981,\n",
              "       23245, 43981,  8889,  1331, 43981, 25272, 31976, 19123, 43981,\n",
              "       18063, 36309, 24099, 16915, 43981, 34710, 36633, 25272, 20413,\n",
              "       43981, 33458, 14926, 43981, 12139, 12289, 39617, 36633,  9483,\n",
              "       42958])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "probe_input_ids = train_input_ids[0]\n",
        "probe_input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWNRYYmUYw-D"
      },
      "source": [
        "and the first 10 corresponding tokens are: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-mdgLzh9uqa",
        "outputId": "8165992d-9cd1-4039-bf86-649853fa6f77"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1h1vScxYxMw",
        "outputId": "8ba66d22-7fb1-4d72-8d46-b02fd83be132"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'was',\n",
              " 'an',\n",
              " 'absolutely',\n",
              " 'terrible',\n",
              " 'movie.',\n",
              " \"Don't\",\n",
              " 'be',\n",
              " 'lured',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "probe_tokens = [x.decode('utf-8') for x in train_tokens[0].numpy()][:100]\n",
        "probe_tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_9R85S7YOXF"
      },
      "source": [
        "Using only the first record in the training set, identify the **5 words** with the highest impact and the **5 words** with the lowest impact on the score, i.e., identify the 5 words with the largest and  smallest weights, respectively. (Note that multiple occurences of the same word count separately for the exercise).\n",
        "\n",
        "HINT: You should create a list of (word/weight) pairs, and then sort by the second argument. Python's '.sort()' function may come in handy.  And make sure you decode the integer ids."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making predications using wan model\n",
        "# pred = wan_model.predict(probe_input_ids)"
      ],
      "metadata": {
        "id": "F0vfqzC0AhG9"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"The predication outputs a:\\t\\t\\t\\t{type(pred)}\\n\")\n",
        "# print(f\"The list's first object has a shape of:\\t\\t\\t{pred[0].shape}\\n\")\n",
        "# print(f\"The list's second object has a shape of:\\t\\t{pred[1].shape}\")"
      ],
      "metadata": {
        "id": "N1okZPiU9HKw"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # this is suppose to be a prediction of 1 or 0 \n",
        "# pred1 = pred[0]\n",
        "# pred1[:5]"
      ],
      "metadata": {
        "id": "JsfCwHeQC6PS"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # this is suppose to be a softmax list with 100 weights\n",
        "# weights = pred[1]\n",
        "# weights[:3]"
      ],
      "metadata": {
        "id": "XsGe7eVGZMNi"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "0UBhi3G0S0v8"
      },
      "outputs": [],
      "source": [
        "# ### YOUR CODE HERE\n",
        "\n",
        "# # 'pairs' should be the variable that holds the  token/weight pairs.\n",
        " \n",
        "\n",
        "\n",
        "# ### END YOUR CODE\n",
        "\n",
        "# print('most important tokens:')\n",
        "# print('\\t', pairs[:10])\n",
        "# print('\\nleast important tokens:')\n",
        "# print('\\t', pairs[-10:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURhPvLWRBNd"
      },
      "source": [
        " **QUESTION:**\n",
        "\n",
        " 2.2.2.b List the 5 most important words, with the most important first. (Again, if a word appears twice, you can include it twice.)\n",
        "\n",
        " 2.2.2.c List the 5 least important words in descending order. (Again, if a word appears twice, note it twice in the answers file.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IYOH-QfSj22"
      },
      "source": [
        "### 2.3 Approaches for Training of Embeddings\n",
        "\n",
        "Rerun the DAN Model in 3 separate configurations:\n",
        "\n",
        "\n",
        "1.   embedding_initializer = 'word2vec' and retrain_embeddings=False\n",
        "2.   embedding_initializer = 'word2vec' and retrain_embeddings=True\n",
        "3.   embedding_initializer = 'uniform' and retrain_embeddings=True\n",
        "\n",
        "\n",
        "**NOTE:** Train the model with static embeddings for 10 epochs and the ones with trainable embeddings for 3 epochs each. \n",
        "\n",
        "What do you observe about the effects of initializing and retraining the embedding matrix?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "h6Pxm-2xU1aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899dd132-4f81-40f7-eb13-245c21a2bfb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.6415 - accuracy: 0.6485 - val_loss: 0.5911 - val_accuracy: 0.7058\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.5506 - accuracy: 0.7282 - val_loss: 0.5406 - val_accuracy: 0.7288\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.5211 - accuracy: 0.7435 - val_loss: 0.5253 - val_accuracy: 0.7402\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5078 - accuracy: 0.7527 - val_loss: 0.5211 - val_accuracy: 0.7416\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4992 - accuracy: 0.7595 - val_loss: 0.5132 - val_accuracy: 0.7526\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4946 - accuracy: 0.7596 - val_loss: 0.5049 - val_accuracy: 0.7568\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4893 - accuracy: 0.7614 - val_loss: 0.5009 - val_accuracy: 0.7562\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4849 - accuracy: 0.7689 - val_loss: 0.5079 - val_accuracy: 0.7504\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.4824 - accuracy: 0.7671 - val_loss: 0.4984 - val_accuracy: 0.7616\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.4790 - accuracy: 0.7711 - val_loss: 0.4959 - val_accuracy: 0.7598\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# 1\n",
        "embedding_initializer = 'word2vec'\n",
        "retrain_embeddings=False\n",
        "\n",
        "dan1_model_shuffled = create_dan_model(retrain_embeddings=retrain_embeddings, embedding_initializer=embedding_initializer)                                                      \n",
        "\n",
        "#use dan_shuffled_history = ... below\n",
        "\n",
        "dan1_shuffled_history = dan1_model_shuffled.fit(train_input_ids,\n",
        "                            train_input_labels,\n",
        "                            validation_data=(test_input_ids, test_input_labels),\n",
        "                            batch_size=32,\n",
        "                            epochs=10,\n",
        "                            shuffle=True\n",
        "                            )\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0jwQ6ailUm4"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.3.a First, what is the highest validation accuracy that you just observed for the static model initialized with the word2vec after 10 epochs?  (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "qwlDqMTxVwbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3da58a-906c-4dd7-ee02-5bb1b2b4b185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 30s 46ms/step - loss: 0.5026 - accuracy: 0.7507 - val_loss: 0.4321 - val_accuracy: 0.8010\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.3207 - accuracy: 0.8663 - val_loss: 0.4787 - val_accuracy: 0.7830\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2386 - accuracy: 0.9075 - val_loss: 0.5247 - val_accuracy: 0.7806\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# 2\n",
        "embedding_initializer = 'word2vec' \n",
        "retrain_embeddings=True\n",
        "\n",
        "\n",
        "dan2_model_shuffled = create_dan_model(retrain_embeddings=retrain_embeddings, embedding_initializer=embedding_initializer)                                                      \n",
        "\n",
        "#use dan_shuffled_history = ... below\n",
        "\n",
        "dan2_shuffled_history = dan2_model_shuffled.fit(train_input_ids,\n",
        "                            train_input_labels,\n",
        "                            validation_data=(test_input_ids, test_input_labels),\n",
        "                            batch_size=32,\n",
        "                            epochs=3,\n",
        "                            shuffle=True\n",
        "                            )\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXr9UY7lfHE"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "\n",
        "2.3.b What is the highest validation accuracy that you observed for the model where you initialized with word2vec vectors but allow them to retrain for 3 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "H0rMPTAOVw70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b48b352-99c2-480a-8eb9-067fccfed2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 29s 45ms/step - loss: 0.5041 - accuracy: 0.7501 - val_loss: 0.4467 - val_accuracy: 0.7904\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.3166 - accuracy: 0.8681 - val_loss: 0.4693 - val_accuracy: 0.7830\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2306 - accuracy: 0.9122 - val_loss: 0.5239 - val_accuracy: 0.7724\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# 3\n",
        "embedding_initializer = 'uniform' \n",
        "retrain_embeddings=True\n",
        "\n",
        "dan3_model_shuffled = create_dan_model(retrain_embeddings=retrain_embeddings, embedding_initializer=embedding_initializer)                                                      \n",
        "\n",
        "#use dan_shuffled_history = ... below\n",
        "\n",
        "dan3_shuffled_history = dan3_model_shuffled.fit(train_input_ids,\n",
        "                            train_input_labels,\n",
        "                            validation_data=(test_input_ids, test_input_labels),\n",
        "                            batch_size=32,\n",
        "                            epochs=3,\n",
        "                            shuffle=True\n",
        "                            )\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO791d-oYOgg"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.3.c What is the highest validation accuracy that you observed for the model where you initialized randomly and then trained?  (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BGRT1g6a0T6"
      },
      "source": [
        "\n",
        "## 3. BERT-based Classification Models\n",
        "\n",
        "Now we turn to classification with BERT. We will perform classifications with various models that are based on pre-trained BERT models.\n",
        "\n",
        "\n",
        "### 3.1. Basics\n",
        "\n",
        "Let us first explore some basics of BERT. \n",
        "\n",
        "We've already loaded the pretrained BERT model and tokenizer that we'll use (\n",
        "'bert-base-cased').\n",
        "\n",
        "Now, consider this input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "aM3UggLagPn4"
      },
      "outputs": [],
      "source": [
        "test_input = ['this bank is closed on Sunday', 'the steepest bank of the river is dangerous']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWaNDy5UbmGU"
      },
      "source": [
        "Now apply the BERT tokenizer to tokenize it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmoptRz0bq1o",
        "outputId": "a217cf31-a34d-4ecb-932e-d74882d2a71c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[ 101, 1142, 3085, 1110, 1804, 1113, 3625,  102,    0,    0,    0,\n",
              "           0],\n",
              "       [ 101, 1103, 9458, 2556, 3085, 1104, 1103, 2186, 1110, 4249,  102,\n",
              "           0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "tokenized_input = bert_tokenizer(test_input, \n",
        "                                 max_length=12,\n",
        "                                 truncation=True,\n",
        "                                 padding='max_length', \n",
        "                                 return_tensors='tf')\n",
        "\n",
        "tokenized_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8WYd810dQwh"
      },
      "source": [
        " **QUESTION:** \n",
        " \n",
        " 3.1.a  Why do the attention_masks have 4 and 1 zeros, respectively?  Choose the correct one and enter it in the answers file.\n",
        "\n",
        "  *  For the first example the last four tokens belong to a different segment. For the second one it is only the last token.\n",
        "\n",
        "  *  For the first example 4 positions are padded while for the second one it is only one.\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "Next, let us look at the BERT outputs for these 2 sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "4hpNQPvBehMc"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "bert_output = bert_model(tokenized_input)\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVNsqd6QRepy"
      },
      "source": [
        " **QUESTION:** \n",
        " \n",
        " 3.1.b How many outputs are there?\n",
        " - Two arrays\n",
        "\n",
        " Enter your code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAfOnO9zov-y",
        "outputId": "3f23cd63-f243-407f-baee-4f52e85e631d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
            "array([[[ 0.3945215 ,  0.04198515,  0.06480412, ...,  0.05045468,\n",
            "          0.2235888 ,  0.24238206],\n",
            "        [-0.09458941,  0.06673873, -0.03607529, ...,  0.21925794,\n",
            "         -0.06967171,  0.7444843 ],\n",
            "        [ 0.00561046,  0.31316507, -0.17982745, ...,  0.19563255,\n",
            "         -0.10614748,  0.477736  ],\n",
            "        ...,\n",
            "        [ 0.22268727, -0.115586  ,  0.15854388, ...,  0.3002531 ,\n",
            "          0.01634064,  0.5133399 ],\n",
            "        [ 0.31638375, -0.10986965,  0.23661818, ...,  0.10924109,\n",
            "         -0.14340335,  0.32835382],\n",
            "        [ 0.34834027, -0.10076497,  0.26903206, ...,  0.127076  ,\n",
            "         -0.1843014 ,  0.26176214]],\n",
            "\n",
            "       [[ 0.44506386,  0.22265004, -0.09972464, ..., -0.23736243,\n",
            "          0.12722528,  0.07778173],\n",
            "        [ 0.07407638, -0.3180583 , -0.1192466 , ..., -0.0668015 ,\n",
            "         -0.3061705 ,  0.46923533],\n",
            "        [ 0.3145813 ,  0.62658817,  0.00606306, ..., -0.03697472,\n",
            "         -0.08461309,  0.7268304 ],\n",
            "        ...,\n",
            "        [ 0.69994617, -0.11628406,  0.01613361, ..., -0.47437245,\n",
            "          0.05725142,  0.21830113],\n",
            "        [ 0.5602969 ,  0.08535822, -0.91923475, ..., -0.3102014 ,\n",
            "         -0.09382506,  0.34911028],\n",
            "        [-0.26863506,  0.11328826,  0.07555693, ...,  0.37382194,\n",
            "          0.00740089,  0.16682105]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
            "array([[-0.6653193 ,  0.47329736,  0.9998541 , ...,  0.99994165,\n",
            "        -0.73006696,  0.97826153],\n",
            "       [-0.62586117,  0.44416317,  0.99979794, ...,  0.99991226,\n",
            "        -0.7422423 ,  0.9725732 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "print(bert_output)\n",
        "len(bert_output)\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stkFg6HBHXZO",
        "outputId": "f583afa9-2d53-43f2-a299-bff0b5a12470"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
              "array([[[ 0.3945215 ,  0.04198515,  0.06480412, ...,  0.05045468,\n",
              "          0.2235888 ,  0.24238206],\n",
              "        [-0.09458941,  0.06673873, -0.03607529, ...,  0.21925794,\n",
              "         -0.06967171,  0.7444843 ],\n",
              "        [ 0.00561046,  0.31316507, -0.17982745, ...,  0.19563255,\n",
              "         -0.10614748,  0.477736  ],\n",
              "        ...,\n",
              "        [ 0.22268727, -0.115586  ,  0.15854388, ...,  0.3002531 ,\n",
              "          0.01634064,  0.5133399 ],\n",
              "        [ 0.31638375, -0.10986965,  0.23661818, ...,  0.10924109,\n",
              "         -0.14340335,  0.32835382],\n",
              "        [ 0.34834027, -0.10076497,  0.26903206, ...,  0.127076  ,\n",
              "         -0.1843014 ,  0.26176214]],\n",
              "\n",
              "       [[ 0.44506386,  0.22265004, -0.09972464, ..., -0.23736243,\n",
              "          0.12722528,  0.07778173],\n",
              "        [ 0.07407638, -0.3180583 , -0.1192466 , ..., -0.0668015 ,\n",
              "         -0.3061705 ,  0.46923533],\n",
              "        [ 0.3145813 ,  0.62658817,  0.00606306, ..., -0.03697472,\n",
              "         -0.08461309,  0.7268304 ],\n",
              "        ...,\n",
              "        [ 0.69994617, -0.11628406,  0.01613361, ..., -0.47437245,\n",
              "          0.05725142,  0.21830113],\n",
              "        [ 0.5602969 ,  0.08535822, -0.91923475, ..., -0.3102014 ,\n",
              "         -0.09382506,  0.34911028],\n",
              "        [-0.26863506,  0.11328826,  0.07555693, ...,  0.37382194,\n",
              "          0.00740089,  0.16682105]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyTtWQCUlFxZ",
        "outputId": "a59ec221-ac84-4466-c951-71f52a447896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 3085, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(bert_tokenizer('bank'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYXhams6Bs6"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        " 3.1.c Which output do we need to use to get token-level embeddings?\n",
        "\n",
        " * the first <- ???\n",
        " \n",
        " * the second \n",
        " \n",
        " Put your answer in the answers file.\n",
        "\n",
        "**QUESTION:** \n",
        "\n",
        " 3.1.d In the tokenized input, which input_id number (i.e. the vocabulary id) corresponds to 'bank' in the two sentences? ('bert_tokenizer.tokenize()' may come in handy.. and don't forget the CLS token! )\n",
        "\n",
        " * 3085\n",
        " \n",
        "**QUESTION:** \n",
        "\n",
        " 3.1.e In the array of tokens, which position index number corresponds to 'bank' in the first sentence? ('bert_tokenizer.tokenize()' may come in handy.. and don't forget the CLS token! )\n",
        " * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X-bPMr56Bs6",
        "outputId": "fea14c5f-2789-4758-c5c3-30ff88fc04ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1142, 3085, 1110, 1804, 1113, 3625, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "#d/e. -> Look at tokens generated by the bert tokenizer for the first example\n",
        "\n",
        "bert_tokenizer(['this bank is closed on Sunday'])\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmC3H1-96Bs6"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.1.f Which array position index number corresponds to 'bank' in the second sentence?\n",
        "* 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiJrrKo26Bs6",
        "outputId": "c22cf10a-bb0c-4d43-aa2d-969f07cb6ea2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1103, 9458, 2556, 3085, 1104, 1103, 2186, 1110, 4249, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "#f. -> Look at tokenization for the second example\n",
        "\n",
        "bert_tokenizer('the steepest bank of the river is dangerous')\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd-Q-3MA6Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 3.1.g What is the cosine similarity between the BERT embeddings for the two occurences of 'bank' in the two sentences?\n",
        " * 0.74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "ZB30lRLBpKUA"
      },
      "outputs": [],
      "source": [
        "def cosine_similarities(vecs):\n",
        "    for v_1 in vecs:\n",
        "        similarities = ''\n",
        "        for v_2 in vecs:\n",
        "            similarities += ('\\t' + str(np.dot(v_1, v_2)/np.sqrt(np.dot(v_1, v_1) * np.dot(v_2, v_2)))[:4])\n",
        "        print(similarities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVIt83S26Bs6",
        "outputId": "5a4341f2-e4e8-43a8-bea7-fe1db403720c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t1.0\t0.74\n",
            "\t0.74\t1.0\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "#g.  -> get the vectors and calculate cosine similarity between the two 'bank' BERT embedddings\n",
        "\n",
        "bert_bank_output = bert_model(tokenized_input)\n",
        "bank_1 = bert_bank_output[0][0,2]\n",
        "bank_2 = bert_bank_output[0][1,4]\n",
        "banks = [bank_1, bank_2]\n",
        "\n",
        "cosine_similarities(banks)\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a2zCWHP6Bs6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "3.1.h How does this relate to the cosine similarity of 'this' (in sentence 1) and the first 'the' (in sentence 2). Compute their cosine similarity.\n",
        " * .81\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3rccfHyqWNw",
        "outputId": "78b2d9c3-aa5f-485a-cf2e-df72ab0d6322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[ 101, 1142, 3085, 1110, 1804, 1113, 3625,  102,    0,    0,    0,\n",
              "           0],\n",
              "       [ 101, 1103, 9458, 2556, 3085, 1104, 1103, 2186, 1110, 4249,  102,\n",
              "           0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "# test_input = ['this bank is closed on Sunday', 'the steepest bank of the river is dangerous']\n",
        "tokenized_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnEWs6St6Bs6",
        "outputId": "1c2331ac-4d00-4c35-db73-fbbf8adfe66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t1.0\t0.81\n",
            "\t0.81\t1.0\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "bert_output = bert_model(tokenized_input)\n",
        "this = bert_output[0][0,1]\n",
        "the = bert_output[0][1,1]\n",
        "th_ = [this, the]\n",
        "\n",
        "cosine_similarities(th_)\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# str1 = train_examples_str[0]\n",
        "# str1\n",
        "\n",
        "# # train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "# # test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "# str1_tokenized = bert_tokenizer(str1,\n",
        "#               max_length=MAX_SEQUENCE_LENGTH,\n",
        "#               truncation=True,\n",
        "#               padding='max_length',\n",
        "#               return_tensors='tf')\n",
        "# str1_tokenized\n",
        "# # l = {'input_ids': bert_train_inputs[0][0],\n",
        "# #      'token_type_ids': bert_train_inputs[1][0],\n",
        "# #      'attention_mask': bert_train_inputs[2][0]} \n",
        "\n",
        "# # l"
      ],
      "metadata": {
        "id": "B_3ya-GSUY0p"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# l_out = bert_model(str1_tokenized)\n",
        "# l_out"
      ],
      "metadata": {
        "id": "zxjF1wZnQyGo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBOvsTBwm_Vi"
      },
      "source": [
        "### 3.2 CLS-Token-based Classification \n",
        "\n",
        "In the live session we discussed classification with BERT using the pooled token. We now will do the same but extract the [CLS] token output for each example and use that for classification purposes.\n",
        "\n",
        "Consult the model from the live session and change accordingly. Make sure the BERT model is fully trainable.\n",
        "\n",
        "**HINT:**\n",
        "You will want to extract the output of the [CLS] token from the BERT output similarly to what we did above to get the output for 'bank', etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_out[0][:, 0, :]"
      ],
      "metadata": {
        "id": "c4XTyc3aOkzo"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "V1OAGPTNgPr6"
      },
      "outputs": [],
      "source": [
        "def create_bert_cls_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100, \n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the CLS Token output for classification purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    cls_token = bert_out[0][:, 0, :]\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
        "\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "\n",
        "\n",
        "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
        "    \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "                                 metrics='accuracy')\n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcK2PyPNoNc2"
      },
      "source": [
        "Now create the model and train for 2 epochs. Use batch size 8 and the appropriate validation/test set. (We don't make a distinction here between validation and test although we might in other contexts.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "hIXDr8OdiSyv"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "bert_cls_model = create_bert_cls_model()\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX5cc37Qvrey",
        "outputId": "c4f94fb9-504b-4a23-9b11-2512a38a7cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500/2500 [==============================] - 534s 198ms/step - loss: 0.4298 - accuracy: 0.7980 - val_loss: 0.4351 - val_accuracy: 0.7486\n",
            "Epoch 2/2\n",
            "2500/2500 [==============================] - 477s 191ms/step - loss: 0.3017 - accuracy: 0.8770 - val_loss: 0.4655 - val_accuracy: 0.8134\n"
          ]
        }
      ],
      "source": [
        "bert_classification_model_history = bert_cls_model.fit(\n",
        "    bert_train_inputs,\n",
        "    bert_train_labels,\n",
        "    validation_data=(bert_test_inputs, bert_test_labels),\n",
        "    batch_size=8,\n",
        "    epochs=2\n",
        ")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLjgxylMnC0x"
      },
      "source": [
        " **QUESTION:** \n",
        " \n",
        " 3.2.a What is the highest validation accuracy that you observed for the [CLS]-classification model after training for 2 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMVEBuxro4j"
      },
      "source": [
        "### 3.3 Classification by Averaging the BERT outputs\n",
        "\n",
        "Instead of using only the output vector for the [CLS] token, we will now average the output vectors from BERT for all of the tokens in the full sequence.\n",
        "\n",
        "**HINT:**\n",
        "You will want to get the full sequence of token output vectors from the BERT model and then apply an average across the tokens. You may want to use:\n",
        "\n",
        "```\n",
        "tf.math.reduce_mean()\n",
        "```\n",
        "but you can also do it in other ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# l_out[0].shape"
      ],
      "metadata": {
        "id": "X8m1LrMo4SPP"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = tf.math.reduce_mean(l_out[0], axis=1) \n",
        "# a\n"
      ],
      "metadata": {
        "id": "BUDFW-eY4jX_"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "sB2WKwLTk4LY"
      },
      "outputs": [],
      "source": [
        "def create_bert_avg_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100, \n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the average of the BERT output tokens\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    bert_out_avg = tf.math.reduce_mean(bert_out[0], axis=1)\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(bert_out_avg)\n",
        "\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "\n",
        "\n",
        "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
        "    \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "                                 metrics='accuracy')\n",
        "\n",
        "    \n",
        "    \n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcLrgI49tBde"
      },
      "source": [
        "Now create the model and train for 2 epochs. Use batch size 8 and the appropriate validation/test set. (We again don't make a distinction here.)  Remember that all layers of the BERT model should be trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "AtS29uRbk4Os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac4dde5-3e25-4246-80d7-3d89d8424e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500/2500 [==============================] - 545s 202ms/step - loss: 0.1984 - accuracy: 0.9258 - val_loss: 0.4126 - val_accuracy: 0.7858\n",
            "Epoch 2/2\n",
            "2500/2500 [==============================] - 481s 192ms/step - loss: 0.1299 - accuracy: 0.9520 - val_loss: 0.5360 - val_accuracy: 0.8224\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "bert_avg_model = create_bert_avg_model()\n",
        "\n",
        "bert_avg_classification_model_history = bert_avg_model.fit(\n",
        "    bert_train_inputs,\n",
        "    bert_train_labels,\n",
        "    validation_data=(bert_test_inputs, bert_test_labels),\n",
        "    batch_size=8,\n",
        "    epochs=2\n",
        ")  \n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiWb3y9anNlG"
      },
      "source": [
        " **QUESTION:** \n",
        " \n",
        " 3.3.a What is the highest validation accuracy that you observed for the BERT-averaging-classification model after training for 2 epochs? (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpVZXfOAt0UC"
      },
      "source": [
        "### 3.4 Adding a CNN on top of BERT\n",
        "\n",
        "Can we also combine advanced architectures? Absolutely! In the end we are dealing with tensors and it does not matter whether they are coming from static word2vec embeddings or context-based BERT embeddings. (Whether we want to is another question, but let's try it here.)\n",
        "\n",
        "\n",
        "**HINT:**\n",
        "You should appropriately stitch together the BERT-based components and the CNN components from the lesson notebook. Remember that BERT provides a sequence of contextualized token embeddings as its main output, and a CNN takes a sequence of vectors as input.\n",
        "\n",
        "Use the provided hyperparameters for CNN filter sizes and numbers of filters. Keep the same hyperparameters for the rest of the model, including a dropout layer and dense layer after the CNN, with the provided dropout rate and hidden_size. Again make sure the BERT model is trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "6IJoPmcHk4UO"
      },
      "outputs": [],
      "source": [
        "def create_bert_cnn_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          num_filters = [100, 100, 50, 25],\n",
        "                          kernel_sizes = [2, 3, 4, 5],\n",
        "                          dropout = 0.3,\n",
        "                          hidden_size = 100, \n",
        "                          learning_rate=0.00005):\n",
        "  \n",
        "    \"\"\"\n",
        "    Build a  classification model with BERT, where you apply CNN layers  to the BERT output\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    # bert input\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "    \n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    bert_out = bert_out[0]\n",
        "\n",
        "    # CNN\n",
        "    conv_layers_for_all_kernel_sizes = []\n",
        "    for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
        "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(bert_out)\n",
        "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
        "    \n",
        "    conv_output = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
        "\n",
        "    # hidden/output layers\n",
        "    last_hidden_output = tf.keras.layers.Dense(hidden_size, activation='relu', name='last_hidden_output')(conv_output)\n",
        "    last_hidden_output = tf.keras.layers.Dropout(dropout)(last_hidden_output)  \n",
        "    \n",
        "    bert_cnn_prediction = keras.layers.Dense(1, activation='sigmoid')(last_hidden_output)\n",
        "\n",
        "    bert_cnn_classification_model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=bert_cnn_prediction)\n",
        "    bert_cnn_classification_model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',  # From information theory notebooks.\n",
        "                      metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return bert_cnn_classification_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0oFm8vRDigz",
        "outputId": "422cf7c7-eb5f-4038-f753-40e62ec5e7c3"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
              "array([[[ 0.3945215 ,  0.04198515,  0.06480412, ...,  0.05045468,\n",
              "          0.2235888 ,  0.24238206],\n",
              "        [-0.09458941,  0.06673873, -0.03607529, ...,  0.21925794,\n",
              "         -0.06967171,  0.7444843 ],\n",
              "        [ 0.00561046,  0.31316507, -0.17982745, ...,  0.19563255,\n",
              "         -0.10614748,  0.477736  ],\n",
              "        ...,\n",
              "        [ 0.22268727, -0.115586  ,  0.15854388, ...,  0.3002531 ,\n",
              "          0.01634064,  0.5133399 ],\n",
              "        [ 0.31638375, -0.10986965,  0.23661818, ...,  0.10924109,\n",
              "         -0.14340335,  0.32835382],\n",
              "        [ 0.34834027, -0.10076497,  0.26903206, ...,  0.127076  ,\n",
              "         -0.1843014 ,  0.26176214]],\n",
              "\n",
              "       [[ 0.44506386,  0.22265004, -0.09972464, ..., -0.23736243,\n",
              "          0.12722528,  0.07778173],\n",
              "        [ 0.07407638, -0.3180583 , -0.1192466 , ..., -0.0668015 ,\n",
              "         -0.3061705 ,  0.46923533],\n",
              "        [ 0.3145813 ,  0.62658817,  0.00606306, ..., -0.03697472,\n",
              "         -0.08461309,  0.7268304 ],\n",
              "        ...,\n",
              "        [ 0.69994617, -0.11628406,  0.01613361, ..., -0.47437245,\n",
              "          0.05725142,  0.21830113],\n",
              "        [ 0.5602969 ,  0.08535822, -0.91923475, ..., -0.3102014 ,\n",
              "         -0.09382506,  0.34911028],\n",
              "        [-0.26863506,  0.11328826,  0.07555693, ...,  0.37382194,\n",
              "          0.00740089,  0.16682105]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-0.6653193 ,  0.47329736,  0.9998541 , ...,  0.99994165,\n",
              "        -0.73006696,  0.97826153],\n",
              "       [-0.62586117,  0.44416317,  0.99979794, ...,  0.99991226,\n",
              "        -0.7422423 ,  0.9725732 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## bert_model.trainable = True\n",
        "\n",
        "def create_bert_cnn_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          num_filters = [100, 100, 50, 25],\n",
        "                          kernel_sizes = [2, 3, 4, 5],\n",
        "                          dropout = 0.3,\n",
        "                          hidden_size = 100, \n",
        "                          learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a  classification model with BERT, where you apply CNN layers  to the BERT output\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    \n",
        "    # bert input\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "    \n",
        "    bert_model.trainable = True\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    bert_seq = bert_out[0]\n",
        "\n",
        "    # CNN\n",
        "    conv_layers_for_all_kernel_sizes = []\n",
        "    for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
        "        conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(bert_seq)\n",
        "        conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "        conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
        "    \n",
        "    conv_output = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
        "\n",
        "    # hidden/output layers\n",
        "    last_hidden_output = tf.keras.layers.Dense(hidden_size, activation='relu', name='last_hidden_output')(conv_output)\n",
        "    last_hidden_output = tf.keras.layers.Dropout(dropout)(last_hidden_output)  \n",
        "    \n",
        "    bert_cnn_prediction = keras.layers.Dense(1, activation='sigmoid')(last_hidden_output)\n",
        "\n",
        "    bert_cnn_classification_model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=bert_cnn_prediction)\n",
        "    bert_cnn_classification_model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',  # From information theory notebooks.\n",
        "                      metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return bert_cnn_classification_model"
      ],
      "metadata": {
        "id": "gDT7Y2lVCof7"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KVHhxxIhkqS"
      },
      "source": [
        "Train this model for 2 epochs as well with mini-batch size of 8:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_train_inputs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeKKS0CeEYCj",
        "outputId": "78d0c8e6-f9ef-443f-ade3-e253bba3daba"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20000, 100), dtype=int32, numpy=\n",
              "array([[1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1],\n",
              "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gobUGAVFk4XG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a377436e-e6be-4908-b7f4-664e4c4c00dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "bert_cnn_model = create_bert_cnn_model()\n",
        "\n",
        "bert_cnn_model_history = bert_cnn_model.fit(\n",
        "    bert_train_inputs,\n",
        "    bert_train_labels,\n",
        "    validation_data=(bert_test_inputs, bert_test_labels),\n",
        "    batch_size=8,\n",
        "    epochs=2\n",
        ")  \n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19zjdjc0nTn8"
      },
      "source": [
        " **QUESTION:** \n",
        " \n",
        "3.4.a What is the highest validation accuracy that you observed for the BERT-CNN-classification model after 2 epochs?  (Copy and paste the decimal value for the highest validation accuracy, e.g. a number like 0.5678 or 0.8765)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y3e9X8bvhZf"
      },
      "source": [
        "# That's It! \n",
        "## Congratulations... You are done! \n",
        "## We hope you learned a ton!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxoSk-w16Bs8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}